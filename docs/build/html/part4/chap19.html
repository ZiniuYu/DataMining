

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 19 Decision Tree Classifier &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 20 Linear Discriminant Analysis" href="chap20.html" />
    <link rel="prev" title="Chapter 18 Probabilistic Classification" href="chap18.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index4.html">Part 4 Classification</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap18.html">Chapter 18 Probabilistic Classification</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 19 Decision Tree Classifier</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#decision-trees">19.1 Decision Trees</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decision-tree-algorithm">19.2 Decision Tree Algorithm</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#split-point-evaluation-measures">19.2.1 Split Point Evaluation Measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="#evaluating-split-points">19.2.2 Evaluating Split Points</a></li>
<li class="toctree-l4"><a class="reference internal" href="#computational-complexity">19.2.3 Computational Complexity</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap20.html">Chapter 20 Linear Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap21.html">Chapter 21 Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap22.html">Chapter 22 Classification Assessment</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../part5/index5.html">Part 5 Regression</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index4.html">Part 4 Classification</a> &raquo;</li>
        
      <li>Chapter 19 Decision Tree Classifier</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part4/chap19.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\H}{\boldsymbol{\mathrm{H}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}
\newcommand{\Z}{\boldsymbol{\mathrm{Z}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cE}{\mathcal{E}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-19-decision-tree-classifier">
<h1>Chapter 19 Decision Tree Classifier<a class="headerlink" href="#chapter-19-decision-tree-classifier" title="Permalink to this headline">Â¶</a></h1>
<p>Let <span class="math notranslate nohighlight">\(\cl{R}\)</span> denote the data space that encompasses the set of input points <span class="math notranslate nohighlight">\(\D\)</span>.
A decision tree uses an axis-parallel hyperplane to split the data space
<span class="math notranslate nohighlight">\(\cl{R}\)</span> into two resulting half-spaces or regines, <span class="math notranslate nohighlight">\(\cl{R}_1\)</span> and
<span class="math notranslate nohighlight">\(\cl{R}_2\)</span>, which also induces a partition of the input points into
<span class="math notranslate nohighlight">\(\D_1\)</span> and <span class="math notranslate nohighlight">\(\D_2\)</span>, respectively.
Each of these regions is recursively split via axis-parallel hyperplanes until
the points within an induced partition are relatively pure in terms of their
class labels, that is, most of the points belong to the same class.
The resulting hierarchy of split decisions constitutes the decision tree model,
with the leaf nodes labeled with the majority class among pooints in those
regions. To classify a new <em>test</em> point we have to recursively evaluate which
half-space it belongs to until we reach a leaf node in the decision tree, at
which point we predict its class as the label of the leaf.</p>
<div class="section" id="decision-trees">
<h2>19.1 Decision Trees<a class="headerlink" href="#decision-trees" title="Permalink to this headline">Â¶</a></h2>
<p><strong>Axis-Parallel Hyperplanes</strong></p>
<p>A hyperplane <span class="math notranslate nohighlight">\(h(\x)\)</span> is defined as the set of all points <span class="math notranslate nohighlight">\(\x\)</span> that satisfy the following equation</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(h(\x):\w^T\x+b=0\)</span></p>
</div>
<p>Here <span class="math notranslate nohighlight">\(\w\in\R^d\)</span> is a <em>weight vector</em> that is normal to the hyperplane,
and <span class="math notranslate nohighlight">\(b\)</span> is the offset of the hyperplane from the origin.
A decision tree considers only <em>axis-parallel hyperplanes</em>, that is, the weight
vector must be parallel to one of the original dimensions or axes <span class="math notranslate nohighlight">\(X_j\)</span>.
Put differently, the weight vector <span class="math notranslate nohighlight">\(\w\)</span> is restricted <em>a priori</em> to one of
the standard basis vectors <span class="math notranslate nohighlight">\(\{\e_1,\e_2,\cds,\e_d\}\)</span>, where
<span class="math notranslate nohighlight">\(\e_i\in\R^d\)</span> has a 1 for the <span class="math notranslate nohighlight">\(j\)</span>th dimension, and 0 for all other
dimensions.
If <span class="math notranslate nohighlight">\(\x=(x_1,x_2,\cds,x_d)^T\)</span> and assuming <span class="math notranslate nohighlight">\(\w=\e_j\)</span>, we can rewrite as</p>
<div class="math notranslate nohighlight">
\[h(\x):\e_j^T\x+b=x_j+b=0\]</div>
<p>where the choice of the offset <span class="math notranslate nohighlight">\(b\)</span> yields different hyperplanes along dimension <span class="math notranslate nohighlight">\(X_j\)</span>.</p>
<p><strong>Split Points</strong></p>
<p>A hyperplane specifies a decision or <em>split point</em> because it splits the data space <span class="math notranslate nohighlight">\(\cl{R}\)</span> into two half-spaces.
All points <span class="math notranslate nohighlight">\(\x\)</span> such that <span class="math notranslate nohighlight">\(h(\x)\leq 0\)</span> are on the hyperplane or to
one side of the hyperplane, whereas all points such that <span class="math notranslate nohighlight">\(h(\x)&gt;0\)</span> are on
the hyperplane or to one side of the hyperplane, whereas all points such that
<span class="math notranslate nohighlight">\(h(\x)&gt;0\)</span> are on the other side.
The split point associated with an axis-parallel hyperplane can be written as
<span class="math notranslate nohighlight">\(h(\x)\leq 0\)</span>, which implies that <span class="math notranslate nohighlight">\(x_i+b\leq 0\)</span>, or
<span class="math notranslate nohighlight">\(x_i\leq-b\)</span>.
Because <span class="math notranslate nohighlight">\(x_i\)</span> is some value from dimension <span class="math notranslate nohighlight">\(X_j\)</span> and the offset
<span class="math notranslate nohighlight">\(b\)</span> can be chosen to be any value, the generic form of a split point for a
numeric attribute <span class="math notranslate nohighlight">\(X_j\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[X_j\leq v\]</div>
<p>where <span class="math notranslate nohighlight">\(v=-b\)</span> is some value in the domain of attribute <span class="math notranslate nohighlight">\(X_j\)</span>.
The decision or split point <span class="math notranslate nohighlight">\(X_j\leq v\)</span> thus splits the input data space
<span class="math notranslate nohighlight">\(\cl{R}\)</span> into two regions <span class="math notranslate nohighlight">\(\cl{R}_Y\)</span> and <span class="math notranslate nohighlight">\(\cl{R}_N\)</span>, which
denote the set of <em>all possible points</em> that satisfy the decision and those that
do not.</p>
<p><strong>Data partition</strong></p>
<p>Each split of <span class="math notranslate nohighlight">\(\cl{R}\)</span> into <span class="math notranslate nohighlight">\(\cl{R}_Y\)</span> and <span class="math notranslate nohighlight">\(\cl{R}_N\)</span> also
induces a binary partition of the corresponding input data points <span class="math notranslate nohighlight">\(\D\)</span>.
That is, a split point of the form <span class="math notranslate nohighlight">\(X_j\leq v\)</span> induces the data partition</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\D_Y=\{\x^T|\x\in\D,x_j\leq v\}\\\D_N=\{\x^T|\x\in\D,x_j&gt;v\}\end{aligned}\end{align} \]</div>
<p><strong>Purity</strong></p>
<p>Purity is the fraction of points with the majority label in <span class="math notranslate nohighlight">\(\D_j\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp purity(\D_j)=\max_i\bigg\{\frac{n_{ji}}{n_j}\bigg\}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(n_j=|\D_j|\)</span> is the total number of data points in the region
<span class="math notranslate nohighlight">\(\cl{R}_j\)</span>, and <span class="math notranslate nohighlight">\(n_{ji}\)</span> is the number of points in <span class="math notranslate nohighlight">\(\D_j\)</span>
with class label <span class="math notranslate nohighlight">\(c_i\)</span>.</p>
<p><strong>Categorical Attributes</strong></p>
<p>For a categorical attribute <span class="math notranslate nohighlight">\(X_j\)</span>, the split points or decisions are of
the <span class="math notranslate nohighlight">\(X_j\in V\)</span>, where <span class="math notranslate nohighlight">\(V\subset dom(X_j)\)</span>, and <span class="math notranslate nohighlight">\(dom(X_j)\)</span>
denotes the domain for <span class="math notranslate nohighlight">\(X_j\)</span>.
It results in two âhalf-spacesâ, one region <span class="math notranslate nohighlight">\(\cl{R}_Y\)</span> consisting of
points <span class="math notranslate nohighlight">\(\x\)</span> that satisfy the condition <span class="math notranslate nohighlight">\(x_i\in V\)</span>, and the other
region <span class="math notranslate nohighlight">\(\cl{R}_N\)</span> comprising points that satisfy the condition
<span class="math notranslate nohighlight">\(x_i\notin V\)</span>.</p>
<p><strong>Decision Rules</strong></p>
<p>A tree can be read as set of decision rules, with each ruleâs antecedent
comprising the decisions on the internal nodes along a path to a leaf, and its
consequent being the label of the leaf node.
Further, because the regions are all disjoint and cover the entire space, the
set of rules can be interpreted as a set of alternatives or disjunctions.</p>
</div>
<div class="section" id="decision-tree-algorithm">
<h2>19.2 Decision Tree Algorithm<a class="headerlink" href="#decision-tree-algorithm" title="Permalink to this headline">Â¶</a></h2>
<img alt="../_images/Algo19.1.png" src="../_images/Algo19.1.png" />
<div class="section" id="split-point-evaluation-measures">
<h3>19.2.1 Split Point Evaluation Measures<a class="headerlink" href="#split-point-evaluation-measures" title="Permalink to this headline">Â¶</a></h3>
<p><strong>Entropy</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp H(\D)=-\sum_{i=1}^kP(c_i|\D)\log_2P(C_i|\D)\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(P(c_i|\D)\)</span> is the probability of class <span class="math notranslate nohighlight">\(c_i\)</span> in <span class="math notranslate nohighlight">\(\D\)</span>, and <span class="math notranslate nohighlight">\(k\)</span> is the number of classes.
If a region is pure, that is, has points from the same class, then the entropy is zero.
On the other hand, if the classes are all miaxed up, and each appears with equal
probability <span class="math notranslate nohighlight">\(P(c_i|\D)=\frac{1}{k}\)</span>, then the entropy has the highest
value, <span class="math notranslate nohighlight">\(H(\D)=\log_2k\)</span>.</p>
<p>Define the <em>split entropy</em> as the weighted entropy of the resulting partitions, given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp H(\D_Y,\D_N)=\frac{n_Y}{n}H(\D_Y)+\frac{n_N}{n}H(\D_N)\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(n=|\D|\)</span> is the number of points in <span class="math notranslate nohighlight">\(\D\)</span>, and <span class="math notranslate nohighlight">\(n_Y=|\D_Y|\)</span> and <span class="math notranslate nohighlight">\(n_N=|\D_N|\)</span>.</p>
<p>The <em>information gain</em> for a given split point is defined as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(Gain(\D,\D_Y,\D_N)=H(\D)-H(\D_Y,\D_N)\)</span></p>
</div>
<p>The higher the information gain, the more the reduction in entropy, and the
better the split point. Thus, given split points and their corresponding
partitions, we can score each split point and choose the one that gives the
highest information gain.</p>
<p><strong>Gini Index</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp G(\D)=1-\sum_{i=1}^kP(c_i|\D)^2\)</span></p>
</div>
<p>If the partition is pure, then the probability of the majority class is 1 and
the probability of all other classes is 0, and thus, the Gini index is 0.
On the other hand, when each class is equally represented, with probability
<span class="math notranslate nohighlight">\(P(c_i|\D)=\frac{1}{k}\)</span>, then the Gini index has value
<span class="math notranslate nohighlight">\(\frac{k-1}{k}\)</span>.</p>
<p>We can compute the weighted Gini index of a split point as follows:</p>
<div class="math notranslate nohighlight">
\[G(\D_Y,\D_N)=\frac{n_Y}{n}G(\D_Y)+\frac{n_N}{n}G(\D_N)\]</div>
<p>The lower the Gini index value, the better the split point.</p>
<p>The Classification And Regression Trees (CART) measure is given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp CART(\D_Y,\D_N)=2\frac{n_Y}{n}\frac{n_N}{n}\sum_{i=1}^k|P(c_i|\D_Y)-P(c_i|\D_N)|\)</span></p>
</div>
<p>This measure thus prefers a split point that maximizes the difference between
the class probability mass function for the two partitions; the higher the CART
measure, the better the split point.</p>
</div>
<div class="section" id="evaluating-split-points">
<h3>19.2.2 Evaluating Split Points<a class="headerlink" href="#evaluating-split-points" title="Permalink to this headline">Â¶</a></h3>
<p><strong>Numeric Attributes</strong></p>
<p>One reasonable approach is to consider only the midpoints between two successive
distinct values for <span class="math notranslate nohighlight">\(X\)</span> in the sample <span class="math notranslate nohighlight">\(\D\)</span>.
Because there can be at most <span class="math notranslate nohighlight">\(n\)</span> distinct values for <span class="math notranslate nohighlight">\(X\)</span>, there are
at most <span class="math notranslate nohighlight">\(n-1\)</span> midpoint values to consider.</p>
<p>Let <span class="math notranslate nohighlight">\(\{v_1,\cds,v_m\}\)</span> denote the set of all such midpoints, such that <span class="math notranslate nohighlight">\(v_1&lt;v_2&lt;\cds&lt;v_m\)</span>.
For each split point <span class="math notranslate nohighlight">\(X\leq v\)</span>, we have to estimate the class PMFs:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\hat{P}(c_i|\D_Y)=\hat{P}(c_i|X\leq v)\\\hat{P}(c_i|\D_N)=\hat{P}(c_i|X&gt;v)\end{aligned}\end{align} \]</div>
<p>Let <span class="math notranslate nohighlight">\(I()\)</span> be an indicator variable that takes on the value 1 only when its argument is true, and is 0 otherwise.</p>
<div class="math notranslate nohighlight">
\[\hat{P}(c_i|X\leq v)=\frac{\hat{P}(X\leq v|c_i)\hat{P}(c_i)}
{\hat{P}(X\leq v)}=\frac{\hat{P}(X\leq v|c_i)\hat{P}(c_i)}
{\sum_{j=1}^k\hat{P}(X\leq v|c_j)\hat{P}(c_j)}\]</div>
<div class="math notranslate nohighlight">
\[\hat{P}(c_i)=\frac{1}{n}\sum_{j=1}^nI(y_j=c_i)=\frac{n_i}{n}\]</div>
<p>Define <span class="math notranslate nohighlight">\(N_{vi}\)</span> as the number of points <span class="math notranslate nohighlight">\(x_j\leq v\)</span> with class
<span class="math notranslate nohighlight">\(c_i\)</span>, where <span class="math notranslate nohighlight">\(x_j\)</span> is the value of data point <span class="math notranslate nohighlight">\(\x_j\)</span> for the
attribute <span class="math notranslate nohighlight">\(X\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[N_{vi}=\sum_{j=1}^nI(x_j\leq v\ \rm{and}\ y_j=c_i)\]</div>
<p>We can then estimate <span class="math notranslate nohighlight">\(P(X\leq v|c_i)\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\hat{P}(X\leq v|c_i)=\frac{\hat{P}(X\leq v\ \rm{and}\ c_i)}{\hat{P}(c_i)}=
\bigg(\frac{1}{n}\sum_{j=1}^nI(x_j\leq v\ \rm{and}\ y_j=c_i)\bigg)\bigg/
(n_i/n)=\frac{N_{vi}}{n_i}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat{P}(c_i|\D_N)=\hat{P}(c_i|X&gt;v)=\)</span>
<span class="math notranslate nohighlight">\(\dp\frac{\hat{P}(X&gt;v|c_i)|\hat{P}(c_i)}{\sum_{j=1}^k\hat{P}(X&gt;v|c_j)\hat{P}(c_j)}\)</span>
<span class="math notranslate nohighlight">\(=\dp\frac{n_i-N_{vi}}{\sum_{j=1}^k(n_j-N_{vj})}\)</span></p>
</div>
<img alt="../_images/Algo19.2.png" src="../_images/Algo19.2.png" />
<p>The total cost of numeric split point evaluation is <span class="math notranslate nohighlight">\(O(n\log n)\)</span>.</p>
<p><strong>Categorical Attributes</strong></p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a categorical attribute we evaluate split points of the form
<span class="math notranslate nohighlight">\(X\in V\)</span>, where <span class="math notranslate nohighlight">\(V\subset dom(X)\)</span> and <span class="math notranslate nohighlight">\(V\ne\emptyset\)</span>.
Because the split point <span class="math notranslate nohighlight">\(X\in V\)</span> yields the same partition as
<span class="math notranslate nohighlight">\(X\in\bar{V}\)</span>, where <span class="math notranslate nohighlight">\(\bar{V}=dom(X)\\V\)</span> is the complement of
<span class="math notranslate nohighlight">\(V\)</span>, the total number of distinct partitions is given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\sum_{i=1}^{\lfloor m/2\rfloor}\bp m\\i\ep=O(2^{m-1})\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(m=|dom(X)|\)</span>.
The number of possible split points to consider is therefore exponential in
<span class="math notranslate nohighlight">\(m\)</span>, which can pose problems if <span class="math notranslate nohighlight">\(m\)</span> is large.
One simplification is to restrict <span class="math notranslate nohighlight">\(V\)</span> to be of size one, so that there are
only <span class="math notranslate nohighlight">\(m\)</span> split points of the form <span class="math notranslate nohighlight">\(X_j\in\{ v\}\)</span>, where
<span class="math notranslate nohighlight">\(v\in dom(X_j)\)</span>.</p>
<p>To evaluate a given split point <span class="math notranslate nohighlight">\(X\in V\)</span> we have to compute the following class probability mass functions:</p>
<div class="math notranslate nohighlight">
\[P(c_i|\D_Y)=P(c_i|X\in V)\quad\quad P(c_i|\D_N)=P(c_i|X\notin V)\]</div>
<p>Making use of the Bayes theorem, we have</p>
<div class="math notranslate nohighlight">
\[P(c_i|X\in V)=\frac{P(X\in V|c_i)P(c_i)}{P(X\in V)}=\frac{P(X\in V|c_i)P(c_i)}{\sum_{j=1}^kP(X\in V|c_j)P(c_j)}\]</div>
<p>However, note that a given point <span class="math notranslate nohighlight">\(\x\)</span> can take on only one value in the
domain of <span class="math notranslate nohighlight">\(X\)</span>, and thus the values <span class="math notranslate nohighlight">\(v\in dom(X)\)</span> are mutually
exclusive.
Therefore, we have</p>
<div class="math notranslate nohighlight">
\[P(X\in V|c_i)=\sum_{v\in V}P(X=v|c_i)\]</div>
<div class="math notranslate nohighlight">
\[P(c_i|\D_Y)=\frac{\sum_{v\in V}P(X=v|c_i)P(c_i)}{\sum_{j=1}^k\sum_{v\in V}P(X=v|c_j)P(c_j)}\]</div>
<p>Define <span class="math notranslate nohighlight">\(n_{vi}\)</span> as the number of points <span class="math notranslate nohighlight">\(\x_j\in\D\)</span>, with value
<span class="math notranslate nohighlight">\(x_j=v\)</span> for attribute <span class="math notranslate nohighlight">\(X\)</span> and having class <span class="math notranslate nohighlight">\(y_j=c_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[n_{vi}=\sum_{j=1}^nI(x_j=v\ \rm{and}\ y_j=c_i)\]</div>
<p>The class conditional empirical PMF for <span class="math notranslate nohighlight">\(X\)</span> is then given as</p>
<div class="math notranslate nohighlight">
\[\hat{P}(X=v|c_i)=\frac{\hat{P}(X=v\ \rm{and}\ c_i)}{\hat{P}(c_i)}
=\bigg(\frac{1}{n}\sum_{j=1}^nI(x_j=v\ \rm{and}\ y_i=c_i)\bigg)\bigg/(n_i/n)
=\frac{n_{vi}}{n_i}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat{P}(c_i|\D_Y)=\)</span>
<span class="math notranslate nohighlight">\(\dp\frac{\sum_{v\in V}\hat{P}(X=v|c_i)\hat{P}(c_i)}{\sum_{j=1}^k\sum_{v\in V}\hat{P}(X=v|c_j)\hat{P}(c_j)}\)</span>
<span class="math notranslate nohighlight">\(\dp=\frac{\sum_{v\in V}n_{vi}}{\sum_{j=1}^k\sum_{v\in V}n_{vj}}\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat{P}(c_i|\D_N)=\hat{P}(c_i|X\notin V)=\frac{\sum_{v\notin V}n_{vi}}{\sum_{j=1}^k\sum_{v\notin V}n_{vj}}\)</span></p>
</div>
<img alt="../_images/Algo19.3.png" src="../_images/Algo19.3.png" />
<p>The total cost for categorical attributes is <span class="math notranslate nohighlight">\(O(n+mk2^{m-1})\)</span>.
If we make the assumption that <span class="math notranslate nohighlight">\(2^{m-1}=O(n)\)</span>, that is, if we bound the
maximum size of <span class="math notranslate nohighlight">\(V\)</span> to <span class="math notranslate nohighlight">\(l=O(\log n)\)</span>, then the cost of categorical
splits is bounded as <span class="math notranslate nohighlight">\(O(n\log n)\)</span>, ignoring <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div>
<div class="section" id="computational-complexity">
<h3>19.2.3 Computational Complexity<a class="headerlink" href="#computational-complexity" title="Permalink to this headline">Â¶</a></h3>
<p>The total cost in the worst case is <span class="math notranslate nohighlight">\(O(dn^2\log n)\)</span>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap20.html" class="btn btn-neutral float-right" title="Chapter 20 Linear Discriminant Analysis" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap18.html" class="btn btn-neutral float-left" title="Chapter 18 Probabilistic Classification" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>