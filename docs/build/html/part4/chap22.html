

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 22 Classification Assessment &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 5 Regression" href="../part5/index5.html" />
    <link rel="prev" title="Chapter 21 Support Vector Machines" href="chap21.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index4.html">Part 4 Classification</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap18.html">Chapter 18 Probabilistic Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap19.html">Chapter 19 Decision Tree Classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap20.html">Chapter 20 Linear Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap21.html">Chapter 21 Support Vector Machines</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 22 Classification Assessment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#classification-performance-measures">22.1 Classification Performance Measures</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#contingency-table-based-measures">22.1.1 Contingency Table-based Measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="#binary-classification-positive-and-negative-class">22.1.2 Binary Classification: Positive and Negative Class</a></li>
<li class="toctree-l4"><a class="reference internal" href="#roc-analysis">22.1.3 ROC Analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#classifier-evaluation">22.2 Classifier Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#k-fold-cross-validation">22.2.1 <span class="math notranslate nohighlight">\(K\)</span>-fold Cross-Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bootstrap-resampling">22.2.2 Bootstrap Resampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#confidence-intervals">22.2.3 Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#comparing-classifiers-paired-t-test">22.2.4 Comparing Classifiers: Paired <span class="math notranslate nohighlight">\(t\)</span>-Test</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#bias-variance-decomposition">22.3 Bias-Variance Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ensemble-classifiers">22.4 Ensemble Classifiers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#bagging">22.4.1 Bagging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#random-forest-bagging-decision-trees">22.4.2 Random Forest: Bagging Decision Trees</a></li>
<li class="toctree-l4"><a class="reference internal" href="#boosting">22.4.3 Boosting</a></li>
<li class="toctree-l4"><a class="reference internal" href="#stacking">22.4.4 Stacking</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../part5/index5.html">Part 5 Regression</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index4.html">Part 4 Classification</a> &raquo;</li>
        
      <li>Chapter 22 Classification Assessment</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part4/chap22.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\ra}{\rightarrow}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-22-classification-assessment">
<h1>Chapter 22 Classification Assessment<a class="headerlink" href="#chapter-22-classification-assessment" title="Permalink to this headline">Â¶</a></h1>
<p>We may think of the classifier as a model or function <span class="math notranslate nohighlight">\(M\)</span> that predicts
the class label <span class="math notranslate nohighlight">\(\hat{y}\)</span> for a given input example <span class="math notranslate nohighlight">\(\x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{y}=M(\x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\x=(x_1,x_2,\cds,x_d)^T\in\R^d\)</span> is a point in <span class="math notranslate nohighlight">\(d\)</span>-dimensional
space and <span class="math notranslate nohighlight">\(\hat{y}=\{c_1,c_2,\cds,c_k\}\)</span> is its predicted class.</p>
<p>To build the classification model <span class="math notranslate nohighlight">\(M\)</span> we need a <em>training set</em> of points along with their known classes.
Different classifiers are obtained depending on the assumptions used to build the model <span class="math notranslate nohighlight">\(M\)</span>.
Once the model <span class="math notranslate nohighlight">\(M\)</span> has been trained, we assess its performance over a
separate <em>testing set</em> of points for which we know the true classes.
Finally, the model can be deployed to predict the class for future points whose class we typically do not know.</p>
<div class="section" id="classification-performance-measures">
<h2>22.1 Classification Performance Measures<a class="headerlink" href="#classification-performance-measures" title="Permalink to this headline">Â¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\D\)</span> be the testing set comprising <span class="math notranslate nohighlight">\(n\)</span> points in a <span class="math notranslate nohighlight">\(d\)</span>
dimensional space, let <span class="math notranslate nohighlight">\(\{c_1,c_2,\cds,c_k\}\)</span> denote the set of <span class="math notranslate nohighlight">\(k\)</span>
class labels, and let <span class="math notranslate nohighlight">\(M\)</span> be a classifier.
For <span class="math notranslate nohighlight">\(\x_i\in\D\)</span>, let <span class="math notranslate nohighlight">\(y_i\)</span> denote its true class, and let
<span class="math notranslate nohighlight">\(\hat{y_i}=M(\x_i)\)</span> denote its predicted class.</p>
<p><strong>Error Rate</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Error\ Rate=\frac{1}{n}\sum_{i=1}^nI(y_i\ne\hat{y_i})\)</span></p>
</div>
<p><strong>Accuracy</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Accuracy=\frac{1}{n}\sum_{i=1}^nI(y_i=\hat{y_i})=1-Error\ Rate\)</span></p>
</div>
<div class="section" id="contingency-table-based-measures">
<h3>22.1.1 Contingency Table-based Measures<a class="headerlink" href="#contingency-table-based-measures" title="Permalink to this headline">Â¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\cl{D}=\{\D_1,\D_2,\cds,\D_k\}\)</span> denote a partitioning of the testing
points based on their true class labels, where</p>
<div class="math notranslate nohighlight">
\[\D_j=\{\x_i^T|y_i=c_j\}\quad\rm{and}\quad n_i=|\D_i|\]</div>
<p>Let <span class="math notranslate nohighlight">\(\cl{R}=\{\bs{\rm{R}}_1,\bs{\rm{R}}_2,\cds,\bs{\rm{R}}_k\}\)</span> denote a
partitioning of the testing points based on the predicted labels, that is,</p>
<div class="math notranslate nohighlight">
\[\bs{\rm{R}}_j=\{\x_i^T|\hat{y_i}=c_j\}\quad\rm{and}\quad m_j=|\bs{\rm{R}}_j|\]</div>
<p>The partitionings <span class="math notranslate nohighlight">\(\cl{R}\)</span> and <span class="math notranslate nohighlight">\(\cl{D}\)</span> induce a <span class="math notranslate nohighlight">\(k\times k\)</span>
contingency table <span class="math notranslate nohighlight">\(\N\)</span>, also called a <em>confusion matrix</em>, defined as
follows:</p>
<div class="math notranslate nohighlight">
\[\N(i,j)=n_{ij}=|\bs{\rm{R}}_i\cap\D_j|=|\{\x_a\in\D|\hat{y_a}=c_i\ \rm{and}\ y_a=c_j\}|\]</div>
<p>where <span class="math notranslate nohighlight">\(1\leq i,j\leq k\)</span>.</p>
<p><strong>Accuracy/Precision</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp acc_i=prec_i=\frac{n_{ii}}{m_i}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(m_i\)</span> is the number of examples predicted as <span class="math notranslate nohighlight">\(c_i\)</span> by classifier <span class="math notranslate nohighlight">\(M\)</span>.
The higher the accuracy on class <span class="math notranslate nohighlight">\(c_i\)</span> the better the classifier.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Accuracy=Precision=\sum_{i=1}^k\bigg(\frac{m_i}{n}\bigg)acc_i=\frac{1}{n}\sum_{i=1}^kn_{ii}\)</span></p>
</div>
<p><strong>Coverage/Recall</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp coverage_i=recall_i=\frac{n_{ii}}{n_i}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(n_i\)</span> is the number of points in class <span class="math notranslate nohighlight">\(c_i\)</span>.
The higher the coverage the better the classifier.</p>
<p><strong>F-measure</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp F_i=\frac{2}{\frac{1}{prec_i}+\frac{1}{recall_i}}=\frac{2\cd prec_i\cd recall_i}{prec_i+recall_i}\)</span>
<span class="math notranslate nohighlight">\(\dp=\frac{2n_{ii}}{n_i+m_i}\)</span></p>
</div>
<p>The higher the <span class="math notranslate nohighlight">\(F_i\)</span> value the better the classifier.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp F=\frac{1}{k}\sum_{i=1}^rF_i\)</span></p>
</div>
<p>For a perfect classifier, the maximum value of the F-measure is 1.</p>
</div>
<div class="section" id="binary-classification-positive-and-negative-class">
<h3>22.1.2 Binary Classification: Positive and Negative Class<a class="headerlink" href="#binary-classification-positive-and-negative-class" title="Permalink to this headline">Â¶</a></h3>
<p>When there are only <span class="math notranslate nohighlight">\(k=2\)</span> classes, we call class <span class="math notranslate nohighlight">\(c_i\)</span> the positive
class and <span class="math notranslate nohighlight">\(c_2\)</span> the negative class.</p>
<ul class="simple">
<li><p><em>True Positives (TP)</em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[TP=n_{11}|\{\x_i|\hat{y_1}=y_1=c_1\}|\]</div>
<ul class="simple">
<li><p><em>False Positives (FP)</em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[FP=n_{12}=|\{\x_i|\hat{y_i}=c_1\ \rm{and}\ y_i=c_2\}|\]</div>
<ul class="simple">
<li><p><em>False Negatives (FN)</em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[FN=n_{21}=|\{\x_i|\hat{y_i}=c_2\ \rm{and}\ y_i=c_1\}|\]</div>
<ul class="simple">
<li><p><em>True Negatives (TN)</em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[TN=n_{22}=|\{\x_i|\hat{y_i}=y_i=c_2\}|\]</div>
<p><strong>Error Rate</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Error\ Rate=\frac{FP+FN}{n}\)</span></p>
</div>
<p><strong>Accuracy</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Accuracy=\frac{TP+TN}{n}\)</span></p>
</div>
<p><strong>Class-specific Precision</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp prec_P=\frac{TP}{TP+FP}=\frac{TP}{m_1}\)</span></p>
<p><span class="math notranslate nohighlight">\(\dp prec_N=\frac{TN}{TN+FN}=\frac{TN}{m_2}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(m_i=|\bs{\rm{R}}_i|\)</span> is the number of points predicted by <span class="math notranslate nohighlight">\(M\)</span> as having class <span class="math notranslate nohighlight">\(c_i\)</span>.</p>
<p><strong>Sensitivity: True Positive Rate</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp TPR=recall_P=\frac{TP}{TP+FN}=\frac{TP}{n_1}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(n_1\)</span> is the size of the positive class.</p>
<p><strong>Specificity: True Negative Rate</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp TNR=specificity=recall_N=\frac{TN}{FP+TN}=\frac{TN}{n_2}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(n_2\)</span> is the size of the negative class.</p>
<p><strong>False Negative Rate</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(FNR=\frac{FN}{TP+FN}=\frac{FN}{n_1}=1-sensitivity\)</span></p>
</div>
<p><strong>False Positive Rate</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(FPR=\frac{FP}{FP+TN}=\frac{FP}{n_2}=1-specificity\)</span></p>
</div>
</div>
<div class="section" id="roc-analysis">
<h3>22.1.3 ROC Analysis<a class="headerlink" href="#roc-analysis" title="Permalink to this headline">Â¶</a></h3>
<p>Receiver Operating Characteristic (ROC) analysis is a popular strategy for
assessing the performance of classifiers when there are two classes.</p>
<p>Typically, a binary calssifier chooses some positive score threshold
<span class="math notranslate nohighlight">\(\rho\)</span>, and classifies all points with score above <span class="math notranslate nohighlight">\(\rho\)</span> as
positive, with the remaining points classified as negative.
ROC analysis plots the performance of the classifier over all possible values of
the threshold parameter <span class="math notranslate nohighlight">\(\rho\)</span>.
In particular, for each value of <span class="math notranslate nohighlight">\(\rho\)</span>, it plots the false positive rate
on the <span class="math notranslate nohighlight">\(x\)</span>-axis versus the true positive rate on the <span class="math notranslate nohighlight">\(y\)</span>-axis.
The resulting plot is called the <em>ROC curve</em> or <em>ROC plot</em> for the classifier.</p>
<p>Let <span class="math notranslate nohighlight">\(S(\x_i)\)</span> denote the real-valued score for the positive class output
by a classifier <span class="math notranslate nohighlight">\(M\)</span> for the point <span class="math notranslate nohighlight">\(\x_i\)</span>.
Let the maximum and minimum score thresholds observed on testing dataset <span class="math notranslate nohighlight">\(\D\)</span> be as follows:</p>
<div class="math notranslate nohighlight">
\[\rho^\min=\min_i\{S(\x_i)\}\quad\quad\rho^\max=\max_i\{S(\x_i)\}\]</div>
<p>Initially, we classify all points as negative.
Both <em>TP</em> and <em>FP</em> are thus initially zero, resulting in <em>TPR</em> and <em>FPR</em> rates
of zero, which correspond to the point (0,0) at the lower left corner in
the ROC plot.
Next for each distinct value of <span class="math notranslate nohighlight">\(\rho\)</span> in the range
<span class="math notranslate nohighlight">\([\rho^\min,\rho^\max]\)</span>, we tabulate the set of positive points:</p>
<div class="math notranslate nohighlight">
\[\bs{\rm{R}}_1(\rho)=\{\x_i\in\D:S(\x_i)&gt;\rho\}\]</div>
<p>and we compute the corresponding true and false positive rates, to obtain a new point in the ROC plot.
Finally, in the last step, we classify all points as positive.
Both <em>FN</em> and <em>TN</em> are thus zero, resulting in <em>TPR</em> and <em>FPR</em> values of 1.
This results in the point (1,1) at the top right-hand corner in the ROC plot.
An ideal classifier corresponds to the top left point (0,1), which correspoinds
to the case <span class="math notranslate nohighlight">\(FPR=0\)</span> and <span class="math notranslate nohighlight">\(TPR=1\)</span>, that is, the classifier has no
false positives, and identifies all true positives.</p>
<p>As such, a ROC curve indicates the extent to which the classifier ranks positive
instances higher than the negative instances.
An ideal classifier should score all positive points higher than any negative point.
Thus, a classifier with a curve closer to the ideal case, that is, closer to the
upper left corner, is a better classifier.</p>
<p><strong>Area Under ROC Curve</strong></p>
<p>Because the total area of the plot is 1, the AUC lies in the interval <span class="math notranslate nohighlight">\([0,1]\)</span> - the higher the better.
The AUC value is essentially the probability that the classifier will rank a
random positive test case higher than a random negative test instance.</p>
<p><strong>ROC/AUC Algorithm</strong></p>
<img alt="../_images/Algo22.1.png" src="../_images/Algo22.1.png" />
<p><strong>Random Classifier</strong></p>
<p>A random classifier corresponds to a diagonal line in the ROC plot.
It follows that if the ROC curve for any classifier is below the diagonal, it
indicates performance worse than random guessing.
For such cases, inverting the class assignment will produce a better classifier.</p>
<p><strong>Class Imbalance</strong></p>
<p>It is worth remarking that ROC curves are insensitive to class skew.
This is because the <em>TPR</em>, interpreted a s the probability of predicting a
positive point as positive, and the <em>FPR</em>, interpreted as the probability of
predicting a negative point as positive, do not depend on the ratio of the
positive to negative class size.</p>
</div>
</div>
<div class="section" id="classifier-evaluation">
<h2>22.2 Classifier Evaluation<a class="headerlink" href="#classifier-evaluation" title="Permalink to this headline">Â¶</a></h2>
<p>The input dataset <span class="math notranslate nohighlight">\(\D\)</span> is randomly split into a disjoint training set and testing set.
The training set is used to learn the model <span class="math notranslate nohighlight">\(M\)</span>, and the testing set is
used to evaluate the measure <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="section" id="k-fold-cross-validation">
<h3>22.2.1 <span class="math notranslate nohighlight">\(K\)</span>-fold Cross-Validation<a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this headline">Â¶</a></h3>
<p>Cross-validation divides the dataset <span class="math notranslate nohighlight">\(\D\)</span> into <span class="math notranslate nohighlight">\(K\)</span> equal-sized
parts, called <em>folds</em>, namely <span class="math notranslate nohighlight">\(\D_1,\D_2,\cds,\D_k\)</span>.
Each fold <span class="math notranslate nohighlight">\(\D_i\)</span> is, in turn, treated as the testing set, with the
remaining folds comprising the training set
<span class="math notranslate nohighlight">\(\D\backslash\D_i=\bigcup_{j\ne i}\D_j\)</span>.
After training the model <span class="math notranslate nohighlight">\(M_i\)</span> on <span class="math notranslate nohighlight">\(\D\backslash\D_i\)</span>, we assess its
performance on the testing set <span class="math notranslate nohighlight">\(\D_i\)</span> to obtain the <span class="math notranslate nohighlight">\(i\)</span>th estimate
<span class="math notranslate nohighlight">\(\th_i\)</span>.
The expected value of the performance measure can then be estimated as</p>
<div class="math notranslate nohighlight">
\[\hat{\mu_\th}=E[\th]=\frac{1}{K}\sum_{i=1}^K\th_i\]</div>
<p>and its variance as</p>
<div class="math notranslate nohighlight">
\[\hat{\sg_\th}^2=\frac{1}{K}\sum_{i=1}^K(\th_i-\hat{\mu_\th})^2\]</div>
<img alt="../_images/Algo22.2.png" src="../_images/Algo22.2.png" />
<p>Usually <span class="math notranslate nohighlight">\(K\)</span> is chosen to be 5 or 10.
The special case, when <span class="math notranslate nohighlight">\(K=n\)</span>, is called <em>leave-one-out</em> cross-validation,
where the tseting set comprises a single point and the remaining data is used
for training purposes.</p>
</div>
<div class="section" id="bootstrap-resampling">
<h3>22.2.2 Bootstrap Resampling<a class="headerlink" href="#bootstrap-resampling" title="Permalink to this headline">Â¶</a></h3>
<p>The bootstrap method draws <span class="math notranslate nohighlight">\(K\)</span> random samples of size <span class="math notranslate nohighlight">\(n\)</span> <em>with replacement</em> from <span class="math notranslate nohighlight">\(\D\)</span>.
Each sample <span class="math notranslate nohighlight">\(\D_i\)</span> is thus the same size as <span class="math notranslate nohighlight">\(\D\)</span>, and has several repeated points.
The probability that a point is selected is given as <span class="math notranslate nohighlight">\(p=\frac{1}{n}\)</span>, and
thus the probability that it is not selected is</p>
<div class="math notranslate nohighlight">
\[q=1-p=\bigg(1-\frac{1}{n}\bigg)\]</div>
<p>Because <span class="math notranslate nohighlight">\(\D_i\)</span> has <span class="math notranslate nohighlight">\(n\)</span> points, the probability that <span class="math notranslate nohighlight">\(\x_j\)</span> is
not selected even after <span class="math notranslate nohighlight">\(n\)</span> tries is given as</p>
<div class="math notranslate nohighlight">
\[P(\x_j\notin\D_i)=q^n=\bigg(1-\frac{1}{n}\bigg)^n\simeq e\im=0.368\]</div>
<p>On the other hand, the probability that <span class="math notranslate nohighlight">\(\x_j\in\D_i\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[P(\x_j\in\D_i)=1-P(\x_j\notin\D_i)=1-0.368=0.632\]</div>
<p>This means that each bootstrp sample contains approximately 63.2% of the points from <span class="math notranslate nohighlight">\(\D\)</span>.</p>
<img alt="../_images/Algo22.3.png" src="../_images/Algo22.3.png" />
</div>
<div class="section" id="confidence-intervals">
<h3>22.2.3 Confidence Intervals<a class="headerlink" href="#confidence-intervals" title="Permalink to this headline">Â¶</a></h3>
<p>The sum of a large number of independent and identically distributed (IID)
random variables has approximately a normal distribution, regardless of the
distribution of the individual random variables.
Let <span class="math notranslate nohighlight">\(\th_1,\th_2,\cds,\th_K\)</span> be a sequence of IID random variables,
representing, for example, the error rate or some other performance measure over
the <span class="math notranslate nohighlight">\(K\)</span>-folds in cross-validation or <span class="math notranslate nohighlight">\(K\)</span> bootstrap samples.
Assume that each <span class="math notranslate nohighlight">\(\th_i\)</span> has a finite mean <span class="math notranslate nohighlight">\(E[\th_i]=\mu\)</span> and finite
variance <span class="math notranslate nohighlight">\(\rm{var}(\th_i)=\sg^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[\hat{\mu}=\frac{1}{K}(\th_1+\th_2+\cds+\th_K)\]</div>
<div class="math notranslate nohighlight">
\[E[\hat{mu}]=E\bigg[\frac{1}{K}(\th_1+\th_2+\cds+\th_K)\bigg]=\frac{1}{K}\sum_{i=1}^KE[\th_i]=\frac{1}{K}(K\mu)=\mu\]</div>
<div class="math notranslate nohighlight">
\[\rm{var}(\hat{\mu})=var\bigg(\frac{1}{K}(\th_1+\th_2+\cds+\th_K)\bigg)=
\frac{1}{K^2}\sum_{i=1}^K\rm{var}(\th_i)=\frac{1}{K^2}(K\sg^2)=
\frac{\sg^2}{K}\]</div>
<div class="math notranslate nohighlight">
\[std(\hat{\mu})=\sqrt{\rm{var}(\hat{\mu})}=\frac{\sg}{\sqrt{K}}\]</div>
<div class="math notranslate nohighlight">
\[Z_K=\frac{\hat{\mu}-E[\hat{\mu}]}{std(\hat{\mu})}=\frac{\hat{\mu}-\mu}
{\frac{\sg}{\sqrt{K}}}=\sqrt{K}\bigg(\frac{\hat{\mu}-\mu}{\sg}\bigg)\]</div>
<p><span class="math notranslate nohighlight">\(Z_K\)</span> specifiese the deviation of the estimated mean from the true mean in terms of its standard deviation.
The central limit theorem states that, as the sample size increases, the random
variable <span class="math notranslate nohighlight">\(Z_K\)</span> <em>converges in distribution</em> to the standard normal
distribution.
That is, as <span class="math notranslate nohighlight">\(K\rightarrow\infty\)</span>, for any <span class="math notranslate nohighlight">\(x\in\R\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\lim_{k\rightarrow\infty}P(Z_K\leq x)=\Phi(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(x)\)</span> is the cumulative distribution function for the standard normal density function <span class="math notranslate nohighlight">\(f(x|0,1)\)</span>.
Given significance level <span class="math notranslate nohighlight">\(\alpha\in(0,1)\)</span>, let <span class="math notranslate nohighlight">\(z_{\alpha/w}\)</span> denote
the critical <span class="math notranslate nohighlight">\(z\)</span>-score value for the standard normal distribution that
encompasses <span class="math notranslate nohighlight">\(\alpha/2\)</span> of the probability mass in the right tail, defined
as</p>
<div class="math notranslate nohighlight">
\[P(Z_K\geq z_{\alpha/w})=\frac{\alpha}{2},\rm{or\ equivalently\ }
\Phi(z_{\alpha/2})=P(Z_K\leq z_{\alpha/2})=1-\frac{\alpha}{2}\]</div>
<p>Also, because the normal distribution is symmetric about the mean, we have</p>
<div class="math notranslate nohighlight">
\[P(Z_K\geq -z_{\alpha/2})=1-\frac{\alpha}{2},\rm{or\ equivalently\ }\Phi(-z_{\alpha/2})=\frac{\alpha}{2}\]</div>
<p>Thus, given confidence level <span class="math notranslate nohighlight">\(1-\alpha\)</span>, we can find the lower and upper
critical <span class="math notranslate nohighlight">\(z\)</span>-score values, so as to encompass <span class="math notranslate nohighlight">\(1-\alpha\)</span> fraction of
the probability mass, which is given as</p>
<div class="math notranslate nohighlight">
\[P(-z_{\alpha/2}\leq Z_K\leq z_{\alpha/2})=\Phi(z_{\alpha/2})-
\Phi(-z_{\alpha/2})=1-\frac{\alpha}{2}-\frac{\alpha}{2}=1-\alpha\]</div>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}-z_{\alpha/2}\leq Z_K\leq z_{\alpha/2}&amp;\Rightarrow -z_{\alpha/2}\leq\sqrt{K}
\bigg(\frac{\hat{\mu}-\mu}{\sg}\bigg)\leq z_{\alpha/2}\\&amp;\Rightarrow -z_{\alpha/2}\frac{\sg}{\sqrt{K}}\leq\hat{\mu}-\mu\leq z_{\alpha/2}\frac{\sg}{\sqrt{K}}\\&amp;\Rightarrow \bigg(\hat{\mu}-z_{\alpha/2}\frac{\sg}{\sqrt{K}}\bigg)\leq\mu
\leq\bigg(\hat{\mu}+z_{\alpha/2}\frac{\sg}{\sqrt{K}}\bigg)\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp P\bigg(\hat{\mu}-z_{\alpha/2}\frac{\sg}{\sqrt{K}}\leq\mu\)</span>
<span class="math notranslate nohighlight">\(\dp\leq\hat{\mu}+z_{\alpha/2}\frac{\sg}{\sqrt{K}}\bigg)=1-\alpha\)</span></p>
</div>
<p>Thus, for any given level of confidence <span class="math notranslate nohighlight">\(1-\alpha\)</span>, we can compute the
corresponding <span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> confidence interval
<span class="math notranslate nohighlight">\((\hat{\mu}-z_{\alpha/2}\frac{\sg}{\sqrt{K}},\)</span>
<span class="math notranslate nohighlight">\(\hat{\mu}+z_{\alpha/2}\frac{\sg}{\sqrt{K}})\)</span>.</p>
<p><strong>Unknown Variance</strong></p>
<p>We can replace <span class="math notranslate nohighlight">\(\sg^2\)</span> by the sample variance</p>
<div class="math notranslate nohighlight">
\[\hat{\sg}^2=\frac{1}{K}\sum_{i=1}^K(\th_i\hat{\mu})^2\]</div>
<p>because <span class="math notranslate nohighlight">\(\hat{\sg}^2\)</span> is a <em>consistent</em> estimator for <span class="math notranslate nohighlight">\(\sg^2\)</span>, that
is, as <span class="math notranslate nohighlight">\(K\rightarrow\infty\)</span>, <span class="math notranslate nohighlight">\(\hat{\sg}^2\)</span> converges with
probability 1, also called <em>converges almost surely</em>, to <span class="math notranslate nohighlight">\(\sg^2\)</span>.
The central limit theorem then states that the random variable <span class="math notranslate nohighlight">\(Z_K^*\)</span>
defined below converges in distribution to the standard normal distribution:</p>
<div class="math notranslate nohighlight">
\[Z_K^*=\sqrt{K}\bigg(\frac{\hat{\mu}-\mu}{\hat{\sg}}\bigg)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\lim_{K\rightarrow\infty}P\bigg(\hat{\mu}-z_{\alpha/2}\frac{\hat{\sg}}{\sqrt{K}})\)</span>
<span class="math notranslate nohighlight">\(\dp\leq\mu\leq\hat{\mu}-z_{\alpha/2}\frac{\hat{\sg}}{\sqrt{K}}\bigg)=1-\alpha\)</span></p>
</div>
<p>In other words, <span class="math notranslate nohighlight">\((\hat{\mu}-z_{\alpha/2}\frac{\hat{\sg}}{\sqrt{K}},)\)</span>
<span class="math notranslate nohighlight">\(\hat{\mu}-z_{\alpha/2}\frac{\hat{\sg}}{\sqrt{K}})\)</span> is the
<span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><strong>Small Sample Size</strong></p>
<p>Consider the random variables <span class="math notranslate nohighlight">\(V_i\)</span>, for <span class="math notranslate nohighlight">\(i=1,\cds,K\)</span>, defined as</p>
<div class="math notranslate nohighlight">
\[V_i=\frac{\th_i-\hat{\mu}}{\sg}\]</div>
<p>Further, consider the sum of their squares:</p>
<div class="math notranslate nohighlight">
\[S=\sum_{i=1}^KV_i^2=\sum_{i=1}^K\bigg(\frac{\th_i-\hat{\mu}}{\sg}\bigg)^2=
\frac{1}{\sg^2}\sum_{i=1}^K(\th_i-\hat{\mu})^2=\frac{K\hat{\sg}^2}{\sg^2}\]</div>
<p>If we assume that the <span class="math notranslate nohighlight">\(V_i\)</span>âs are IID with the standard normal
distribution, then the sum <span class="math notranslate nohighlight">\(S\)</span> follows a chi-squared distribution with
<span class="math notranslate nohighlight">\(K-1\)</span> degrees of freedom, denoted <span class="math notranslate nohighlight">\(\chi^2(K-1)\)</span>, since <span class="math notranslate nohighlight">\(S\)</span> is
the sum of the squares of <span class="math notranslate nohighlight">\(K\)</span> random variables <span class="math notranslate nohighlight">\(V_i\)</span>.
There are only <span class="math notranslate nohighlight">\(K-1\)</span> degrees of freedom because each <span class="math notranslate nohighlight">\(V_i\)</span> depends
on <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and the sum of the <span class="math notranslate nohighlight">\(\th_i\)</span>âs is thus fixed.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Z_K^*&amp;=\sqrt{K}\bigg(\frac{\hat{\mu}-\mu}{\hat{\sg}}\bigg)=\bigg(\frac{\hat{\mu}-\mu}{\hat{\sg}/\sqrt{K}}\bigg)\\&amp;=\bigg(\frac{\hat{\mu}-\mu}{\hat{\sg}/\sqrt{K}}\bigg/
\frac{\hat{\sg}/\sqrt{K}}{\sg/\sqrt{K}}\bigg)=\bigg(
\frac{\frac{\hat{\mu}-\mu}{\hat{\sg}/\sqrt{K}}}{\hat{\sg}/\sg}\bigg)=
\frac{Z_K}{\sqrt{S/K}}\end{aligned}\end{align} \]</div>
<p>Assuming that <span class="math notranslate nohighlight">\(Z_K\)</span> follows a standard normal distribution, and noting
that <span class="math notranslate nohighlight">\(S\)</span> follows a chi-squared distribution with <span class="math notranslate nohighlight">\(K-1\)</span> degrees of
freedom, then the distribution of <span class="math notranslate nohighlight">\(Z_K^*\)</span> is precisely the Studentâs
<span class="math notranslate nohighlight">\(t\)</span> distribution with <span class="math notranslate nohighlight">\(K-1\)</span> degrees of freedom.
Thus, in the small sample case, instead of using the standard normal density to
derive the confidence interval, we use the <span class="math notranslate nohighlight">\(t\)</span> distribution.
In particular, given confidence level <span class="math notranslate nohighlight">\(1-\alpha\)</span> we choose the critical
value <span class="math notranslate nohighlight">\(t_{\alpha/2}\)</span> such that the cumulative <span class="math notranslate nohighlight">\(t\)</span> distribution
function with <span class="math notranslate nohighlight">\(K-1\)</span> degrees of freedom encompasses <span class="math notranslate nohighlight">\(\alpha/2\)</span> of the
probability mass in the right tail.
That is,</p>
<div class="math notranslate nohighlight">
\[P(Z_K^*\geq t_{\alpha/2})=1-T_{K-1}(t_{\alpha/2})=\alpha/2\]</div>
<div class="math notranslate nohighlight">
\[P\bigg(\hat{\mu}-t_{\alpha/2}\frac{\hat{\sg}}{\sqrt{K}}\leq\mu\leq
\hat{\mu}-t_{\alpha/2}\frac{\hat{\sg}}{\sqrt{K}}\bigg)=1-\alpha\]</div>
<p>The <span class="math notranslate nohighlight">\(100(1-\alpha)%\)</span> confidence interval for the true mean <span class="math notranslate nohighlight">\(\mu\)</span> is thus</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\bigg(\hat{\mu}-t_{\alpha/2}\frac{\hat{\sg}}{\sqrt{K}}\leq\)</span>
<span class="math notranslate nohighlight">\(\dp\mu\leq\hat{\mu}-t_{\alpha/2}\frac{\hat{\sg}}{\sqrt{K}}\bigg)\)</span></p>
</div>
<p>As <span class="math notranslate nohighlight">\(K\)</span> increases, the <span class="math notranslate nohighlight">\(t\)</span> distribution very rapidly converges in
distribution to the standard normal distribution, consistent with the large
sample case.
Thus, for large samples, we may use the usual <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> threshold.</p>
</div>
<div class="section" id="comparing-classifiers-paired-t-test">
<h3>22.2.4 Comparing Classifiers: Paired <span class="math notranslate nohighlight">\(t\)</span>-Test<a class="headerlink" href="#comparing-classifiers-paired-t-test" title="Permalink to this headline">Â¶</a></h3>
<p>We look at a method that allows us to test for a significant difference in the
classification performance of two alternative classifiers, <span class="math notranslate nohighlight">\(M^A\)</span> and
<span class="math notranslate nohighlight">\(M^B\)</span>.
We want to assess which of them has a superior classification performance on a given dataset <span class="math notranslate nohighlight">\(\D\)</span>.
We perform a <em>paired test</em>, with both classifiers trained and tested on the same data.
Let <span class="math notranslate nohighlight">\(\th_1^A,\th_2^A,\cds,\th_K^A\)</span> and
<span class="math notranslate nohighlight">\(\th_1^B,\th_2^B,\cds,\th_K^B\)</span> denote the performance values for
<span class="math notranslate nohighlight">\(M^A\)</span> and <span class="math notranslate nohighlight">\(M^B\)</span>, respectively.
To determine if the two classifiers have different or similar performance,
define the random variable <span class="math notranslate nohighlight">\(\delta_i\)</span> as the difference in their
performance on the <span class="math notranslate nohighlight">\(i\)</span>th dataset:</p>
<div class="math notranslate nohighlight">
\[\delta_i=\th_i^A-\th_i^B\]</div>
<div class="math notranslate nohighlight">
\[\hat{\mu_delta}=\frac{1}{K}\sum_{i=1}^K\delta_i\quad\quad\hat{\sg_\delta}^2=
\frac{1}{K}\sum_{i=1}^K(\delta_i-\hat{\mu_\delta})^2\]</div>
<p>The null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> is that their performance is the same, that is,
the true expected difference is zero, whereas the alternative hypothesis
<span class="math notranslate nohighlight">\(H_a\)</span> is that they are not the same, that is, the true expected difference
<span class="math notranslate nohighlight">\(\mu_\delta\)</span> is not zero:</p>
<div class="math notranslate nohighlight">
\[H_0: \mu_\delta=0\quad\quad H_a:\mu_\delta\neq 0\]</div>
<div class="math notranslate nohighlight">
\[Z_\delta^*=\sqrt{K}\bigg(\frac{\hat{\mu_\delta}-\mu_\delta}{\hat{\sg_\delta}}\bigg)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Z_\delta^*=\frac{\sqrt{K}\hat{\mu_\delta}}{\hat{\sg_\delta}}\sum t_{K-1}\)</span></p>
</div>
<p>where the notation <span class="math notranslate nohighlight">\(Z_\delta^*\sim t_{K-1}\)</span> means that <span class="math notranslate nohighlight">\(Z_\delta^*\)</span>
follows the <span class="math notranslate nohighlight">\(t\)</span> distribution with <span class="math notranslate nohighlight">\(K-1\)</span> degress of freedom.</p>
<p>Given a desired confidence level <span class="math notranslate nohighlight">\(1-\alpha\)</span>, we conclude that</p>
<div class="math notranslate nohighlight">
\[P(-t_{\alpha/2}\leq Z_\delta^*\leq t_{\alpha/2})=1-\alpha\]</div>
<img alt="../_images/Algo22.4.png" src="../_images/Algo22.4.png" />
</div>
</div>
<div class="section" id="bias-variance-decomposition">
<h2>22.3 Bias-Variance Decomposition<a class="headerlink" href="#bias-variance-decomposition" title="Permalink to this headline">Â¶</a></h2>
<p>Given a training set <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span>,
with their corresponding classes <span class="math notranslate nohighlight">\(y_i\)</span>, a learned classification model
<span class="math notranslate nohighlight">\(M\)</span> predicts the class for a given test point <span class="math notranslate nohighlight">\(\x\)</span>.
A <em>loss function</em> specifies the cost or penalty of predicting the class to be
<span class="math notranslate nohighlight">\(\hat{y}=M(\x)\)</span>, when the true class is <span class="math notranslate nohighlight">\(y\)</span>.
A commonly used loss function for classification is the <em>zero-one loss</em>, defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}L(y,M(\x))=I(M(\x)\neq y)=\left\{\begin{array}{lr}0\quad\rm{if\ }M(\x)=y\\
1\quad\rm{if\ }M(\x)\neq y\end{array}\right.\end{split}\]</div>
<p>Another commonly used loss function is the <em>squared loss</em>, defined as</p>
<div class="math notranslate nohighlight">
\[L(y,M(\x))=(y-M(\x))^2\]</div>
<p>where we assume that the classes are discrete valued, and not categorical.</p>
<p><strong>Expected Loss</strong></p>
<p>The goal of learning a classification model can be cast as minimizing the expected loss:</p>
<div class="math notranslate nohighlight">
\[E_y[L(y,M(\x))|\x]=\sum_yL(y,M(\x))\cd P(y|\x)\]</div>
<p>where <span class="math notranslate nohighlight">\(E_y\)</span> denotes that the expectation is taken over the different class values <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Minimizing the expected zero-one loss corresponds to minimizing the error rate.
Let <span class="math notranslate nohighlight">\(M(\x)=c_i\)</span>, then we have</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}E_y[L(y,M(\x))|\x]&amp;=E_y[I(y\neq M(\x))|\x]\\&amp;=\sum_y I(y\neq c_i)\cd P(y|\x)\\&amp;=\sum_{y\neq c_i}P(y|\x)\\&amp;=1-P(c_i|\x)\end{aligned}\end{align} \]</div>
<p>Thus, to minimize the expected loss we should choose <span class="math notranslate nohighlight">\(c_i\)</span> as the class
that maximizes the posterior probability, that is,
<span class="math notranslate nohighlight">\(c_i=\arg\max_yP(y|\x)\)</span>.</p>
<p><strong>Bias and Variance</strong></p>
<p>Intuitively, the <em>bias</em> of a classifier refers to the systematic deviation of
its predicted decision boundary from the true decision boundary, whereas the
<em>variance</em> of a classifier refers to the deviation among the learned decision
boundaries over different training sets.
More formally, because <span class="math notranslate nohighlight">\(M\)</span> depends on the training set, given a test point
<span class="math notranslate nohighlight">\(\x\)</span>, we denote its predicted value as <span class="math notranslate nohighlight">\(M(\x,\D)\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}E[L&amp;(y,M(\x,\D))|\x,\D]\\&amp;=E_y[(y-M(\x,\D))^2|\x,\D]\\&amp;=E_y[(y-E_y[y|\x]+E_y[y|\x]-M(\x,\D))^2|\x,\D]\\&amp;=E_y[(y-E_y[y|\x])^2|\x,\D]+E_y[(M(\x,\D)-E_y[y|\x])^2|\x,\D]\\&amp;\quad+E_y[2(y-E_y[y|\x])\cd(E_y[y|\x]-M(\x,\D))|\x,\D]\\&amp;=E_y[(y-E_y[y|\x])^2|\x,\D]+(M(\x,\D)-E_y[y|\x])^2\\&amp;\quad+2(E_y[y|\x]-M(\x,\D))\cd(E_y[y|\x]-E_y[y|\x])\\&amp;=E_y[(y-E_y[y|\x])^2|\x,\D]+(M(\x,\D)-E_y[y|\x])^2\end{aligned}\end{align} \]</div>
<p>The average or expected squared error for a given test point <span class="math notranslate nohighlight">\(\x\)</span> over all training sets is then given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}E_\D[&amp;(M(\x,\D)-E_y[y|\x])^2]\\&amp;=E_\D[(M(\x,\D)-E_\D[M(\x,\D)]+E_\D[M(\x,\D)]-E_y[y|\x])^2]\\&amp;=E_\D[(M(\x,\D)-E_\D[M(\x,\D)])^2]+E_\D[(E_\D[M(\x,\D)]-E_y[y|\x])^2]\\&amp;\quad+2(E_\D[M(\x,\D)]-E_y[y|\x])\cd(E_\D[M(\x,\D)]-E_\D[M(\x,\D)])\\&amp;=E_\D[(M(\x,\D)-E_\D[M(\x,\D)])^2]+(E_\D[M(\x,\D)]-E_y[y|\x])^2\end{aligned}\end{align} \]</div>
<p>The expected squared loss over all test points <span class="math notranslate nohighlight">\(\x\)</span> and over all training
sets <span class="math notranslate nohighlight">\(\D\)</span> of size <span class="math notranslate nohighlight">\(n\)</span> yields the following decomposition into noise,
variance and bias terms:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(E_{\x,\D,y}[(y-M(\x,\D))^2]\)</span></p>
<p><span class="math notranslate nohighlight">\(\quad=E_{\x,\D,y}[(y-E_y[y|\x])^2|\x,\D]+E_{\x,\D}[(M(\x,\D)-E_y[y|\x])^2]\)</span></p>
<p><span class="math notranslate nohighlight">\(\quad=E_{\x,y}[(y-E_y[y|\x])^2]+E_{\x,\D}[(M(\x,\D)-E_\D[M(\x,\D)])]\)</span></p>
<p><span class="math notranslate nohighlight">\(\quad\quad+E_\x[(E_\D[M(\x,\D)]-E_y[y|\x])^2]\)</span></p>
</div>
<p>Thus, the expected square loss over all test points and training sets can be
decomposed into three terms: noise, average bias, and average variance.
In general, the expected loss can be attributed to high bias or high variance,
with typically a trade-off between these two terms.
Ideally, we seek a balance between these opposing trends, that is, we prefer a
classifier with an acceptable bias (reï¬ecting domain or dataset specific
assumptions) and as low a variance as possible.</p>
</div>
<div class="section" id="ensemble-classifiers">
<h2>22.4 Ensemble Classifiers<a class="headerlink" href="#ensemble-classifiers" title="Permalink to this headline">Â¶</a></h2>
<p>A classifier is called <em>unstable</em> if small pertubations in the training set
result in large changes in the prediction or decision boundary.
High variance classifiers are inherently unstable, since they tend to overfit the data.
On the other hand, high bias methods typically underfit the data, and usually have low variance.
In either case, the aim of learning is to reduce classification error by reducing the variance or bias, ideally both.
Ensemble methods create a <em>combined classifier</em> using the output of multiple
<em>base classifiers</em>, which are trained on different data subsets.</p>
<div class="section" id="bagging">
<h3>22.4.1 Bagging<a class="headerlink" href="#bagging" title="Permalink to this headline">Â¶</a></h3>
<p><em>Bagging</em>, which stands for <em>Bootstrap Aggregation</em>, is an ensemble
classification method that employs multiple bootstrap samples (with replacement)
from the input training data <span class="math notranslate nohighlight">\(\D\)</span> to create slightly different training
set <span class="math notranslate nohighlight">\(\D_t,t=1,2,\cds,K\)</span>.
Different base classifiers <span class="math notranslate nohighlight">\(M_t\)</span> are learned, with <span class="math notranslate nohighlight">\(M_t\)</span> trained on <span class="math notranslate nohighlight">\(\D_t\)</span>.
Given any test point <span class="math notranslate nohighlight">\(\x\)</span>, it is first classified using each of the <span class="math notranslate nohighlight">\(K\)</span> base classifiers, <span class="math notranslate nohighlight">\(M_t\)</span>.
Let the number of classifiers that predict the class of <span class="math notranslate nohighlight">\(\x\)</span> as <span class="math notranslate nohighlight">\(c_j\)</span> be given as</p>
<div class="math notranslate nohighlight">
\[v_j(\x)=|\{M_t(\x)=c_j|t=1,\cds,L\}|\]</div>
<p>The combined classifier, denoted <span class="math notranslate nohighlight">\(\M^K\)</span>, predicts the class of a test
point <span class="math notranslate nohighlight">\(\x\)</span> by <em>majority voting</em> among the <span class="math notranslate nohighlight">\(k\)</span> classes:</p>
<div class="math notranslate nohighlight">
\[\M^K(\x)=\arg\max_{c_j}\{v_j(\x)|j=1,\cds,k\}\]</div>
<p>For binary classification, assuming that the classes are given as
<span class="math notranslate nohighlight">\(\{+1,-1\}\)</span>, the combined classifier <span class="math notranslate nohighlight">\(\M^K\)</span> can be expressed more
simply as</p>
<div class="math notranslate nohighlight">
\[\M^K(\x)=\rm{sign}\bigg(\sum_{t=1}^KM_t(\x)\bigg)\]</div>
<p>Bagging can help reduce the variance, especially if the base classifiers are
unstable, due to the averaging effect of majority voting. It does not, in
general, have much effect on the bias.</p>
</div>
<div class="section" id="random-forest-bagging-decision-trees">
<h3>22.4.2 Random Forest: Bagging Decision Trees<a class="headerlink" href="#random-forest-bagging-decision-trees" title="Permalink to this headline">Â¶</a></h3>
<p>A <em>random forest</em> is an ensemble of <span class="math notranslate nohighlight">\(K\)</span> classifiers, <span class="math notranslate nohighlight">\(M_1,\cds,M_K\)</span>,
where each classifier is a decision tree created from a different bootstrap
sample, as in bagging.
However, the key difference from bagging is that the trees are built by sampling
a random subset of the attributes at each internal node in the decision tree.
The random sampling of the attributes results in reducing the correlation between the trees in the ensemble.</p>
<p>Let <span class="math notranslate nohighlight">\(\D\)</span> be the training dataset comprising <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(\x_j\in\R^d\)</span> along with the corresponding class <span class="math notranslate nohighlight">\(y_j\)</span>.
Let <span class="math notranslate nohighlight">\(\D_t\)</span> denote the <span class="math notranslate nohighlight">\(t\)</span>th bootstrap sample of size <span class="math notranslate nohighlight">\(n\)</span>
drawn from <span class="math notranslate nohighlight">\(\D\)</span> via sampling with replacement.
Let <span class="math notranslate nohighlight">\(p\leq d\)</span> denote the number of attributes to sample for evaluating the split points.
The random forest algorithm uses the <span class="math notranslate nohighlight">\(t\)</span>th bootstrap sample to learn a
decision tree model <span class="math notranslate nohighlight">\(M_t\)</span> via the decision tree method with one major
change.
Instead of evaluating all the <span class="math notranslate nohighlight">\(d\)</span> attributes to find the best split point,
it samples <span class="math notranslate nohighlight">\(p\leq d\)</span> attributes at random, and evaluates split points for
only those attributes.</p>
<p>The <span class="math notranslate nohighlight">\(K\)</span> decision trees <span class="math notranslate nohighlight">\(M_1,M_2,\cds,M_K\)</span> comprise the random forest
model <span class="math notranslate nohighlight">\(\M_k\)</span>, which predicts the class of a test point <span class="math notranslate nohighlight">\(\x\)</span> by
majority voting as in bagging:</p>
<div class="math notranslate nohighlight">
\[\M^K(\x)=\arg\max_{c_j}\{v_j(\x)|j=1,\cds,k\}\]</div>
<p>where <span class="math notranslate nohighlight">\(v_j\)</span> is the number of trees that predict the class of <span class="math notranslate nohighlight">\(\x\)</span> as <span class="math notranslate nohighlight">\(c_j\)</span>.
That is,</p>
<div class="math notranslate nohighlight">
\[v_j(\x)=|\{M_t(\x)=c_j|t=1,\cds,K\}|\]</div>
<p>Notice that if <span class="math notranslate nohighlight">\(p=d\)</span> the the random forest approach is equivalent to bagging over decision tree models.</p>
<img alt="../_images/Algo22.5.png" src="../_images/Algo22.5.png" />
<p>Given bootstrap sample <span class="math notranslate nohighlight">\(\D_t\)</span>, any point in <span class="math notranslate nohighlight">\(\D\backslash\D_t\)</span> is
called an <em>out-of-bag</em> point for classifier <span class="math notranslate nohighlight">\(M_t\)</span>, since it was not used
to train <span class="math notranslate nohighlight">\(M_t\)</span>.
One of the side-benefits of the bootstrap approach is that we can compute the
<em>out-of-bag</em> error rate for the random forest by considering the prediction of
each model <span class="math notranslate nohighlight">\(M_t\)</span> over its out-of-bag points.
Let <span class="math notranslate nohighlight">\(v_j(\x)\)</span> be the number of votes for class <span class="math notranslate nohighlight">\(c_j\)</span> over all
decision trees in the ensemble where <span class="math notranslate nohighlight">\(\x\)</span> was out-of-bag.
We can aggregate these votes after we train each classifier <span class="math notranslate nohighlight">\(M_t\)</span>, by
incrementing the value <span class="math notranslate nohighlight">\(v_j(\x)\)</span> if <span class="math notranslate nohighlight">\(\hat{y}=M_t(\x)=c_j\)</span> and if
<span class="math notranslate nohighlight">\(\x\)</span> is out-of-bag for <span class="math notranslate nohighlight">\(M_t\)</span>.
The out-of-bag (OOB) error for the random forest is given as:</p>
<div class="math notranslate nohighlight">
\[\epsilon_{oob}=\frac{1}{n}\cd\sum_{i=1}^nI(y_i\ne\arg\max_{c_j}\{v_j(\x_i)|(\x_i,y_i)\in\D\})\]</div>
<p>The out-of-bag error rate approximates the cross-validation error rate quite
well, and can be used in lieu of <span class="math notranslate nohighlight">\(k\)</span>-fold cross-validation to evaluate the
random forest model.</p>
</div>
<div class="section" id="boosting">
<h3>22.4.3 Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">Â¶</a></h3>
<p>Starting from an initial training sample <span class="math notranslate nohighlight">\(\D_1\)</span>, we train the base
classifier <span class="math notranslate nohighlight">\(M_1\)</span>, and obtain its training error rate.
To construct the next sample <span class="math notranslate nohighlight">\(\D_2\)</span>, we select the misclassified instances
with higher probability, and after training <span class="math notranslate nohighlight">\(M_2\)</span>, we obtain its training
error rate.
To construct <span class="math notranslate nohighlight">\(\D_3\)</span>, those instances that are hard to classify by
<span class="math notranslate nohighlight">\(M_1\)</span> or <span class="math notranslate nohighlight">\(M_2\)</span>, have a higher probability of being selected.
This process is repeated for <span class="math notranslate nohighlight">\(K\)</span> iterations.
Finally, the combined classifier is obtained via weighted voting over the output
of the <span class="math notranslate nohighlight">\(K\)</span> base classifiers <span class="math notranslate nohighlight">\(M_1,M_2,\cds,M_K\)</span>.</p>
<p><strong>Adaptive Boosting: AdaBoost</strong></p>
<p>Let <span class="math notranslate nohighlight">\(\D\)</span> be the input training set, comprising <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span>.
The boosting process will be repeated <span class="math notranslate nohighlight">\(K\)</span> times.
Let <span class="math notranslate nohighlight">\(t\)</span> denote the iteration and let <span class="math notranslate nohighlight">\(\alpha_t\)</span> denote the weight
for the <span class="math notranslate nohighlight">\(r\)</span>th classifier <span class="math notranslate nohighlight">\(M_t\)</span>.
Let <span class="math notranslate nohighlight">\(w_i^t\)</span> denote the weight for <span class="math notranslate nohighlight">\(\x_i\)</span>, with
<span class="math notranslate nohighlight">\(\w^t=(w_1^t,w_2^t,\cds,w_n^t)^T\)</span> being the weight vector over all the
points for the <span class="math notranslate nohighlight">\(t\)</span>th iteration.
In fact, <span class="math notranslate nohighlight">\(\w\)</span> is a probability vector, whose elements sum to one.
Initially all points have equal weights, that is,</p>
<div class="math notranslate nohighlight">
\[\w^0=\bigg(\frac{1}{n},\frac{1}{n},\cds,\frac{1}{n}\bigg)^T=\frac{1}{n}\1\]</div>
<p>During iteration <span class="math notranslate nohighlight">\(t\)</span>, the training sample <span class="math notranslate nohighlight">\(\D_t\)</span> is obtained via
weighted resampling using the distribution <span class="math notranslate nohighlight">\(\w^{t-1}\)</span>, that is, we draw a
sample of size <span class="math notranslate nohighlight">\(n\)</span> with replacement, such that the <span class="math notranslate nohighlight">\(i\)</span>th point is
chosen according to its probability <span class="math notranslate nohighlight">\(w_i^{t-1}\)</span>.
Next, we train the classifier <span class="math notranslate nohighlight">\(M_t\)</span> using <span class="math notranslate nohighlight">\(\D_t\)</span>, and compute its
weighted error rate <span class="math notranslate nohighlight">\(\epsilon_t\)</span> on the entire input dataset <span class="math notranslate nohighlight">\(\D\)</span>:</p>
<div class="math notranslate nohighlight">
\[\epsilon_t=\sum_{i=1}^nw_i^{t-1}\cd I(M_t(\x_i)\neq y_i)\]</div>
<p>The weight <span class="math notranslate nohighlight">\(\alpha_t\)</span> for the <span class="math notranslate nohighlight">\(t\)</span>th classifier is then set as</p>
<div class="math notranslate nohighlight">
\[\alpha_t=\ln\bigg(\frac{1-\epsilon_t}{\epsilon_t}\bigg)\]</div>
<p>and the weight for each point <span class="math notranslate nohighlight">\(\x_i\in\D\)</span> is updated based on whether the point is misclassified or not</p>
<div class="math notranslate nohighlight">
\[w_i^t=w_i^{t-1}\cd\exp\{\alpha_t\cd I(M_t(\x_i)\neq y_i)\}\]</div>
<p>Thus, if the predicted class matches the true class, that is, if
<span class="math notranslate nohighlight">\(M_t(\x_i)=y_i\)</span>, then <span class="math notranslate nohighlight">\(I(M_t(\x_i)\neq y_i)=0\)</span>, and the weight for
point <span class="math notranslate nohighlight">\(\x_i\)</span> remains unchanged.
On the other hand, if the point is misclassified, that is,
<span class="math notranslate nohighlight">\(M_t(\x_i)\neq y_i\)</span>, then we have <span class="math notranslate nohighlight">\(I(M_t(\x_i)\neq y_i)=1\)</span> and</p>
<div class="math notranslate nohighlight">
\[w_i^t=w_i^{t-1}\cd\exp\{\alpha_t\}=w_i^{t-1}\exp\bigg\{\ln\bigg(
    \frac{1-\epsilon_t}{\epsilon}\bigg)\bigg\}=w_i^{t-1}
    \bigg(\frac{1}{\epsilon_t}-1\bigg)\]</div>
<p>Once the point weights have been updated, we re-normalize the weights so that <span class="math notranslate nohighlight">\(\w^t\)</span> is a probability vector:</p>
<div class="math notranslate nohighlight">
\[\w_t=\frac{\w^t}{\1^T\w^t}=\frac{1}{\sum_{j=1}^nw_j^t}(w_1^t,w_2^t,\cds,w_n^t)^T\]</div>
<img alt="../_images/Algo22.6.png" src="../_images/Algo22.6.png" />
<p><strong>Combined Classifier</strong></p>
<p>Given the set of boosted classifiers, <span class="math notranslate nohighlight">\(M_1,M_2,\cds,M_K\)</span>, along with their
weights <span class="math notranslate nohighlight">\(\alpha_1,\alpha_2,\cds,\alpha_K\)</span>, the class for a test case
<span class="math notranslate nohighlight">\(\x\)</span> is obtained via weighted majority voting.
Let <span class="math notranslate nohighlight">\(v_j(\x)\)</span> denote the weighted vote for class <span class="math notranslate nohighlight">\(c_j\)</span> over the <span class="math notranslate nohighlight">\(K\)</span> classifiers, given as</p>
<div class="math notranslate nohighlight">
\[v_j(\x)=\sum_{t=1}^K\alpha_t\cd I(M_t(\x)=c_j)\]</div>
<p>The combined classifier, denoted <span class="math notranslate nohighlight">\(\M^K\)</span>, then predicts the class for <span class="math notranslate nohighlight">\(\x\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\M^K(\x)=\arg\max_{c_j}\{v_j(\x)|j=1,\cds,k\}\]</div>
<p>In the case of binary classification, with classes <span class="math notranslate nohighlight">\(\{+1,-1\}\)</span>, the
combined classifier <span class="math notranslate nohighlight">\(\M^K\)</span> can be expressed more simply as</p>
<div class="math notranslate nohighlight">
\[\M^K(\x)=\rm{sign}\bigg(\sum_{t=1}^K\alpha_tM_t(\x)\bigg)\]</div>
<p><strong>Bagging as a Special Case of AdaBoost</strong></p>
<p>Bagging can be considered as a special case of AdaBoost, where
<span class="math notranslate nohighlight">\(w_t=\frac{1}{n}\1\)</span>, and <span class="math notranslate nohighlight">\(\alpha_t=1\)</span> for all <span class="math notranslate nohighlight">\(K\)</span> iterations.</p>
</div>
<div class="section" id="stacking">
<h3>22.4.4 Stacking<a class="headerlink" href="#stacking" title="Permalink to this headline">Â¶</a></h3>
<p>Stacking or staced generalization is an ensemble technique where we employ two layers of classifiers.
The first layer is composed of <span class="math notranslate nohighlight">\(K\)</span> base classifiers which are trained
independently on the entire training data <span class="math notranslate nohighlight">\(\D\)</span>.
However, the base classiï¬ers should differ from or be complementary to each
other as much as possible so that they perform well on different subsets of the
input space.
The second layer comprises a combiner classiï¬er <span class="math notranslate nohighlight">\(C\)</span> that is trained on the
predicted classes from the base classiï¬ers, so that it automatically learns how
to combine the outputs of the base classiï¬ers to make the ï¬nal prediction for a
given input.</p>
<img alt="../_images/Algo22.7.png" src="../_images/Algo22.7.png" />
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../part5/index5.html" class="btn btn-neutral float-right" title="Part 5 Regression" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap21.html" class="btn btn-neutral float-left" title="Chapter 21 Support Vector Machines" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>