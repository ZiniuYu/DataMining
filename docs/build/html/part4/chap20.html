

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 20 Linear Discriminant Analysis &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 21 Support Vector Machines" href="chap21.html" />
    <link rel="prev" title="Chapter 19 Decision Tree Classifier" href="chap19.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index4.html">Part 4 Classification</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap18.html">Chapter 18 Probabilistic Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap19.html">Chapter 19 Decision Tree Classifier</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 20 Linear Discriminant Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#optimal-linear-discriminant">20.1 Optimal Linear Discriminant</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-discriminant-analysis">20.2 Kernel Discriminant Analysis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap21.html">Chapter 21 Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap22.html">Chapter 22 Classification Assessment</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../part5/index5.html">Part 5 Regression</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index4.html">Part 4 Classification</a> &raquo;</li>
        
      <li>Chapter 20 Linear Discriminant Analysis</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part4/chap20.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\H}{\boldsymbol{\mathrm{H}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}
\newcommand{\Z}{\boldsymbol{\mathrm{Z}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cE}{\mathcal{E}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-20-linear-discriminant-analysis">
<h1>Chapter 20 Linear Discriminant Analysis<a class="headerlink" href="#chapter-20-linear-discriminant-analysis" title="Permalink to this headline">¶</a></h1>
<p>Given labeled data consisting of <span class="math notranslate nohighlight">\(d\)</span>-dimensional points <span class="math notranslate nohighlight">\(\x_i\)</span> along
with their classes <span class="math notranslate nohighlight">\(y_i\)</span>, the goal of linear discriminant analysis (LDA)
is to find a vector <span class="math notranslate nohighlight">\(\w\)</span> that maximizes the separation between the classes
after projection onto <span class="math notranslate nohighlight">\(\w\)</span>.</p>
<div class="section" id="optimal-linear-discriminant">
<h2>20.1 Optimal Linear Discriminant<a class="headerlink" href="#optimal-linear-discriminant" title="Permalink to this headline">¶</a></h2>
<p>Let us assume that the dataset <span class="math notranslate nohighlight">\(\D\)</span> consists of <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span>, with the corresponding class label
<span class="math notranslate nohighlight">\(y_i\in\{c_1,c_2,\cds,c_k\}\)</span>.
Let <span class="math notranslate nohighlight">\(\D_i\)</span> denote the subset of points labeled with class <span class="math notranslate nohighlight">\(c_i\)</span>,
i.e., <span class="math notranslate nohighlight">\(\D_i=\{\x_j^T|y_j=c_i\}\)</span>, and let <span class="math notranslate nohighlight">\(|\D_i|=n_i\)</span> denote the
number of points with class <span class="math notranslate nohighlight">\(c_i\)</span>.
We assume that there are only <span class="math notranslate nohighlight">\(k=2\)</span> classes.
Thus, the dataset <span class="math notranslate nohighlight">\(\D\)</span> can be partitioned into <span class="math notranslate nohighlight">\(\D_1\)</span> and <span class="math notranslate nohighlight">\(\D_2\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\w\)</span> be a unit vector, that is, <span class="math notranslate nohighlight">\(\w^T\w=1\)</span>.
The projection of any <span class="math notranslate nohighlight">\(d\)</span>-dimensional point <span class="math notranslate nohighlight">\(\x_i\)</span> onto the vector <span class="math notranslate nohighlight">\(\w\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\x_i\pr=\bigg(\frac{\w^T\x_i}{\w^T\w}\bigg)\w=(\w^T\x_i)\w=a_i\w\]</div>
<p>where <span class="math notranslate nohighlight">\(a_i\)</span> is the offset or scalar projection of <span class="math notranslate nohighlight">\(\x_i\)</span> on the line <span class="math notranslate nohighlight">\(\w\)</span>:</p>
<div class="math notranslate nohighlight">
\[a_i=\w^T\x_i\]</div>
<p>We also call <span class="math notranslate nohighlight">\(a_i\)</span> a <em>projected point</em>.
Thus the set of <span class="math notranslate nohighlight">\(n\)</span> projected points <span class="math notranslate nohighlight">\(\{a_1,a_2,\cds,a_n\}\)</span>
represents a mapping from <span class="math notranslate nohighlight">\(\R^d\)</span> to <span class="math notranslate nohighlight">\(\R\)</span>, that is, from the original
<span class="math notranslate nohighlight">\(d\)</span>-dimensional space to a 1-dimensional space of offsets along
<span class="math notranslate nohighlight">\(\w\)</span>.</p>
<p>Each projected point <span class="math notranslate nohighlight">\(a_i\)</span> has associated with it the original class label
<span class="math notranslate nohighlight">\(y_i\)</span>, and thus we can compute, for each of the two classes, the mean of
the projected points ,called the <em>projected mean</em>, as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}m_1&amp;=\frac{1}{n_1}\sum_{\x_i\in\D_1}a_i\\&amp;=\frac{1}{n_1}\sum_{\x_i\in\D_1}\w^T\x_i\\&amp;=\w^T\bigg(\frac{1}{n_1}\sum_{\x_i\in\D_1}\x_i\bigg)\\&amp;=\w^T\mmu_1\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\mmu_1\)</span> is the mean of all point in <span class="math notranslate nohighlight">\(\D_1\)</span>.
Likewise, we can obtain</p>
<div class="math notranslate nohighlight">
\[m_2=\w^T\mmu_2\]</div>
<p>To maximize the separation between the classes, it seems reasonable to maximize
the difference between the projected means, <span class="math notranslate nohighlight">\(|m_1-m_2|\)</span>.
For good separation, the variance of the projected points for each class should also not be too large.
LDA maximizes the separation by ensuring that the <em>scatter</em> <span class="math notranslate nohighlight">\(s_i^2\)</span> for
the projected points within each class is small</p>
<div class="math notranslate nohighlight">
\[s_i^2=\sum_{\x_j\in\D_i}(a_j-m_i)^2=n_i\sg_i^2\]</div>
<p>We can incorporate the two LDA criteria, namely, maximizing the distance between
projected means and minimizing the sum of projected scatter, into a single
maximization criterion called the <em>Fisher LDA objective</em>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\max_\w J(\w)\frac{(m_1-m_2)^2}{s_1^2+s_2^2}\)</span></p>
</div>
<p>The vector <span class="math notranslate nohighlight">\(\w\)</span> is also called the <em>optimal linear discriminant (LD)</em>.</p>
<p>We can rewrite <span class="math notranslate nohighlight">\((m_1-m_2)^2\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}(m_1-m_2)^2&amp;=(\w^T(\mmu_1-\mmu_2))^2\\&amp;=\w^T((\mmu_1-\mmu_2)(\mmu_1-\mmu_2)^T)\w\\&amp;=\w^T\B\w\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\B=(\mmu_1-\mmu_2)(\mmu_1-\mmu_2)^T\)</span> is a <span class="math notranslate nohighlight">\(d\times d\)</span>
rank-one matrix called the <em>between-class scatter matrix</em>.</p>
<p>As for the projected scatter for class <span class="math notranslate nohighlight">\(c_1\)</span>, we can compute it as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}s_1^2&amp;=\sum_{\x_i\in\D_1}(a_i-m_1)^2\\&amp;=\sum_{\x_i\in\D_1}(\w^T\x_i-\w^T\mmu_1)^2\\&amp;=\sum_{\x_i\in\D_1}(\w^T(\x_i-\mmu_1))^2\\&amp;=\w^T\bigg(\sum_{\x_i\in\D_1}(\x_i-\mmu_1)(\x_i-\mmu_1)^T\bigg)\w\\&amp;=\w^T\S_1\w\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\S_1\)</span> is the <em>scatter matrix</em> for <span class="math notranslate nohighlight">\(\D_1\)</span>.
Likewise, we can obtain</p>
<div class="math notranslate nohighlight">
\[s_2^2=\w^T\S_2\w\]</div>
<p>Notice again that the scatter matrix is essentially the same as the covariance
matrix, but instead of recording the average deviation from the mean, it records
the total deviation, that is,</p>
<div class="math notranslate nohighlight">
\[\S_i=n_i\Sg_i\]</div>
<div class="math notranslate nohighlight">
\[s_1^2+s_2^2=\w^T\S_1\w+\w^T\S_2\w=\w^T(\S_1+\S_2)\w=\w^T\S\w\]</div>
<p>where <span class="math notranslate nohighlight">\(\S=\S_1+\S_2\)</span> denote the <em>within-class scatter matrix</em> for the pooled data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\max_\w J(\w)=\frac{\w^T\B\w}{\w^T\S\w}\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[\frac{d}{d\w}J(\w)=\frac{2\B\w(\w^T\S\w)-2\B\w(\w^T\B\w)}{(\w^T\S\w)^2}=\0\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\B\w(\w^T\S\w)&amp;=\S\w(\w^T\B\w)\\\B\w&amp;=\S\w\bigg(\frac{\w^T\B\w}{\w^T\S\w}\bigg)\\\B\w&amp;=J(\w)\S\w\\\B\w&amp;=\ld\S\w\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\ld=J(\w)\)</span>.
If <span class="math notranslate nohighlight">\(\S\)</span> is <em>nonsingular</em>, that is, if <span class="math notranslate nohighlight">\(\S\im\)</span> exists</p>
<div class="math notranslate nohighlight">
\[\S\im\B\w=\ld\S\im\S\w\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\((\S\im\B)\w=\ld\w\)</span></p>
</div>
<p>Thus, if <span class="math notranslate nohighlight">\(\S\im\)</span> exists, then <span class="math notranslate nohighlight">\(\ld=J(\w)\)</span> is an eigenvalue, and
<span class="math notranslate nohighlight">\(\w\)</span> is an eigenvector of the matrix <span class="math notranslate nohighlight">\(\S\im\B\)</span>.
To maximize <span class="math notranslate nohighlight">\(J(\w)\)</span> we look for the largest eigenvalue <span class="math notranslate nohighlight">\(\ld\)</span>, and
the coresponding dominant eigenvector <span class="math notranslate nohighlight">\(\w\)</span> specifies the best linear
discriminant vector.</p>
<img alt="../_images/Algo20.1.png" src="../_images/Algo20.1.png" />
<p>The total time complexity is <span class="math notranslate nohighlight">\(O(d^3+nd^2)\)</span>.</p>
<p>For the two class scenario, if <span class="math notranslate nohighlight">\(\S\)</span> is nonsingular, we can directly solve
for <span class="math notranslate nohighlight">\(\w\)</span> without computing the eigenvalues and eigenvectors.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\B\w&amp;=((\mmu_1-\mmu_2)(\mmu_1-\mmu_2)^T)\w\\&amp;=(\mmu_1-\mmu_2)((\mmu_1-\mmu_2)^T\w)\\&amp;=b(\mmu_1-\mmu_2)\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(n=(\mmu_1-\mmu_2)^T\w\)</span> is just a scalar multiplier.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\B\w&amp;=\ld\S\w\\b(\mmu_1-\mmu_2)&amp;=\ld\S\w\\\w&amp;=\frac{b}{\ld}\S\im(\mmu_1-\mmu_2)\end{aligned}\end{align} \]</div>
<p>Because <span class="math notranslate nohighlight">\(\frac{b}{\ld}\)</span> is just a scalar, we can solve for the best linear discriminant as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\w=\S\im(\mmu_1-\mmu_2)\)</span></p>
</div>
</div>
<div class="section" id="kernel-discriminant-analysis">
<h2>20.2 Kernel Discriminant Analysis<a class="headerlink" href="#kernel-discriminant-analysis" title="Permalink to this headline">¶</a></h2>
<p>The goal of kernel LDA is to find the direction vector <span class="math notranslate nohighlight">\(\w\)</span> in feature space that maximizes</p>
<div class="math notranslate nohighlight">
\[\max_\w J(\w)=\frac{(m_1-m_2)^2}{s_1^2+s_2^2}\]</div>
<p><strong>Optimal LD: Linear Combination of Feature Points</strong></p>
<p>The mean for class <span class="math notranslate nohighlight">\(c_i\)</span> in feature space is given as</p>
<div class="math notranslate nohighlight">
\[\mmu_i^\phi=\frac{1}{n_i}\sum_{\x_j\in\D_i}\phi(\x_j)\]</div>
<p>and the covariance matrix for class <span class="math notranslate nohighlight">\(c_i\)</span> in feature space is</p>
<div class="math notranslate nohighlight">
\[\Sg_i^\phi=\frac{1}{n_i}\sum_{\x_j\in\D_i}(\phi(\x_j)-\mmu_i^\phi)(\phi(\x_j)-\mmu_I^\phi)^T\]</div>
<p>The between-class and within-class scatter matrices are defined as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\B_\phi=(\mmu_1^\phi-\mmu_2^\phi)(\mmu_1^\phi-\mmu_2^\phi)^T=\d_\phi\d_\phi^T\\\S_\phi=n_1\Sg_1^\phi+n_2\Sg_2^\phi\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\((\S_\phi\im\B_\phi)\w=\ld\w\)</span></p>
</div>
<p>where we assume that <span class="math notranslate nohighlight">\(\S_\phi\)</span> is non-singular.
Let <span class="math notranslate nohighlight">\(\delta_i\)</span> denote the <span class="math notranslate nohighlight">\(i\)</span>th eigenvalue and <span class="math notranslate nohighlight">\(\u_i\)</span> the
<span class="math notranslate nohighlight">\(i\)</span>th eigenvector of <span class="math notranslate nohighlight">\(\S_\phi\)</span>, for <span class="math notranslate nohighlight">\(i=1,\cds,d\)</span>.
The eigen-decomposition of <span class="math notranslate nohighlight">\(\S_\phi\)</span> yields <span class="math notranslate nohighlight">\(\S_\phi=\U\Delta\U^T\)</span>,
with the inverse of <span class="math notranslate nohighlight">\(\S_\phi\)</span> given as <span class="math notranslate nohighlight">\(\S_\phi\im=\U\Delta\im\U^T\)</span>.
Here <span class="math notranslate nohighlight">\(\U\)</span> is the matrix whose columns are the eigenvectors of
<span class="math notranslate nohighlight">\(\S_\phi\)</span> and <span class="math notranslate nohighlight">\(\Delta\)</span> is the diagonal matrix of eigenvalues of
<span class="math notranslate nohighlight">\(\S_\phi\)</span>.
The inverse <span class="math notranslate nohighlight">\(\S_\phi\im\)</span> can thus be expressed as the spectral sum</p>
<div class="math notranslate nohighlight">
\[\S_\phi\im=\sum_{r=1}^d\frac{1}{\delta_r}\u_r\u_r^T\]</div>
<div class="math notranslate nohighlight">
\[\ld\w=\bigg(\sum_{r=1}^d\frac{1}{\delta_r}\u_r\u_r^T\bigg)\d_\phi\d_\phi^T\w
=\sum_{r=1}^d\frac{1}{\delta_r}(\u_r(\u_r^T\d_\phi)(\d_\phi^T\w))=
\sum_{r=1}^db_r\u_r\]</div>
<p>where <span class="math notranslate nohighlight">\(b_r=\frac{1}{\delta_r}(\u_r^T\d_\phi)(\d_\phi^T\w)\)</span> is a scalar value.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\w&amp;=\frac{1}{\ld}\sum_{r=1}^db_r\bigg(\sum_{j=1}^nc_{rj}\phi(\x_j)\bigg)\\&amp;=\sum_{j=1}^n\phi(\x_j)\bigg(\sum_{r=1}^d\frac{b_rc_{rj}}{\ld}\bigg)\\&amp;=\sum_{j=1}^na_j\phi(\x_j)\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(a_j=\sum_{r=1}^db_rc_{rj}/\ld\)</span> is a scalar value for the feature point <span class="math notranslate nohighlight">\(\phi(\x_j)\)</span>.
Therefore, the direction vector <span class="math notranslate nohighlight">\(\w\)</span> can be expressed as a linear combination of the points in feature space.</p>
<p><strong>LDA Objective via Kernel Matrix</strong></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}m_i=\w^T\mmu_i^\phi&amp;=\bigg(\sum_{j=1}^na_j\phi(\x_j)\bigg)^T\bigg(\frac{1}{n_i}\sum_{\x_k\in\D_i}\phi(\x_k)\bigg)\\&amp;=\frac{1}{n_i}\sum_{j=1}^n\sum_{\x_k\in\D_i}a_j\phi(\x_j)^T\phi(\x_k)\\&amp;=\frac{1}{n_i}\sum_{j=1}^n\sum_{\x_k\in\D_i}a_jK(\x_j,\x_k)\\&amp;=\a^T\m_i\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\a=(a_1,a_2,\cds,a_m)^T\)</span> is the weight vector, and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\m_i=\frac{1}{n_i}\bp \sum_{\x_k\in\D_i}K(\x_1,\x_k)\\
\sum_{\x_k\in\D_i}K(\x_2,\x_k)\\\vds\\\sum_{\x_k\in\D_i}K(\x_n,\x_k)\ep=
\frac{1}{n_i}\K^{c_i}\1_{n_i}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}(m_1-m_2)^2&amp;=(\w^T\mmu_1^\phi-\w^T\mmu_2^\phi)^2\\&amp;=(\a^T\m_1-\a^T\m_2)^2\\&amp;=\a^T(\m_1-\m_2)(\m_1-\m_2)^T\a\\&amp;=\a^T\M\a\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}s_1^2&amp;=\sum_{\x_i\in\D_1}\lv\w^T\phi(\x_i)-\w^T\mmu_1^\phi\rv^2\\&amp;=\sum_{\x_i\in\D_1}\lv\w^T\phi(\x_i)\rv^2-2\sum_{\x_i\in\D_1}
\w^T\phi(\x_i)\cd\w^T\mmu_1^\phi+\sum_{\x_i\in\D_1}\lv\w^T\mmu_1^\phi\rv^2\\&amp;=\bigg(\sum_{\x_i\in\D_1}\lv\sum_{j=1}^na_j\phi(\x_j)^T
\phi(\x_i)\rv^2\bigg)-2\cd n_1\cd\lv\w^T\mmu_1^\phi\rv^2+
n_1\cd\lv\w^T\mmu_1^\phi\rv^2\\&amp;=\bigg(\sum_{\x_i\in\D_1}\a^T\K_i\K_i^T\a\bigg)-n_1\cd\a^T\m_1\m_1^T\a\\&amp;=\a^T\bigg(\bigg(\sum_{\x_i\in\D_1}\K_i\K_i^T\bigg)-n_1\m_1\m_1^T\bigg)\a\\&amp;=\a^T\N_1\a\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\N_1&amp;=\bigg(\sum_{\x_i\in\D_1}\K_i\K_i^T\bigg)-n_1\m_1\m_1^T\\&amp;=(\K^{c_1})\bigg(\I_{n_1}-\frac{1}{n_1}\1_{n_1\times n_1}\bigg)(\K^{c_1})^T\end{aligned}\end{align} \]</div>
<p>In a similar manner we get <span class="math notranslate nohighlight">\(s_2^2=\a^T\N_2\a\)</span>.</p>
<div class="math notranslate nohighlight">
\[s_1^2+s_2^2=\a^T(\N_1+\N_2)\a=\a^T\N\a\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\max_\w J(\w)=\max_\a J(\a)=\frac{\a^T\M\a}{\a^\N\a}\)</span></p>
</div>
<p>The weight vector <span class="math notranslate nohighlight">\(\a\)</span> is the eigenvector corresponding to the largest
eigenvalue of the generalized eigenvalue problem:</p>
<div class="math notranslate nohighlight">
\[\M\a=\ld_1\N\a\]</div>
<p>If <span class="math notranslate nohighlight">\(\N\)</span> is nonsingular, <span class="math notranslate nohighlight">\(\a\)</span> is the dominant eigenvetor
corresponding to the largest eigenvalue for the system</p>
<div class="math notranslate nohighlight">
\[(\N\im\M)\a=\ld_1\a\]</div>
<p>As in the case of linear discriminant analysis, when there are only two classes
we do not have to solve for the eigenvector because <span class="math notranslate nohighlight">\(\a\)</span> can be obtained
directly:</p>
<div class="math notranslate nohighlight">
\[\a=\N\im(\m_1-\m_2)\]</div>
<p>we can ensure that <span class="math notranslate nohighlight">\(\w\)</span> is a unit vector if we scale <span class="math notranslate nohighlight">\(\a\)</span> by <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{\a^T\K\a}}\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\w^T\phi(\x)=\sum_{j=1}^na_j\phi(\x_j)^T\phi(\x)=\sum_{j=1}^na_jK(\x_j,\x)\)</span></p>
</div>
<img alt="../_images/Algo20.2.png" src="../_images/Algo20.2.png" />
<p>The complexity of kernel discriminant analysis is <span class="math notranslate nohighlight">\(O(n^3\)</span>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap21.html" class="btn btn-neutral float-right" title="Chapter 21 Support Vector Machines" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap19.html" class="btn btn-neutral float-left" title="Chapter 19 Decision Tree Classifier" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>