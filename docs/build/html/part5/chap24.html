

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 24 Logistic Regression &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 25 Neural Networks" href="chap25.html" />
    <link rel="prev" title="Chapter 23 Linear Regression" href="chap23.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index5.html">Part 5 Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap23.html">Chapter 23 Linear Regression</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 24 Logistic Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#binary-logistic-regression">24.1 Binary Logistic Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#maximum-likelihood-estimation">24.1.1 Maximum Likelihood Estimation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multiclass-logistic-regression">24.2 Multiclass Logistic Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">24.2.1 Maximum Likelihood Estimation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap25.html">Chapter 25 Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap26.html">Chapter 26 Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap27.html">Chapter 27 Regression Evaluation</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index5.html">Part 5 Regression</a> &raquo;</li>
        
      <li>Chapter 24 Logistic Regression</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part5/chap24.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{\mathrm{C}}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\H}{\boldsymbol{\mathrm{H}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}
\newcommand{\Z}{\boldsymbol{\mathrm{Z}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cE}{\mathcal{E}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-24-logistic-regression">
<h1>Chapter 24 Logistic Regression<a class="headerlink" href="#chapter-24-logistic-regression" title="Permalink to this headline">¶</a></h1>
<p>Given a set of predictor attributes or independent variables
<span class="math notranslate nohighlight">\(X_1,X_2,\cds,X_d\)</span>, and given a <em>categorical</em> response or dependent
variable <span class="math notranslate nohighlight">\(Y\)</span>, the aim of <em>logistic regression</em> is to predict the
probability of the response variable values based on the independent variables.
Logistic regression is in fact a classification technique, that given a point
<span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span> predicts <span class="math notranslate nohighlight">\(P(c_i|\x_j)\)</span> for each class <span class="math notranslate nohighlight">\(c_i\)</span> in
the domain of <span class="math notranslate nohighlight">\(Y\)</span> (the set of possible classes or values for the response
variable).</p>
<div class="section" id="binary-logistic-regression">
<h2>24.1 Binary Logistic Regression<a class="headerlink" href="#binary-logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>In logistic regression, we are given a set of <span class="math notranslate nohighlight">\(d\)</span> predictor or independent
variables <span class="math notranslate nohighlight">\(X_1,X_2,\cds,X_d\)</span>, and a <em>binary</em> or <em>Bernoulli</em> response
variable <span class="math notranslate nohighlight">\(Y\)</span> that takes on only two values, namely, 0 and 1.
Thus, we are given a training dataset <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span> and the corresponding observed values <span class="math notranslate nohighlight">\(y_i\in\{0,1\}\)</span>.
We augment the data matrix <span class="math notranslate nohighlight">\(\D\)</span> by adding a new attribute <span class="math notranslate nohighlight">\(X_0\)</span> that
is always fixed at the value 1 for each point, so that
<span class="math notranslate nohighlight">\(\td{\x_i}=(1,x_1,x_2,\cds,x_d)^T\in\R^{d+1}\)</span> denotes the augmented point,
and the multivariate random vector <span class="math notranslate nohighlight">\(\td\X\)</span>, comprising all the independent
attributes is given as <span class="math notranslate nohighlight">\(\td\X=(X_0,X_1,\cds,X_d)^T\)</span>.
The augmented training dataset is given as <span class="math notranslate nohighlight">\(\td\D\)</span> comprising the
<span class="math notranslate nohighlight">\(n\)</span> augmented points <span class="math notranslate nohighlight">\(\td{\x_i}\)</span> along with the class labels
<span class="math notranslate nohighlight">\(y_i\)</span> for <span class="math notranslate nohighlight">\(i=1,2,\cds,n\)</span>.</p>
<p>Since there are only two outcomes for the response variable <span class="math notranslate nohighlight">\(Y\)</span>, its
probability mass function for <span class="math notranslate nohighlight">\(\td\X=\td\x\)</span> is given as:</p>
<div class="math notranslate nohighlight">
\[P(Y=1|\td\X=\td\x)=\pi(\td\x)\quad\quad P(Y=0|\td\X=\td\x)=1-\pi(\td\x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi(\td\x)\)</span> is the unknown true parameter value, denoting the
probability of <span class="math notranslate nohighlight">\(Y=1\)</span> given <span class="math notranslate nohighlight">\(\td\X=\td\x\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}E[Y|\td\X=\td\x]&amp;=1\cd P(Y=1|\td\X=\td\x)+0\cd P(Y=0|\td\X=\td\x)\\&amp;=P(Y=1|\td\X=\td\x)=\pi(\td\x)\end{aligned}\end{align} \]</div>
<p>Therefore, in logistic regression, instead of directly predicting the response
value, the goal is to learn the probability, <span class="math notranslate nohighlight">\(P(Y=1|\td\X=\td\x)\)</span>, which
is also the expected value of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(\td\X=\td\x\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(P(Y=1|\td\X=\td\x)\)</span> is a probability, it is <strong>not appropriate</strong> to directly use the linear regression model</p>
<div class="math notranslate nohighlight">
\[f(\td\x)=\omega_0\cd x_0+\omega_1\cd x_1+\omega_2\cd x_2+\cds+\omega_d\cd x_d=\td{\bs\omega}^T\td\x\]</div>
<p>where <span class="math notranslate nohighlight">\(\td{\bs\omega}=(\omega_0,\omega_1,\cds,\omega_d)^T\in\R^{d+1}\)</span> is
the true augmented weight vector, with <span class="math notranslate nohighlight">\(\omega_0=\beta\)</span> the true unknown
bias term, and <span class="math notranslate nohighlight">\(\omega_i\)</span> the true unknown regression coefficient or
weight for attribute <span class="math notranslate nohighlight">\(X_i\)</span>.
The reason we cannot simply use <span class="math notranslate nohighlight">\(P(Y=1|\td\X=\td\x)=f(\td\x)\)</span> is due to
the fact that <span class="math notranslate nohighlight">\(f(\td\x)\)</span> can be arbitrarily large or arbitrarily small,
whereas for logistic regression, we require that the output represents a
probability value, and thus we need a model that results in an output that lies
in the interval <span class="math notranslate nohighlight">\([0,1]\)</span>.
The name “logistic regression” comes from the <em>logstic</em> function (also called
the <em>sigmoid</em> function) that meets this requirement.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\th(z)=\frac{1}{1+\exp\{-z\}}=\frac{\exp\{z\}}{1+\exp\{z\}}\)</span></p>
</div>
<p>The logstic function “squashes” the output to be between 0 and 1 for any scalar input <span class="math notranslate nohighlight">\(z\)</span>.</p>
<div class="math notranslate nohighlight">
\[1-\th(z)=1-\frac{\exp\{z\}}{1+\exp\{z\}}=\frac{1+\exp\{z\}-\exp\{z\}}{1+\exp\{z\}}=\frac{1}{1+\exp\{z\}}=\th(-z)\]</div>
<p>Using the logistic function, we define the logistic regression model as follows:</p>
<div class="math notranslate nohighlight">
\[P(Y=1|\td\X=\td\x)=\pi(\td\x)=\th(f(\td\x))=\th(\td{\bs\omega}^T\td\x)-
\frac{\exp\{\td{\bs\omega}^T\td\x\}}{1+\exp\{\td{\bs\omega}^T\td\x\}}\]</div>
<p>On the other hand, the probability for <span class="math notranslate nohighlight">\(Y=0\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[P(Y=0|\td\X=\td\x)=1-P(Y=1|\td\X=\td\x)=\th(-\td{\bs\omega}^T\td\x)=\frac{1}{1+\exp\{\td{\bs\omega}^T\td\x\}}\]</div>
<p>Combining these two cases the full logistic regression model is given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(P(Y|\td\X=\td\x)=\th(\td{\bs\omega}^T\td\x)^Y\cd\th(-\td{\bs\omega}^T\td\x)^{1-Y}\)</span></p>
</div>
<p><strong>Log-Odds Ratio</strong></p>
<p>Define the <em>odds ratio</em> for the occurence of <span class="math notranslate nohighlight">\(Y=1\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\rm{odds}(Y=1|\td\X=\td\x)&amp;=\frac{P(Y=1|\td\X=\td\x)}{P(Y=0|\td\X=\td\x)}=
\frac{\th(\td{\bs\omega}^T\td\x)}{\th(-\td{\bs\omega}^T\td\x)}\\&amp;=\frac{\exp\{\td{\bs\omega}^T\td\x\}}{1+\exp\{\td{\bs\omega}^T\td\x\}}\cd(1+\exp\{\td{\bs\omega}^T\td\x\})\\&amp;=\exp\{\td{\bs\omega}^T\td\x\}\end{aligned}\end{align} \]</div>
<p>The logarithm of the odds ratio, called the <em>log-odds ratio</em>, is therefore given as:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\ln(\rm{odds}(Y=1|\td\X=\td\x))&amp;=\ln\bigg(\frac{P(Y=1|\td\X=\td\x)}
{1-P(Y=1|\td\X=\td\x)}\bigg)=\ln(\exp\{\td{\bs\omega}^T\td\x\})=
\td{\bs\omega}^T\td\x\\&amp;=\omega_0\cd x_0+\omega_1\cd x_1+\cds+\omega_d\cd x_d\end{aligned}\end{align} \]</div>
<p>The log-odds ratio function is also called the <em>logit</em> function, defined as</p>
<div class="math notranslate nohighlight">
\[\rm{logit}(z)=\ln\bigg(\frac{z}{1-z}\bigg)\]</div>
<p>It is the inverse of the logistic function.
We can see that</p>
<div class="math notranslate nohighlight">
\[\ln(\rm{odds}(Y=1|\td\X=\td\x))=\rm{logit}(P(Y=1|\td\X=\td\x))\]</div>
<p>The logistic regression model is therefore based on the assumption that the log-
odds ratio for <span class="math notranslate nohighlight">\(Y=1\)</span> given <span class="math notranslate nohighlight">\(\td\X=\td\x\)</span> is a linear function (or a
weighted sum) of the independent attributes.
Let us consider the effect of attribute <span class="math notranslate nohighlight">\(X_i\)</span> by fixing the values for all other attributes; we get</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp;\quad\ \ \ln(\rm{odds}(Y=1|\td\X=\td\x))=\omega_i\cd x_i+C\\&amp;\Rightarrow\rm{odds}(Y=1|\td\X=\td\x)=\exp\{\omega_i\cd x_i+C\}=
\exp\{\omega_i\cd x_i\}\cd\exp\{C\}\propto\exp\{\omega_i\cd x_i\}\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is a constant comprising the fixed attributes.
The regression coefficient <span class="math notranslate nohighlight">\(\omega_i\)</span> can therefore be interpreted as the
change in the log-odds ratio for <span class="math notranslate nohighlight">\(Y=1\)</span> for a unit change in <span class="math notranslate nohighlight">\(X_i\)</span>,
or equivalently the odds ratio for <span class="math notranslate nohighlight">\(Y=1\)</span> increases exponentially per unit
change in <span class="math notranslate nohighlight">\(X_i\)</span>.</p>
<div class="section" id="maximum-likelihood-estimation">
<h3>24.1.1 Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\td\D\)</span> be the augmented training dataset comprising the <span class="math notranslate nohighlight">\(n\)</span>
augmented points <span class="math notranslate nohighlight">\(\td{\x_i}\)</span> along with their lables <span class="math notranslate nohighlight">\(y_i\)</span>.
Let <span class="math notranslate nohighlight">\(\td\w=(w_0,w_1,\cds,w_d)^T\)</span> be the augmented weight vector for estimating <span class="math notranslate nohighlight">\(\td\w\)</span>.
Note that <span class="math notranslate nohighlight">\(w_0=b\)</span> denotes the estimated bias term, and <span class="math notranslate nohighlight">\(w_i\)</span> the estimated weight for attribute <span class="math notranslate nohighlight">\(X_i\)</span>.
<em>Likelihood</em> is defined as the probability of the obaserved data given the estimated parameters <span class="math notranslate nohighlight">\(\td\w\)</span>.
We assume that the binary response variables <span class="math notranslate nohighlight">\(y_i\)</span> are all independent.
Threfore, the likelihood of the observed responses is given as</p>
<div class="math notranslate nohighlight">
\[L(\td\w)=P(Y|\td\w)=\prod_{i=1}^nP(y_i|\td{\x_i})=\prod_{i=1}^n
\th(\td\w^T\td{\x_i})^{y_i}\cd\th(-\td\w^T\td{\x_i})^{1-y_i}\]</div>
<p>Instead of trying to maximize the likelihood, we can maximize the logarithm of
the likelihood, called <em>log-likelihood</em>, to convert the product into a summation
as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\ln(L(\td\w))=\sum_{i=1}^ny_i\cd\ln(\th(\td\w^T\td{\x_i}))+(1-y_i)\cd\ln(\th(-\td\w^T\td{\x_i}))\)</span></p>
</div>
<p>The negative of the log-likelihood can also be considered as an error function,
the <em>cross-entropy error function</em>, given as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp E(\td\w)=-\ln(L(\td\w))=\sum_{i=1}^ny_i\cd\ln\bigg(\frac{1}{\th(\td\w^T\td{\x_i})}\bigg)\)</span>
<span class="math notranslate nohighlight">\(\dp(1-y_i)\cd\ln\bigg(\frac{1}{1-\th(\td\w^T\td{\x_i})}\bigg)\)</span></p>
</div>
<p>The task of maximizing the log-likelihood is therefore equivalent to minimizing the cross-entropy error.</p>
<p>We use an iterative <em>gradient ascent</em> method to compute the optimal value.
It can be obtained by taking its partial derivative with respect to <span class="math notranslate nohighlight">\(\td\w\)</span>.</p>
<div class="math notranslate nohighlight">
\[\nabla(\td\w)=\frac{\pd}{\pd\td\w}\{\ln(L(\td\w))\}=\frac{\pd}{\pd\td\w}
\bigg\{\sum_{i=1}^ny_i\cd\ln(\th(z_i))+(1-y_i)\cd\ln(\th(-z-i))\bigg\}\]</div>
<p>where <span class="math notranslate nohighlight">\(z_i=\td\w^T\td{\x_i}\)</span>.
We use the chain rule to obtain the derivative of <span class="math notranslate nohighlight">\(\ln(\th(z_i))\)</span> with respect to <span class="math notranslate nohighlight">\(\td\w\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\th(z_i)}\{\ln(\th(z_i))\}&amp;=\frac{1}{\th(z_i)}\\\frac{\pd}{\pd\th(z_i)}\{\ln(\th(-z_i))\}&amp;=\frac{\pd}{\pd\th(z_i)}\{\ln(1-\th(z_i))\}=\frac{-1}{1-\th(z_i)}\\\frac{\pd\th(z_i)}{\pd z_i}&amp;=\th(z_i)\cd(1-\th(z_i))=\th(z_i)\cd\th(-z_i)\\\frac{\pd z_i}{\pd\td\w}&amp;=\frac{\pd\td\w^T\td{\x_i}}{\pd\td\w}=\td{\x_i}\end{aligned}\end{align} \]</div>
<p>As per the chain rule, we have</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\ln(\th(z_i))}{\pd\td\w}&amp;=\frac{\pd\ln(\th(z_i))}{\pd\th(z_i)}\cd
\frac{\pd\th(z_i)}{\pd(z_i)}\cd\frac{\pd z_i}{\pd\td\w}\\&amp;=\frac{1}{1-\th(z_i)}\cd(\th(z_i)\cd\th(z_i))\cd\td{\x_i}=\th(-z_i)\cd\td{\x_i}\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\ln(\th(-z_i))}{\pd\td\w}&amp;=\frac{\pd\ln(\th(-z_i))}{\pd\th(z_i)}\cd
\frac{\pd\th(z_i)}{\pd(z_i)}\cd\frac{\pd z_i}{\pd\td\w}\\&amp;=\frac{-1}{1-\th(z_i)}\cd(\th(z_i)\cd(1-\th(z_i)))\cd\td{\x_i}=-\th(z_i)\cd\td{\x_i}\end{aligned}\end{align} \]</div>
<p>Substituting the above equations, we get</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla(\td\w)&amp;=\sum_{i=1}^ny_i\cd\th(-z_i)\cd\td{\x_i}-(1-y_i)\cd\th(z_i)\cd\td{\x_i}\\&amp;=\sum_{i=1}^ny_i\cd(\th(-z_i)+\th(z_i))\cd\td{\x_i}-\th(z_i)\cd\td{\x_i}\\&amp;=\sum_{i=1}^n(y_i-\th(z_i))\cd\td{\x_i}\\&amp;=\sum_{i=1}^n(y_i-\th(\td\w^T\td{\x_i}))\cd\td{\x_i}\end{aligned}\end{align} \]</div>
<p>Given the current estimate <span class="math notranslate nohighlight">\(\td\w^t\)</span>, we can obtain the next estimate as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\td\w^{t+1}=\td\w^t+\eta\cd\nabla(\td\w^t)\)</span></p>
</div>
<p>Here, <span class="math notranslate nohighlight">\(\eta&gt;0\)</span> is a user-specified parameter called the <em>learning rate</em>.
At the optimal value of <span class="math notranslate nohighlight">\(\td\w\)</span>, the gradient will be zero, <span class="math notranslate nohighlight">\(\nabla(\td\w)=\0\)</span>, as desired.</p>
<p><strong>Stochastic Gradient Ascent</strong></p>
<p>The gradient ascent method computes the gradient by considering all the data
points, and it is therefore called batch gradient ascent.
For large datasets, it is typically much faster to compute the gradient by
considering only one (randomly chosen) point at a time.
The weight vector is updated after each such partial gradient step, giving rise
to <em>stochastic gradient ascent</em> (SGA) for computing the optimal weight vector
<span class="math notranslate nohighlight">\(\td\w\)</span>.</p>
<p>Given a randomly chosen point <span class="math notranslate nohighlight">\(\td{\x_i}\)</span>, the point-specific gradient is given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\nabla(\td\w,\td{\x_i})=(y_i-\th(\td\w^T\td{\x_i}))\cd\td{\x_i}\)</span></p>
</div>
<img alt="../_images/Algo24.1.png" src="../_images/Algo24.1.png" />
<p>Once the model has been trained, we can predict the response for any new augmented test point <span class="math notranslate nohighlight">\(\td\z\)</span> as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat{y}=\left\{\begin{array}{lr}1\quad\rm{if\ }\th(\td\w^T\z)\geq 0.5\\0\quad\rm{if\ }\th(\td\w^T\z)&lt;0.5\end{array}\right.\)</span></p>
</div>
</div>
</div>
<div class="section" id="multiclass-logistic-regression">
<h2>24.2 Multiclass Logistic Regression<a class="headerlink" href="#multiclass-logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>We model <span class="math notranslate nohighlight">\(Y\)</span> as a <span class="math notranslate nohighlight">\(K\)</span>-dimensional multivariate Bernoulli random variable.
Since <span class="math notranslate nohighlight">\(Y\)</span> can assume only one of the <span class="math notranslate nohighlight">\(K\)</span> values, we use the
<em>one-hot encoding</em> approach to map each categorical value <span class="math notranslate nohighlight">\(c_i\)</span> to the
<span class="math notranslate nohighlight">\(K\)</span>-dimensional binary vector</p>
<div class="math notranslate nohighlight">
\[\e_i=(0,\cds,0,1,0,\cds,0)^T\]</div>
<p>whose <span class="math notranslate nohighlight">\(i\)</span>th element <span class="math notranslate nohighlight">\(e_{ii}=1\)</span>, and all other elements
<span class="math notranslate nohighlight">\(e_{ij}=0\)</span>, so that <span class="math notranslate nohighlight">\(\sum_{j=1}^Ke_{ij}=1\)</span>.
Henceforth, we assume that the categorical response variable <span class="math notranslate nohighlight">\(Y\)</span> is a
multivariate Bernoulli variable <span class="math notranslate nohighlight">\(\Y\in\{\e_1,\e_2,\cds,\e_K\}\)</span>,
with <span class="math notranslate nohighlight">\(Y_j\)</span> referring to the <span class="math notranslate nohighlight">\(j\)</span>th component of <span class="math notranslate nohighlight">\(\Y\)</span>.</p>
<p>The probability mass function for <span class="math notranslate nohighlight">\(\Y\)</span> given <span class="math notranslate nohighlight">\(\td\X=\td\x\)</span> is</p>
<div class="math notranslate nohighlight">
\[P(\Y=\e_i|\td\X=\td\x)=\pi_i(\td\x),\ \rm{for}\ i=1,2,\cds,K\]</div>
<p>Thus, there are <span class="math notranslate nohighlight">\(K\)</span> unknown parameters, which must satisfy the following constraint:</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^K\pi_i(\td\x)=\sum_{i=1}^KP(\Y=\e_i|\td\X=\td\x)=1\]</div>
<p>Given that only one element of <span class="math notranslate nohighlight">\(\Y\)</span> is 1, the probability mass function of <span class="math notranslate nohighlight">\(\Y\)</span> can be written compactly as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp P(\Y|\td\X=\td\x)=\prod_{j=1}^K(\pi_j(\td\x))^{Y_j}\)</span></p>
</div>
<p>The log-odds ratio of class <span class="math notranslate nohighlight">\(c_i\)</span> with respect to class <span class="math notranslate nohighlight">\(c_K\)</span> is assumed to satisfy</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\ln(\rm{odds}(\Y=\e_i|\td\X=\td\x))&amp;=\ln\bigg(\frac{P(\Y=\e_i|\td\X=\td\x)}
{P(\Y=\e_K|\td\X=\td\x)}\bigg)=\ln\bigg(\frac{\pi_i(\td\x)}{\pi_K(\td\x)}
\bigg)=\td{\bs\omega_i}^T\td\x\\&amp;=\omega_{i0}\cd x_0+\omega_{i1}\cd x_1+\cds+\omega_{id}\cd x_d\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\omega_{i0}=\beta_i\)</span> is the true bias value for class <span class="math notranslate nohighlight">\(c_i\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp;\quad\ \frac{\pi_i(\td\x)}{\pi_K(\td\x)}=\exp\{\td{\bs\omega_i}^T\td\x\}\\&amp;\Rightarrow\pi_i(\td\x)=\exp\{\td{\bs\omega_i}^T\td\x\}\cd\pi_K(\td\x)\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp;\quad \ \sum_{j=1}^K\pi_j(\td\x)=1\\&amp;\Rightarrow\bigg(\sum_{j\neq K}\exp\{\td{\bs\omega_j}^T\td\x\}\cd\pi_K(\td\x)\bigg)+\pi_K(\td\x)=1\\&amp;\Rightarrow\pi_K(\td\x)=\frac{1}{1+\sum_{j\neq K}\exp\{\td{\bs\omega_j}^T\td\x\}}\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\pi_i(\td\x)=\exp\{\td{\bs\omega_i}^T\td\x\}\cd\pi_K(\td\x)=\frac{\exp\{
\td{\bs\omega_i}^T\td\x\}}{1+\sum_{j\neq K}\exp\{\td{\bs\omega_j}^T\td\x\}}\]</div>
<p>Finally, setting <span class="math notranslate nohighlight">\(\td{\bs\omega_K}=\0\)</span>, we have
<span class="math notranslate nohighlight">\(\exp\{\td{\bs\omega_K}^T\td\x\}=1\)</span> and thus we can write the full model for
multiclass logistic regression as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\pi_i(\td\x)=\frac{\exp\{\td{\bs\omega_i}^T\td\x\}}{\sum_{j=1}^K\exp\{\td{\bs\omega_j}^T\td\x\}}\)</span>
<span class="math notranslate nohighlight">\(\ \rm{for\ all}\ i=1,2,\cds,K\)</span></p>
</div>
<p>This function is also called the <em>softmax</em> function.
When <span class="math notranslate nohighlight">\(K=2\)</span>, this formulation yields exactly the same model as in binary logistic regression.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\ln\bigg(\frac{\pi_i(\td\x)}{\pi_j(\td\x)}\bigg)&amp;=\ln\bigg(\frac{\pi_i
(\td\x)}{\pi_K(\td\x)}\cd\frac{\pi_K(\td\x)}{\pi_j(\td\x)}\bigg)\\&amp;=\ln\bigg(\frac{\pi_i(\td\x)}{\pi_K(\td\x)}\bigg)+
\ln\bigg(\frac{\pi_K(\td\x)}{\pi_j(\td\x)}\bigg)\\&amp;=ln\bigg(\frac{\pi_i(\td\x)}{\pi_K(\td\x)}\bigg)-
\ln\bigg(\frac{\pi_j(\td\x)}{\pi_K(\td\x)}\bigg)\\&amp;=\td{\bs\omega_i}^T\td\x-\td{\bs\omega_j}^T\td\x\\&amp;=(\td{\bs\omega_i}-\td{\bs\omega_j})^T\td\x\end{aligned}\end{align} \]</div>
<p>That is, the log-odds ratio between any two classes can be computed from the
difference of the corresponding weight vectors.</p>
<div class="section" id="id1">
<h3>24.2.1 Maximum Likelihood Estimation<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\td\D\)</span> be the augmented dataset comprising <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\td{\x_i}\)</span> and their labels <span class="math notranslate nohighlight">\(\y_i\)</span>.
We assume that <span class="math notranslate nohighlight">\(\y_i\)</span> is a one-hot encoded (multivariate Bernoulli)
response vector, so that <span class="math notranslate nohighlight">\(y_{ij}\)</span> denotes the <span class="math notranslate nohighlight">\(j\)</span>th element of <span class="math notranslate nohighlight">\(\y_i\)</span>.
Let <span class="math notranslate nohighlight">\(\td\w_i\in\R^{d+1}\)</span> denote the estimated augmented weight vector for
class <span class="math notranslate nohighlight">\(c_i\)</span>, with <span class="math notranslate nohighlight">\(w_{i0}=b_i\)</span> denoting the bias term.</p>
<p>To find the <span class="math notranslate nohighlight">\(K\)</span> sets of regression weight vectors <span class="math notranslate nohighlight">\(\td{\w_i}\)</span>, for
<span class="math notranslate nohighlight">\(i=1,2,\cds,K\)</span>, we use the gradient ascent approach to maximize the
log-likelihood function.
The likelihood of the data is given as</p>
<div class="math notranslate nohighlight">
\[L(\td\W)=P(\Y|\td\W)=\prod_{i=1}^nP(\y_i|\td\X=\td{\x_i})=\prod_{i=1}^n\prod_{j=1}^K(\pi_j(\td{\x_j}))^{y_{ij}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\td\W=\{\td{\w_1},\td{\w_2},\cds,\td{\w_K}\}\)</span> is the set of <span class="math notranslate nohighlight">\(K\)</span> weight vectors.
The log-likelihood is then given as:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\ln(L(\td\W))=\sum_{i=1}^n\sum_{j=1}^Ky_{ij}\cd\ln(\pi_j(\td{\x_i}))=\sum_{i=1}^n\sum_{j=1}^Ky_{ij}\cd\)</span>
<span class="math notranslate nohighlight">\(\dp\ln\bigg(\frac{\exp\{\td{\w_j}^T\td{\x_i}\}}{\sum_{a=1}^K\exp\{\td{\w_a}^T\td{\x_i}\}}\bigg)\)</span></p>
</div>
<p>Note that the negative of the log-likelihood function can be regarded as an
error function, commonly known as <em>cross-entropy error</em></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\pi_j(\td{\x_i})}\ln(\pi_j(\td{\x_i}))=\frac{1}{\pi_j(\td\x_i)}\\\begin{split}\frac{\pd}{\pd\td{\w_a}}\pi_j(\td{\x_i})=\left\{\begin{array}{lr}
\pi_a(\td{\x_i})\cd(1-\pi_a(\td{\x_i}))\cd\td{\x_i}\quad\rm{if}\ j=a\\
-\pi_a(\td{\x_i})\cd\pi_j(\td{\x_i})\cd\td{\x_i}\quad\quad\quad\rm{if}
\ j\neq a\end{array}\right.\end{split}\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla(\td{\w_a})&amp;=\frac{\pd}{\pd\td{\w_a}}\{\ln(L(\td\W))\}\\&amp;=\sum_{i=1}^n\sum_{j=1}^Ky_{ij}\cd\frac{\pd\ln(\pi_j(\td{\x_i}))}
{\pd\pi_j(\td{\x_i})}\cd\frac{\pd\pi_j(\td{\x_i})}{\pd\td{\w_a}}\\&amp;=\sum_{i=1}^n\bigg(y_{ia}\cd\frac{\pi_a(\td{\x_i})}{\pi_a(\td{\x_i})}\cd
(1-\pi_a(\td{\x_i}))\cd\td{\x_i}+\sum_{j\neq a}y_{ij}\cd\frac{(-\pi_a
(\td{\x_i})\cd\pi_j(\td{\x_i}))}{\pi_j(\td{\x_i})}\cd\td{\x_i}\bigg)\\&amp;=\sum_{i=1}^n\bigg(y_{ia}-y_{ia}\cd\pi_a(\td{\x_i})-\sum_{j\neq a}y_{ij}\cd\pi_a(\td{\x_i})\bigg)\cd\td{\x_i}\\&amp;=\sum_{i=1}^n\bigg(y_{ia}-\sum_{j=1}^Ky_{ij}\cd\pi_a(\td{\x_i})\bigg)\cd\td{\x_i}\\&amp;=\sum_{i=1}^n(y_{ia}-\pi_a(\td{\x_i}))\cd\td{\x_i}\end{aligned}\end{align} \]</div>
<p>For stochastic gradient ascent, we update the weight vectors by considering only one point at a time.
The gradient of the log-likelihood function with respect to <span class="math notranslate nohighlight">\(\td{\w_j}\)</span> at
a given point <span class="math notranslate nohighlight">\(\td{\x_i}\)</span> is given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\nabla(\td{\w_j},\td{\x_i})=(y_{ij}-\pi_j(\td{\x_i}))\cd\td{\x_i}\)</span></p>
</div>
<p>which results in the following update rule for the <span class="math notranslate nohighlight">\(j\)</span>th weight vector:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\td{\w_j}^{t+1}=\td{\w_j}^t+\eta\cd\nabla(\td{\w_j}^t,\td{\x_i})\)</span></p>
</div>
<img alt="../_images/Algo24.2.png" src="../_images/Algo24.2.png" />
<p>Once the model has been trained, we can predict the class for any new augmented test point <span class="math notranslate nohighlight">\(\td\z\)</span> as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat{y}=\arg\max_{c_i}\{\pi_i(\td\z)\}=\arg\max_{c_i}\)</span>
<span class="math notranslate nohighlight">\(\dp\bigg\{\frac{\exp\{\td{\w_i}^T\td\z\}}{\sum_{j=1}^K\exp\{\td{\w_j}^T\td\z\}}\bigg\}\)</span></p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap25.html" class="btn btn-neutral float-right" title="Chapter 25 Neural Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap23.html" class="btn btn-neutral float-left" title="Chapter 23 Linear Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>