

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 23 Linear Regression &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 24 Logistic Regression" href="chap24.html" />
    <link rel="prev" title="Part 5 Regression" href="index5.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index5.html">Part 5 Regression</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 23 Linear Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linear-regression-model">23.1 Linear Regression Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bivariate-regression">23.2 Bivariate Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#geometry-of-bivariate-regression">23.2.1 Geometry of Bivariate Regression</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-regression">22.3 Multiple Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#geometry-of-multiple-regression">23.3.1 Geometry of Multiple Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multiple-regression-algorithm">23.3.2 Multiple Regression Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multiple-regression-stochastic-gradient-descent">23.3.3 Multiple Regression: Stochastic Gradient Descent</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ridge-regression">23.4 Ridge Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ridge-regression-stochastic-gradient-descent">23.4.1 Ridge Regression: Stochastic Gradient Descent</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-regression">23.5 Kernel Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#l-1-regression-lasso">23.6 <span class="math notranslate nohighlight">\(L_1\)</span> Regression: Lasso</a></li>
<li class="toctree-l4"><a class="reference internal" href="#subgradients-and-subdifferential">23.6.1 Subgradients and Subdifferential</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bivariate-l-1-regression">23.6.2 Bivariate <span class="math notranslate nohighlight">\(L_1\)</span> Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multiple-l-1-regression">23.6.3 Multiple <span class="math notranslate nohighlight">\(L_1\)</span> Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap24.html">Chapter 24 Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap25.html">Chapter 25 Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap26.html">Chapter 26 Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap27.html">Chapter 27 Regression Evaluation</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index5.html">Part 5 Regression</a> &raquo;</li>
        
      <li>Chapter 23 Linear Regression</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part5/chap23.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\ra}{\rightarrow}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-23-linear-regression">
<h1>Chapter 23 Linear Regression<a class="headerlink" href="#chapter-23-linear-regression" title="Permalink to this headline">Â¶</a></h1>
<p>Given a set of attributes or variables <span class="math notranslate nohighlight">\(X_1,X_2,\cds,X_d\)</span>, called the
<em>predictor</em>, <em>explanatory</em>, or <em>independent</em> variables, and given a real-valued
attribute of interest <span class="math notranslate nohighlight">\(Y\)</span>, called the <em>response</em> or <em>dependent</em> variable,
the aim of <em>regression</em> is to predict the response variable based on the
independent variables.
That is, the goal is to learn a <em>regression function</em> <span class="math notranslate nohighlight">\(f\)</span>, such that</p>
<div class="math notranslate nohighlight">
\[Y=f(X_1,X_2,\cds,X_d)+\ve=f(\X)+\ve\]</div>
<p>where <span class="math notranslate nohighlight">\(\X=(X_1,X_2,\cds,X_d)^T\)</span> is the multivariate random variable
comprising the predictor attributes, and <span class="math notranslate nohighlight">\(\ve\)</span> is a random <em>error term</em>
that is assumed to be independent of <span class="math notranslate nohighlight">\(\X\)</span>.
In other words, <span class="math notranslate nohighlight">\(Y\)</span> is comrpised of two components, one dependent on the
observed predictor attributes, and the other, coming from the error term,
independent of the predictor attributes.
The error term encapsulates inherent uncertainty in <span class="math notranslate nohighlight">\(Y\)</span>, as well as,
possibly the effect of unobserved, hidden or <em>latent</em> variables.</p>
<div class="section" id="linear-regression-model">
<h2>23.1 Linear Regression Model<a class="headerlink" href="#linear-regression-model" title="Permalink to this headline">Â¶</a></h2>
<p>In <em>linear regression</em> the function <span class="math notranslate nohighlight">\(f\)</span> is assumed to be linear in its parameters, that is</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp f(\X)=\beta+\omega_1X_1+\omega_2X_2+\cds+\omega_dX_d=\beta+\sum_{i=1}^d\omega_iX_i=\beta+\bs{\omega}^T\X\)</span></p>
</div>
<p>Here, the parameter <span class="math notranslate nohighlight">\(\beta\)</span> is the true (unknown) <em>bias</em> term, the
parameter <span class="math notranslate nohighlight">\(\omega_i\)</span> is the true (unknown) <em>regression coefficient</em> or
<em>weight</em> for attribute <span class="math notranslate nohighlight">\(X_i\)</span>, and
<span class="math notranslate nohighlight">\(\bs{\omega}=(\omega_1,\omega_2,\cds,\omega_d)^T\)</span> is the true <span class="math notranslate nohighlight">\(d\)</span>-
dimensional weight vector.
Observe that <span class="math notranslate nohighlight">\(f\)</span> specifies a hyperplane in <span class="math notranslate nohighlight">\(\R^{d+1}\)</span>, where
<span class="math notranslate nohighlight">\(\bs{\omega}\)</span> is the weight vector that is normal or orthogonal to the
hyperplane, and <span class="math notranslate nohighlight">\(\beta\)</span> is the <em>intercept</em> or offset term.
We can see that <span class="math notranslate nohighlight">\(f\)</span> is completely specified by the <span class="math notranslate nohighlight">\(d+1\)</span> parameters
comprising <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\omega_i\)</span>, for <span class="math notranslate nohighlight">\(i=1,\cds,\d\)</span>.</p>
<p>The true bias and regression coefficients are unknown.
Therefore, we have to estimate them from the training dataset <span class="math notranslate nohighlight">\(\D\)</span>
comprising <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span> in a <span class="math notranslate nohighlight">\(d\)</span>-dimensional
space, and the corresponding response values <span class="math notranslate nohighlight">\(y_i\in\R\)</span>, for
<span class="math notranslate nohighlight">\(i=1,2,\cds,n\)</span>.
Let <span class="math notranslate nohighlight">\(b\)</span> denote the estimated value for the true bias <span class="math notranslate nohighlight">\(\beta\)</span>, and
let <span class="math notranslate nohighlight">\(w_i\)</span> denote the estimated value for the true regression coefficient
<span class="math notranslate nohighlight">\(w_i\)</span>, for <span class="math notranslate nohighlight">\(i=1,2,\cds,d\)</span>.
Let <span class="math notranslate nohighlight">\(\w=(w_1,w_2,\cds,w_d)^T\)</span> denote the vector of estimated weights.
Given the estimated bias and weight values, we can predict the response for any
given input or test point <span class="math notranslate nohighlight">\(\x=(x_1,x_2,\cds,x_d)^T\)</span>, as follows:</p>
<div class="math notranslate nohighlight">
\[\hat{y}=b+w_1x_1+\cds+w_dx_d=b+\w^T\x\]</div>
<p>The difference between the observed and predicted response, called the <em>residual error</em>, is given as</p>
<div class="math notranslate nohighlight">
\[\epsilon=y-\hat{y}=y-b-\w^T\X\]</div>
<p>The residual error <span class="math notranslate nohighlight">\(\epsilon\)</span> is an estimator of the random error term <span class="math notranslate nohighlight">\(\ve\)</span>.</p>
<p>A common approach to predicting the bias and regression coefficients is to use the method of <em>least squares</em>.
That is, given the training data <span class="math notranslate nohighlight">\(\D\)</span> with points <span class="math notranslate nohighlight">\(\x_i\)</span> and
response values <span class="math notranslate nohighlight">\(y_i\)</span> (for <span class="math notranslate nohighlight">\(i=1,\cds,n\)</span>), we seek values <span class="math notranslate nohighlight">\(b\)</span>
and <span class="math notranslate nohighlight">\(\w\)</span>, so as to minimize the sum of squared residual errors (SSE)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp SSE=\sum_{i=1}^n\epsilon_i^2=\sum_{i=1}^n(y_i-\hat{y_i})^2=\sum_{i=1}^n(y_i-b-\w^T\x_i)^2\)</span></p>
</div>
</div>
<div class="section" id="bivariate-regression">
<h2>23.2 Bivariate Regression<a class="headerlink" href="#bivariate-regression" title="Permalink to this headline">Â¶</a></h2>
<p>Let us first consider the case where the input data <span class="math notranslate nohighlight">\(\D\)</span> comprises a
single predictor attribute, <span class="math notranslate nohighlight">\(W=(x_1,x_2,\cds,x_n)^T\)</span>, along with the
response variable, <span class="math notranslate nohighlight">\(Y=(y_1,y_2,\cds,y_n)^T\)</span>.
Since <span class="math notranslate nohighlight">\(f\)</span> is linear, we have</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\hat{y_i}=f(x_i)=b+w\cd x_i\)</span></p>
</div>
<p>Thus, we seek the straight line <span class="math notranslate nohighlight">\(f(x)\)</span> with slope <span class="math notranslate nohighlight">\(w\)</span> and intercept <span class="math notranslate nohighlight">\(b\)</span> that <em>best fits</em> the data.
The residual error, which is the difference between the predicted value (also
called <em>fitted value</em>) and the observed value of the response variable, is given
as</p>
<div class="math notranslate nohighlight">
\[\epsilon_i=y_i-\hat{y_i}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(|\epsilon_i|\)</span> denotes the vertical distance between the fitted and observed response.
The best fitting line minimizes the sum of squared errors</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_{b,w}SSE=\sum_{i=1}^n\epsilon_i^2=\sum_{i=1}^n(y_i-\hat{y_i})^2=\sum_{i=1}^n(y_i-b-w\cd x_i)^2\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd b}SSE&amp;=-2\sum_{i=1}^n(y_i-b-w\cd x_i)=0\\&amp;\Rightarrow\sum_{i=1}^n b=\sum_{i=1}^ny_i-w\sum_{i=1}^nx_i\\&amp;\Rightarrow b=\frac{1}{n}\sum_{i=1}^ny_i-w\cd\frac{1}{n}\sum_{i=1}^nx_i\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(b=\mu_Y-w\cd\mu_X\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\mu_Y\)</span> is the sample mean for the response and <span class="math notranslate nohighlight">\(\mu_X\)</span> is the
sample mean for the predictor attribute.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd w}SSE&amp;=-2\sum_{i=1}^nx_i(y_i-b-w\cd x_i)=0\\&amp;\Rightarrow\sum_{i=1}^nx_i\cd y_i-b\sum_{i=1}^nx_i-w\sum_{i=1}^nx_i^2=0\\&amp;\Rightarrow\sum_{i=1}^nx_i\cd y_i-\mu_Y\sum_{i=1}^nx_i+w\cd\mu_X\sum_{i=1}^nx_i-w\sum_{i=1}^nx_i^2=0\\&amp;\Rightarrow w\bigg(\sum_{i=1}^nx_i^2-n\cd\mu_X^2\bigg)=\bigg(\sum_{i=1}^nx_i\cd y_i\bigg)-n\cd\mu_X\mu_Y\\&amp;\Rightarrow w=\frac{(\sum_{i=1}^nx_i\cd y_i)-n\cd\mu_X\cd\mu_Y}{(\sum_{i=1}^nx_i^2)-n\cd\mu_X^2}\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp w=\frac{\sum_{i=1}^n(x_i-\mu_X)(y_i-\mu_Y)}{\sum_{i=1}^n(x_i-\mu_X)^2}=\)</span>
<span class="math notranslate nohighlight">\(\dp\frac{\sg_{XY}}{\sg_X^2}=\frac{\rm{cov}(X,Y)}{\rm{var}(X)}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\sg_X^2\)</span> is the variance of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(\sg_{XY}\)</span> is the
covariance between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.
Noting that the correlation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is given as
<span class="math notranslate nohighlight">\(\rho_{XY}=\frac{\sg_{XY}}{\sg_X\cd\sg_Y}\)</span>, we can also write <span class="math notranslate nohighlight">\(w\)</span> as</p>
<div class="math notranslate nohighlight">
\[w=\rho_{XY}=\frac{\sg_Y}{\sg_X}\]</div>
<div class="math notranslate nohighlight">
\[\hat{y_i}=b+w\cd x_i=\mu_Y-w\cd\mu_X+w\cd x_i=\mu_Y+w(x_i-\mu_X)\]</div>
<p>Thus, the point <span class="math notranslate nohighlight">\((\mu_X,\mu_Y)\)</span> lins on the regression line.</p>
<div class="section" id="geometry-of-bivariate-regression">
<h3>23.2.1 Geometry of Bivariate Regression<a class="headerlink" href="#geometry-of-bivariate-regression" title="Permalink to this headline">Â¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(X=(x_1,x_2,\cds,x_n)^T\)</span> be the <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector denoting
the training data sample, <span class="math notranslate nohighlight">\(Y=(y_1,y_2,\cds,y_n)^T\)</span> the sample vector for
the response variable, and
<span class="math notranslate nohighlight">\(\hat{Y}=(\hat{y_1},\hat{y_2},\cds,\hat{y_n})^T\)</span> the vector of predicted
values, then we can express the <span class="math notranslate nohighlight">\(n\)</span> equations, <span class="math notranslate nohighlight">\(y_i=b+w\cd x_i\)</span> for
<span class="math notranslate nohighlight">\(i=1,2,\cds,n\)</span>, as a single vector equation:</p>
<div class="math notranslate nohighlight">
\[\hat{Y}=b\cd\1+w\cd X\]</div>
<p>This equation indicates that the predicted vector <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is a linear
combination of <span class="math notranslate nohighlight">\(\1\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, i.e., it must lie in the column space
spanned by <span class="math notranslate nohighlight">\(\1\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, given as <span class="math notranslate nohighlight">\(\rm{span}(\{\1,X\})\)</span>.
On the other hand, the response vector <span class="math notranslate nohighlight">\(Y\)</span> will not usually lie in the same column space.
In fact, the residual error vector
<span class="math notranslate nohighlight">\(\bs{\epsilon}=(\epsilon_1,\epsilon_2,\cds,\epsilon_n)^T\)</span> captures the
deviation between the response and predicted vectors</p>
<div class="math notranslate nohighlight">
\[\bs{\epsilon}=Y-\hat{Y}\]</div>
<p>The geometry of the problem makes it clear that the optimal <span class="math notranslate nohighlight">\(\hat{Y}\)</span> that
minimizes the error is the orthogonal projection of <span class="math notranslate nohighlight">\(Y\)</span> onto the subspace
spanned by <span class="math notranslate nohighlight">\(\1\)</span> and <span class="math notranslate nohighlight">\(X\)</span>.
The residual error vector <span class="math notranslate nohighlight">\(\bs{\epsilon}\)</span> is thus <em>orthogonal</em> to the
subspace spanned by <span class="math notranslate nohighlight">\(\1\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, and its squared length (or
magnitude) equals the SSE value, Since</p>
<div class="math notranslate nohighlight">
\[\lv\bs{\epsilon}\rv^2=\lv Y-\hat{Y}\rv^2=\sum_{i=1}^n(y_i-\hat{y_i})^2=\sum_{i=1}^n\epsilon_i=SSE\]</div>
<p>Even though <span class="math notranslate nohighlight">\(\1\)</span> and <span class="math notranslate nohighlight">\(X\)</span> are linearly independent and form a basis
for the column space, they need not be orthogonal.
We can create an orthogonal basis by decomposing <span class="math notranslate nohighlight">\(X\)</span> into a component
along <span class="math notranslate nohighlight">\(\1\)</span> and a component orthogonal to <span class="math notranslate nohighlight">\(\1\)</span>.</p>
<div class="math notranslate nohighlight">
\[\rm{proj}_\1(X)\cd\1=\bigg(\frac{X^T\1}{\1^T\1}\bigg)\cd\1=\bigg(\frac{\sum_{i=1}^nx_i}{n}\bigg)\cd\1=\mu_X\cd\1\]</div>
<div class="math notranslate nohighlight">
\[X=\mu_X\cd\1+(X-\mu_X\cd\1)=\mu_X\cd\1+\bar{X}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{X}=X-\mu_X\cd\1\)</span> is the centered attribute vector, obtained by
subtracting the mean <span class="math notranslate nohighlight">\(\mu_X\)</span> from all points.</p>
<p>The two vectors <span class="math notranslate nohighlight">\(\1\)</span> and <span class="math notranslate nohighlight">\(\bar{X}\)</span> form an <em>orthogonal basis</em> for the subspace.
We can thus obtain the predicted vector <span class="math notranslate nohighlight">\(\bar{Y}\)</span> by projecting <span class="math notranslate nohighlight">\(Y\)</span>
onto <span class="math notranslate nohighlight">\(\1\)</span> and <span class="math notranslate nohighlight">\(\bar{X}\)</span>, and summing up these two components.
That is,</p>
<div class="math notranslate nohighlight">
\[\hat{Y}=\rm{proj}_\1(Y)\cd\1+\rm{proj}_{\bar{X}}(Y)\cd\bar{X}=
\bigg(\frac{Y^T\1}{\1^T\1}\bigg)\1+
\bigg(\frac{Y^T\bar{X}}{\bar{X}^T\bar{X}}\bigg)\bar{X}=
\mu_Y\cd\1+\bigg(\frac{Y^T\bar{X}}{\bar{X}^T\bar{X}}\bigg)\bar{X}\]</div>
<div class="math notranslate nohighlight">
\[\hat{Y}=b\cd\1+w\cd X=b\cd\1+w(\mu_X\cd\1+\bar{X})=(b+w\cd\mu_X)\cd\1+w\cd\bar{X}\]</div>
<p>Since both are expressions for <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, we can equate them to obtain</p>
<div class="math notranslate nohighlight">
\[\mu_Y=b+w\cd\mu_X\quad\rm{or}\quad b=\mu_Y-w\cd\mu_X\quad\quad w=\frac{y^T\bar{X}}{\bar{X}^T\bar{X}}\]</div>
<div class="math notranslate nohighlight">
\[w=\frac{y^T\bar{X}}{\bar{X}^T\bar{X}}=\frac{Y^T\bar{X}}{\lv\bar{X}\rv^2}=
\frac{Y^T(X-\mu_X\cd\1)}{\lv X-\mu_X\cd\1\rv^2}=\frac{(\sum_{i=1}^nx_i\cd
y_i)-n\cd\mu_X\cd\mu_Y}{(\sum_{i=1}^nx_i^2)-n\cd\mu_X^2}\]</div>
</div>
</div>
<div class="section" id="multiple-regression">
<h2>22.3 Multiple Regression<a class="headerlink" href="#multiple-regression" title="Permalink to this headline">Â¶</a></h2>
<p>We now consider the more general case called <em>multiple regression</em> where we have
multiple predictor attributes <span class="math notranslate nohighlight">\(X_1,X_2,\cds,X_d\)</span> and a single response
attribute <span class="math notranslate nohighlight">\(Y\)</span>.
The training data sample <span class="math notranslate nohighlight">\(\D\in\R^{n\times d}\)</span> comprises <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(\x_i=(x_{i1},x_{i2},\cds,x_{id})^T\)</span> in a <span class="math notranslate nohighlight">\(d\)</span>-dimensional space,
along with the corresponding observed response value <span class="math notranslate nohighlight">\(y_i\)</span>.
The vector <span class="math notranslate nohighlight">\(Y=(y_1,y_2,\cds,y_n)^T\)</span> denotes the observed response vector.
The predicted response value for input <span class="math notranslate nohighlight">\(\x_i\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\hat{y_i}=b+w_1x_{i1}+w_2x_{i2}+\cds+w_dx_{id}=b+\w^T\x_i\]</div>
<p>where <span class="math notranslate nohighlight">\(\w=(w_1,w_2,\cds,w_d)^T\)</span> is the weight vector comprising the
regression coefficients or weights <span class="math notranslate nohighlight">\(w_j\)</span> along each attribute <span class="math notranslate nohighlight">\(X_j\)</span>.</p>
<p>Instead of dealing with the bias <span class="math notranslate nohighlight">\(b\)</span> separately from the weights
<span class="math notranslate nohighlight">\(w_i\)</span> for each attribute, we can introduce a new âconstantâ valued
attribute <span class="math notranslate nohighlight">\(X_0\)</span> whose value is always fixed at 1, so that each input point
<span class="math notranslate nohighlight">\(\x_i=(x_{i1},x_{i2},\cds,x_{id})^T\in\R^d\)</span> is mapped to an augmented
point <span class="math notranslate nohighlight">\(\td{\x_i}=(x_{i0},x_{i1},x_{i2},\cds,x_{id})^T\in\R^{d+1}\)</span>, where
<span class="math notranslate nohighlight">\(x_{i0}=1\)</span>.
Likewise, the weight vector <span class="math notranslate nohighlight">\(\w=(w_1,w_2,\cds,w_d)^T\)</span> is mapped to an
augmented weight vector <span class="math notranslate nohighlight">\(\td{\w}=(w_0,w_1,w_2,\cds,w_d)^T\)</span>, where
<span class="math notranslate nohighlight">\(w_0=b\)</span>.
The predicted response value for an augmented <span class="math notranslate nohighlight">\((d+1)\)</span> dimensional point <span class="math notranslate nohighlight">\(\td{\x_i}\)</span> can be written as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\hat{y_i}=w_0x_{i0}+w_1x_{i1}+w_2x_{i2}+\cds+w_dx_{id}=\td{\w}^T\td{\x_i}\)</span></p>
</div>
<p>We can compactly write all thes <span class="math notranslate nohighlight">\(n\)</span> equations as a single matrix equation, given as</p>
<div class="math notranslate nohighlight">
\[\hat{Y}=\td{\D}\td{\w}\]</div>
<p>where <span class="math notranslate nohighlight">\(\td{\D}\in\R^{n\times(d+1})\)</span> is the <em>augmented data matrix</em>, which
includes the constant attribute <span class="math notranslate nohighlight">\(X_0\)</span> in addition to the predictor
attributes <span class="math notranslate nohighlight">\(X_1,X_2,\cds,X_d\)</span>, and
<span class="math notranslate nohighlight">\(\hat{Y}=(\hat{y_1},\hat{y_2},\cds,\hat{y_n})^T\)</span> is the vector of
predicted responses.</p>
<p>The multiple regression task can now be stated as finding the <em>best fitting</em>
<em>hyperplane</em> defined by the weight vector <span class="math notranslate nohighlight">\(\td{\w}\)</span> that minimizes the sum
of squared errors</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_{\td\w}SSE&amp;=\sum_{i=1}^n\epsilon_i^2=\lv\bs\epsilon\rv^2=\lv Y-\hat{Y}\rv^2\\&amp;=(Y-\hat{Y})^T(Y-\hat{Y})=Y^TY-2Y^T\hat{Y}+\hat{Y}^T\hat{Y}\\&amp;=Y^TY-2Y^T(\td\D\td\w)+(\td\D\td\w)^T(\td\D\td\w)\\&amp;=Y^TY-2\td\w^T(\td\D^TY)+\td\w^T(\td\D^T\td\D)\td\w\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\td\w}SSE&amp;=-2\td\D^TY+2(\td\D^T\td\D)\td\w=\0\\&amp;\Rightarrow(\td\D^T\td\D)\td\w=\td\D^TY\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\td\w=(\td\D^T\td\D)\im\td\D^TY\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[\hat{Y}=\td\D\td\w=\td\D(\td\D^T\td\D)\im\td\D Y=\bs{\rm{H}}Y\]</div>
<div class="section" id="geometry-of-multiple-regression">
<h3>23.3.1 Geometry of Multiple Regression<a class="headerlink" href="#geometry-of-multiple-regression" title="Permalink to this headline">Â¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\td\D\)</span> be the augmented data matrix comprising the <span class="math notranslate nohighlight">\(d\)</span>
independent attributes <span class="math notranslate nohighlight">\(X_i\)</span>, along with the new constant attribute
<span class="math notranslate nohighlight">\(X_0=\1\in\R^n\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\td\D=\bp |&amp;|&amp;|&amp;&amp;|\\X_0&amp;X_1&amp;X_2&amp;\cds&amp;X_d\\|&amp;|&amp;|&amp;&amp;| \ep\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\td\w=(w_0,w_1,\cds,w_d)^T\in\R^(d+1)\)</span> be the augmented weight vector
that incorporates the bias term <span class="math notranslate nohighlight">\(b=w_0\)</span>.</p>
<div class="math notranslate nohighlight">
\[\hat{Y}=b\cd\1+w_1\cd X_1+w_2\cd X_2+\cds+w_d\cd X_d=\sum_{i=0}^dw_i\cd X_i=\td\D\td\w\]</div>
<p>This euqation makes it clear that the predicted vector must lie in the column
space of the augmented data matrix <span class="math notranslate nohighlight">\(\td\D\)</span>, denoted <span class="math notranslate nohighlight">\(col(\td\D)\)</span>,
i.e., it must be a linear combination of the attribute vectors <span class="math notranslate nohighlight">\(X_i\)</span>,
<span class="math notranslate nohighlight">\(i=0,\cds,d\)</span>.</p>
<p>To minimize the error in prediction, <span class="math notranslate nohighlight">\(\hat{Y}\)</span> must be the orthogonal
projection of <span class="math notranslate nohighlight">\(Y\)</span> onto the subspace <span class="math notranslate nohighlight">\(col(\td\D)\)</span>.
The residual error vector <span class="math notranslate nohighlight">\(\bs\epsilon=Y-\hat{Y}\)</span> is thus orthogonal to
the subspace <span class="math notranslate nohighlight">\(col(\td\D)\)</span>, which means that it is orthogonal to each
attribute vector <span class="math notranslate nohighlight">\(X_i\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp;\ \ \ \ \ \ \ X_i^T\bs\epsilon=0\\&amp;\Rightarrow X_i^T(Y-\hat{Y})=0\\&amp;\Rightarrow X_i^T\hat{Y}=X_i^TY\\&amp;\Rightarrow X_i^T(\td\D\td\w)=X_i^TY\\&amp;\Rightarrow w_0\cd X_i^TX_0+w_1\cd X_i^TX_1+\cds+w_d\cd X_i^TX_d=X_i^TY\end{aligned}\end{align} \]</div>
<p>We thus have <span class="math notranslate nohighlight">\((d+1)\)</span> equations, called the <em>normal equations</em>, in
<span class="math notranslate nohighlight">\((d+1)\)</span> unknowns, namely the regression coefficients or weights
<span class="math notranslate nohighlight">\(w_i\)</span> (including the bias term <span class="math notranslate nohighlight">\(w_0\)</span>).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bp X_0^TX_0&amp;X_0^TX_1&amp;\cds&amp;X_0^TX_d\\X_1^TX_0&amp;X_1^TX_1&amp;\cds&amp;X_1^TX_d\\
\vds&amp;\vds&amp;\dds&amp;\vds\\X_d^TX_0&amp;X_d^TX_1&amp;\cds&amp;X_d^TX_d\ep\td\w=\td\D^TY\end{split}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}(\td\D^T\td\D)\td\w&amp;=\td\D^TY\\\td\w&amp;=(\td\D^T\td\D)\im(\td\D^TY)\end{aligned}\end{align} \]</div>
<p>More insight can be obtained by noting that the attribute vectors comprising the
column space of <span class="math notranslate nohighlight">\(\td\D\)</span> are not necessarily orthogonal, even if we assume
they are linearly independent.
To obtain the projected vector <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, we first need to construct an orthogonal basis for <span class="math notranslate nohighlight">\(col(\td\D)\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(U_0,U_1,\cds,U_d\)</span> denote the set of orthogonal basis vectors for <span class="math notranslate nohighlight">\(col(\td\D)\)</span>.
We construct these vectors in a step-wise manner via <em>Gram-Schmidt orthogonalization</em>, as follows</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}U_0&amp;=X_0\\U_1&amp;=X_1-p_{10}\cd U_0\\U_2&amp;=X_2-p_{20}\cd U_0-p_{21}\cd U_1\\\vds&amp;=\vds\\U_d&amp;=X_d-p_{d0}\cd U_0-p_{d1}\cd U_1-\cd-p_{d,d-1}\cd U_{d-1}\end{aligned}\end{align} \]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[p_{ji}=\rm{proj}_{U_i}(X_j)=\frac{X_j^TU_i}{\lv U_i\rv^2}\]</div>
<p>denotes the scalar projection of attribute <span class="math notranslate nohighlight">\(X_j\)</span> onto the basis vector <span class="math notranslate nohighlight">\(U_i\)</span>.</p>
<p>Rearranging the equations above, we get</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}X_0&amp;=U_0\\X_1&amp;=p_{10}\cd U_0+U_1\\X_2&amp;=P_{20}\cd U_0+p_{21}\cd U_1+U_2\\\vds&amp;=\vds\\X_d&amp;=p_{d0}\cd U_0+p_{d1}\cd U_1+\cds+p_{d,d-1}\cd U_{d-1}+U_d\end{aligned}\end{align} \]</div>
<p>The Gram-Schmidt method thus results in the so-called <em>QR-factorization</em> of the
data matrix, namely <span class="math notranslate nohighlight">\(\td\D=\bs{\rm{Q}}\bs{\rm{R}}\)</span>, where by construction
<span class="math notranslate nohighlight">\(\bs{\rm{Q}}\)</span> is an <span class="math notranslate nohighlight">\(n\times(d+1)\)</span> matrix with orthogonal columns</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bs{\rm{Q}}=\bp |&amp;|&amp;|&amp;&amp;|\\U_0&amp;U_1&amp;U_2&amp;\cds&amp;U_d\\|&amp;|&amp;|&amp;&amp;| \ep\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(\bs{\rm{R}}\)</span> is the <span class="math notranslate nohighlight">\((d+1)\times(d+1)\)</span> upper-triangular matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bs{\rm{R}}=\bp 1&amp;p_{10}&amp;p_{20}&amp;\cds&amp;p_{d0}\\0&amp;1&amp;p_{21}&amp;\cds&amp;p_{d1}\\0&amp;0&amp;1&amp;
\cds&amp;p_{d2}\\\vds&amp;\vds&amp;\vds&amp;\dds&amp;\vds\\0&amp;0&amp;0&amp;1&amp;p_{d,d-1}\\0&amp;0&amp;0&amp;0&amp;1\ep\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\hat{Y}=\rm{proj}_{U_0}(Y)\cd U_0+\rm{proj}_{U_1}(Y)\cd U_1+\cds+\rm{proj}_{U_d}(Y)\cd U_d\]</div>
<p><strong>Bias Term</strong></p>
<p>Defien <span class="math notranslate nohighlight">\(\bar{X_i}\)</span> to be the centered attribute vector</p>
<div class="math notranslate nohighlight">
\[\bar{X_i}=X_i-\mu_{X_i}\cd\1\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\hat{Y}&amp;=b\cd\1+w_1\cd X_1+w_2\cd X_2+\cds+2_d\cd X_d\\&amp;=b\cd\1+w_1\cd(\bar{X_1}+\mu_{X_1}\cd\1)+\cds+w_d\cd(\bar{X_d}+\mu_{X_d}\cd\1)\\&amp;=(b+w_1\cd\mu_{X_1}+\cds+w_d\cd\mu_{X_d})\cd\1+w_1\cd\bar{X_1}+\cds+w_d\cd\bar{X_d}\end{aligned}\end{align} \]</div>
<p>On the other hand, since <span class="math notranslate nohighlight">\(\1\)</span> is orthogonal to all <span class="math notranslate nohighlight">\(\bar{X_i}\)</span>, we
can obtain another expression for <span class="math notranslate nohighlight">\(\bar{Y}\)</span> in terms of the projection of
<span class="math notranslate nohighlight">\(Y\)</span> onto the subspace spanned by the vectors
<span class="math notranslate nohighlight">\(\{\1,\bar{X_1},\cds,\bar{X_d}\}\)</span>.
Let the new orthogonal basis for these centered attribute vectors be
<span class="math notranslate nohighlight">\(\{\bar{U_0},\bar{U_1},\cds,\bar{U_d}\}\)</span>, where <span class="math notranslate nohighlight">\(\bar{U_0}=\1\)</span>.
Thus, <span class="math notranslate nohighlight">\(\hat{Y}\)</span> can also be written as</p>
<div class="math notranslate nohighlight">
\[\hat{Y}=\rm{proj}_{\bar{U_0}}(Y)\cd\bar{U_0}+\sum_{i=1}^d
\rm{proj}_{\bar{U_i}}(Y)\cd\bar{U_i}=\rm{proj}_\1+\sum_{i=1}^d\rm{proj}_
{\bar{U_i}}(Y)\cd\bar{U_i}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\rm{proj}_\1(Y)=\mu_Y&amp;=(b+w_1\cd\mu_{X_1}+\cds+w_d\cd\mu_{X_d})\\\Rightarrow b&amp;=\mu_Y-w_1\cd\mu_{X_1}-\cds-w_d\cd\mu_{X_d}=\mu_Y-\sum_{i=1}^dw_i\cd\mu_{X_i}\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\rm{proj}_\1(Y)=\frac{Y^T\1}{\1^T\1}=\frac{1}{n}\sum_{i=1}^ny_i=\mu_Y\]</div>
</div>
<div class="section" id="multiple-regression-algorithm">
<h3>23.3.2 Multiple Regression Algorithm<a class="headerlink" href="#multiple-regression-algorithm" title="Permalink to this headline">Â¶</a></h3>
<img alt="../_images/Algo23.1.png" src="../_images/Algo23.1.png" />
<div class="math notranslate nohighlight">
\[\begin{split}\Q^T\Q=\bp\lv U_0\rv^2&amp;0&amp;\cds&amp;0\\0&amp;\lv U_1\rv^2&amp;\cds&amp;0\\0&amp;0&amp;\dds&amp;0\\0&amp;0&amp;\cds&amp;\lv U_d\rv^2\ep=\Delta\end{split}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}(\td\D^T\td\D)\td\w&amp;=\td\D^TY\\(\Q\bs{\rm{R}})^T(\Q\bs{\rm{R}})\td\w&amp;=(\Q\bs{\rm{R}})^TY\\\bs{\rm{R}}^T(\Q^T\Q)\bs{\rm{R}}\td\w&amp;=\bs{\rm{R}}^TQ^TY\\\bs{\rm{R}}^T\Delta\bs{\rm{R}}\td\w&amp;=\bs{\rm{R}}^TQ^TY\\\Delta\bs{\rm{R}}\td\w&amp;=\Q^TY\\\bs{\rm{R}}\td\w&amp;=\Delta\im\Q^TY\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\hat{Y}=\td\D\td\w=\Q\bs{\rm{R}}\bs{\rm{R}}\im\Delta\im\Q^TY=\Q(\Delta\im\Q^TY)\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\Delta\im\Q^TY=\bp\rm{proj}_{U_0}(Y)\\\rm{proj}_{U_1}(Y)\\\vds\\\rm{proj}_{U_d}(Y)\ep\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{Y}=\Q\bp\rm{proj}_{U_0}(Y)\\\rm{proj}_{U_1}(Y)\\\vds\\
\rm{proj}_{U_d}(Y)\ep=\rm{proj}_{U_0}(Y)\cd U_0+\rm{proj}_{U_1}(Y)\cd U_1+
\cds+\rm{proj}_{U_d}(Y)\cd U_d\end{split}\]</div>
</div>
<div class="section" id="multiple-regression-stochastic-gradient-descent">
<h3>23.3.3 Multiple Regression: Stochastic Gradient Descent<a class="headerlink" href="#multiple-regression-stochastic-gradient-descent" title="Permalink to this headline">Â¶</a></h3>
<p>Consider the SSE obejective</p>
<div class="math notranslate nohighlight">
\[\min_{\td\w}SSE=\frac{1}{2}(Y^TY-2\td\w^T(\td\D^TY)+\td\w^T(\td\D^T\td\D)\td\w)\]</div>
<p>The gradient of the SSE objective is given as</p>
<div class="math notranslate nohighlight">
\[\nabla_{\td\w}=\frac{\pd}{\pd\td\w}SSE=-\td\D^TY+(\td\D^T\td\D)\td\w\]</div>
<p>Using gradient descent, starting from an initial weight vector estimate
<span class="math notranslate nohighlight">\(\td\w^0\)</span>, we can iteratively update <span class="math notranslate nohighlight">\(\td\w\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[\td\w^{t+1}=\td\w^t-\eta\cd\nabla_{\td\w}=\td\w^t+\eta\cd\td\D^T(Y-\td\D\cd\td\w^t)\]</div>
<p>In stochastic gradient descent (SGD), we update the weight vector by considering only one (random) point at each time.
Restricting to a single point <span class="math notranslate nohighlight">\(\td\x_k\)</span> in the training data
<span class="math notranslate nohighlight">\(\td\D\)</span>, the gradient at the point <span class="math notranslate nohighlight">\(\td\x_k\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\nabla_{\td\w}(\td\x_k)=-\td\x_ky_k+\td\x_k\td\x_k^T\td\w=-(y_k-\td\x_k^T\td\w)\td\x_k\]</div>
<p>Therefore, the stochastic gradient update rule is given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\td\w^{t+1}&amp;=\td\w^t-\eta\cd\nabla_{\td\w}(\td\x_k)\\&amp;=\td\w^t+\eta\cd(y_k-\td\x_k^T\td\w^t)\cd\td\x_k\end{aligned}\end{align} \]</div>
<img alt="../_images/Algo23.2.png" src="../_images/Algo23.2.png" />
</div>
</div>
<div class="section" id="ridge-regression">
<h2>23.4 Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">Â¶</a></h2>
<p>Instead of trying to simply minimize the squared residual error
<span class="math notranslate nohighlight">\(\lv Y-\hat{Y}\rv^2\)</span>, we add a regularization term involving the squared
norm of the weight vector <span class="math notranslate nohighlight">\((\lv\w\rv^2)\)</span> as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\min_{\td\w}J(\td\w)=\lv Y-\hat{Y}\rv^2+\alpha\cd\lv\td\w\rv^2=\lv Y-\td\D\td\w\rv^2+\alpha\cd\lv\td\w\rv^2\)</span></p>
</div>
<p>Here <span class="math notranslate nohighlight">\(\alpha\geq 0\)</span> is a regularization constant that controls the
tradeoff between minimizing the squared norm of the weight vector and the
squared error.
Recall that <span class="math notranslate nohighlight">\(\lv\td\w\rv^2=\sum_{i=1}^dw_i^2\)</span> is the <span class="math notranslate nohighlight">\(L_2\)</span>tdw`.
For this reason ridge regression is also called <span class="math notranslate nohighlight">\(L_2\)</span> regularized regression.
When <span class="math notranslate nohighlight">\(\alpha=0\)</span>, there is no regularization, but as <span class="math notranslate nohighlight">\(\alpha\)</span>
increases there is more emphasis on minimizing the regression coefficients.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\td\w}&amp;J(\td\w)=\frac{\pd}{\pd\td\w}\{\lv Y-\td\D\td\w\rv^2+\alpha\cd\lv\td\w\rv^2\}=\0\\&amp;\Rightarrow\frac{\pd}{\pd\td\w}\{Y^TY-2\td\w^T(\td\D^TY)+\td\w^T(\td\D^T\td\D)\td\w+\alpha\cd\td\w^T\td\w\}=\0\\&amp;\Rightarrow-2\td\D^TY+2(\td\D^T\td\D)\td\w+2\alpha\cd\td\w=\0\\&amp;\Rightarrow(\td\D^T\td\D+\alpha\cd\I)\td\w=\td\D^TY\end{aligned}\end{align} \]</div>
<p>Therefore, the optimal solution is</p>
<div class="math notranslate nohighlight">
\[\td\w=(\td\D^T\td\D+\alpha\cd\I)\im\td\D^TY\]</div>
<p>Regularized regression is also called <em>ridge regression</em> since we add a âridgeâ
along the main diagonal of the <span class="math notranslate nohighlight">\(\td\D^T\td\D\)</span> matrix, i.e., the optimal
solution depends on the regularized matrix <span class="math notranslate nohighlight">\((\td\D^T\td\D+\alpha\cd\I)\)</span>.
Another advantage of regularization is that if we choose a small positive
<span class="math notranslate nohighlight">\(\alpha\)</span> we are always guaranteed a solution.</p>
<p><strong>Unpenalized Bias Term</strong></p>
<p>To avoid panalizing the bias term, consider the new regulalrized objective with
<span class="math notranslate nohighlight">\(\w=(w_1,w_2,\cds,w_d)^T\)</span>, and without <span class="math notranslate nohighlight">\(w_0\)</span> given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_\w J(\w)&amp;=\lv Y-w_0\cd\1-\D\w\rv^2+\alpha\cd\lv\w\rv^2\\&amp;=\lv Y-w_0\cd\1-\sum_{i=1}^dw_i\cd X_i\rv^2+\alpha\cd\bigg(\sum_{i=1}^dw_i^2\bigg)\\&amp;=\lv Y-w_0\cd\1-\D\w\rv^2+\alpha\cd\lv\w\rv^2\\&amp;=\lv Y-(\mu_Y-\mmu^T\w)\cd\1-\D\w\rv^2+\alpha\cd\lv\w\rv^2\\&amp;=\lv(Y-\mu_Y\cd\1)-(\D-\1\mmu^T)\w\rv^2+\alpha\cd\lv\w\rv^2\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_\w J(\w)=\lv\bar{Y}-\bar\D\bar\w\rv^2+\alpha\cd\lv\w\rv^2\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\bar{Y}=Y-\mu_Y\cd\1\)</span> is the centered response vector, and
<span class="math notranslate nohighlight">\(\bar\D=\D-\1\mmu^T\)</span> is the centered data matrix.
In other words, we can exclude <span class="math notranslate nohighlight">\(w_0\)</span> from the <span class="math notranslate nohighlight">\(L_2\)</span> regularization
obejctive by simply centering the response vector and the unaugmented data
matrix.</p>
<div class="section" id="ridge-regression-stochastic-gradient-descent">
<h3>23.4.1 Ridge Regression: Stochastic Gradient Descent<a class="headerlink" href="#ridge-regression-stochastic-gradient-descent" title="Permalink to this headline">Â¶</a></h3>
<div class="math notranslate nohighlight">
\[\nabla_{\td\w}=\frac{\pd}{\pd\td\w}J(\td\w)=-\td\D^TY+(\td\D^T\td\D)\td\w+\alpha\cd\td\w\]</div>
<p>Using (batch) gradient descent, we can iteratively compute <span class="math notranslate nohighlight">\(\td\w\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[\td\w^{t+1}=\td\w^T-\eta\cd\nabla_{\td\w}=(1-\eta\cd\alpha)\td\w^t+\eta\cd\td\D^T(Y-\td\D\cd\td\w^t)\]</div>
<p>In stochastic gradient descent (SGD), we update the weight vector by considering only one (random) point at each time.
The gradient at the point <span class="math notranslate nohighlight">\(\td{\x_k}\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\nabla_{\td\w}(\td{\x_k})=-\td{\x_k}y_k+\td{\x_k}\td{\x_k}^T\td\w+
\frac{\alpha}{n}\td\w=-(y_k-\td{\x_k}^T\td\w)\td{\x_k}+\frac{\alpha}{n}\td\w\]</div>
<p>Here, we scale the regularization constant <span class="math notranslate nohighlight">\(\alpha\)</span> by dividing it by
<span class="math notranslate nohighlight">\(n\)</span>, the number of points in the training data, since the original ridge
value <span class="math notranslate nohighlight">\(\alpha\)</span> is for all the <span class="math notranslate nohighlight">\(n\)</span> points, whereas we are now
considering only one point at a time.</p>
<div class="math notranslate nohighlight">
\[\td\w^{t+1}=\td\w^t-\eta\cd\nabla_{\td\w}(\td{\x_k})=\bigg(1-
\frac{\eta\cd\alpha}{n}\bigg)\td\w^t+\eta\cd(y_k-\td{\x_k}^T\td\w^t)\cd
\td{\x_k}\]</div>
<img alt="../_images/Algo23.3.png" src="../_images/Algo23.3.png" />
</div>
</div>
<div class="section" id="kernel-regression">
<h2>23.5 Kernel Regression<a class="headerlink" href="#kernel-regression" title="Permalink to this headline">Â¶</a></h2>
<p>To avoid explicitly dealing with the bias term, we add the fixed value 1 as the
first element of <span class="math notranslate nohighlight">\(\phi(\x_i)\)</span> to obtain the <em>augmented transformed point</em>
<span class="math notranslate nohighlight">\(\td\phi(\x_i)^T=\bp 1&amp;\phi(\x_i)^T\ep\)</span>.
Let <span class="math notranslate nohighlight">\(\td{\D_\phi}\)</span> denote the <em>augmented dataset in feature space</em>,
comprising the transformed points <span class="math notranslate nohighlight">\(\td\phi(\x_i)\)</span> for
<span class="math notranslate nohighlight">\(i=1,2,\cds,n\)</span>.
The <em>augmented kernel function</em> in feature space is given as</p>
<div class="math notranslate nohighlight">
\[\td{K}(\x_i,\x_j)=\td\phi(\x_i)^T\td\phi(\x_j)=1+\phi(\x_i)^T\phi(\x_j)=1+K(\x_i,\x_j)\]</div>
<p>Let <span class="math notranslate nohighlight">\(Y\)</span> denote the observed response vector.
We model the predicted response as</p>
<div class="math notranslate nohighlight">
\[\hat{Y}=\td{\D_\phi}\td\w\]</div>
<p>where <span class="math notranslate nohighlight">\(\td\w\)</span> is the augmented weight vector in feature space.
The first element of <span class="math notranslate nohighlight">\(\td\w\)</span> denotes the bias in feature space.</p>
<p>For regularized regression, we have to solve the following objective in feature space:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_{\td\w}J(\td\w)=\lv Y-\hat{Y}\rv^2+\alpha\cd\lv\td\w\rv^2=\)</span>
<span class="math notranslate nohighlight">\(\lv Y-\td{\D_\phi}\td\w\rv^2+\alpha\cd\lv\td\w\rv^2\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\geq 0\)</span> is the regularization constant.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\td\w}&amp;J(\td\w)=\frac{\pd}{\pd\td\w}\{\lv Y-\td{\D_\phi}\td\w\rv^2+\alpha\lv\td\w\rv^2\}=\0\\&amp;\Rightarrow\frac{\pd}{\pd\td\w}\{Y^TY-2\td\w^T(\td{\D\phi}Y)+
\td\w^T(\td{\D_\phi}^T\td{\D_\phi})\td\w+\alpha\cd\td\w^T\td\w\}=\0\\&amp;\Rightarrow-2\td{\D_\phi}^TY+2(\td{\D_\phi}^T\td{\D_\phi})\td\w+2\alpha\cd\td\w=\0\\&amp;\Rightarrow\alpha\cd\td\w=\td{\D_\phi}^TY-(\td{\D_\phi}^T\td{\D_\phi})\td\w\\&amp;\Rightarrow\td\w=\td{\D_\phi}^T\bigg(\frac{1}{\alpha}(Y-\td{\D_\phi}\td\w)\bigg)\\&amp;\Rightarrow\td\w=\td{\D_\phi}^T\c=\sum_{i=1}^nc_i\cd\td\phi(\x_i)\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\c=(c_1,c_2,\cds,c_n)^T=\frac{1}{\alpha}(Y-\td{\D_\phi}\td\w)\)</span></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\c&amp;=\frac{1}{\alpha}(Y-\td{\D_\phi}\td\w)\\\alpha\cd\c&amp;=Y-\td{\D_\phi}\td\w\\\alpha\cd\c&amp;=Y-\td{\D_\phi}(\td{\D_\phi}\c)\\(\td{\D_\phi}\td{\D_\phi}^T+\alpha\cd\I)\c&amp;=Y\\\c&amp;=(\td{\D_\phi}\td{\D_\phi}^T+\alpha\cd\I)\im Y\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\c=(\td\K+\alpha\cd\I)\im Y\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\hat{Y}&amp;=\td{\D_\phi}\td\w\\&amp;=\td{\D_\phi}\td{\D_\phi}^T\c\\&amp;=(\td{\D_\phi}\td{\D_\phi}^T)(\td\K+\alpha\cd\I)\im Y\\&amp;=\td\K(\td\K+\alpha\cd\I)\im Y\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\td\K(\td\K+\alpha\cd\I)\im\)</span> is the <em>kernel hat matrix</em>.</p>
<img alt="../_images/Algo23.4.png" src="../_images/Algo23.4.png" />
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\hat{y}&amp;=\td\phi(\z)^T\td\w=\td\phi(\z)^T(\td{\D_\phi}^T\c)=\td\phi(\z)^T\bigg(\sum_{i=1}^nc_i\cd\td\phi(\x_i)\bigg)\\&amp;=\sum_{i=1}^nc_i\cd\td\phi(\z)^T\td\phi(\x_i)=\sum_{i=1}^nc_i\cd\td{K}(\z,\x_i)=\c^T\td{\K_\z}\end{aligned}\end{align} \]</div>
<div class="section" id="l-1-regression-lasso">
<h3>23.6 <span class="math notranslate nohighlight">\(L_1\)</span> Regression: Lasso<a class="headerlink" href="#l-1-regression-lasso" title="Permalink to this headline">Â¶</a></h3>
<p>The <em>Lasso</em>, which stands for <em>least absolute selection and shrinkage operator</em>,
is a regularization method that aims to sparsify the regression weights.
Instead of using the <span class="math notranslate nohighlight">\(L_2\)</span> or Euclidean norm for weight regularization as
in ridge regression, the Lasso formulation uses the <span class="math notranslate nohighlight">\(L_1\)</span> norm for
regularization</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_\w J(\w)=\frac{1}{2}\cd\lv\bar{Y}-\bar\D\w\rv^2+\alpha\cd\lv\w\rv_1\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\geq 0\)</span> is the regularization constant and</p>
<div class="math notranslate nohighlight">
\[\lv\w\rv_1=\sum_{i=1}^d|w_i|\]</div>
<p>We asume that</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\bar\D&amp;=\D-\1\cd\mmu^T\\\bar{Y}&amp;=Y-\mu_Y\cd\1\end{aligned}\end{align} \]</div>
<p>One benefit of centering is that we do not have to explicitly deal with the bias
term <span class="math notranslate nohighlight">\(b=w_0\)</span>, which is important since we usually do not want to penalize
<span class="math notranslate nohighlight">\(b\)</span>.
Once the regression coefficients have been estimated, we can obtain the bias term as follows:</p>
<div class="math notranslate nohighlight">
\[b=w_0=\mu_Y-\sum_{j=1}^dw_j\cd\mu_{X_j}\]</div>
<p>The main advantage of using the <span class="math notranslate nohighlight">\(L_1\)</span> norm is that it leads to <em>sparsity</em> in the solution vector.
That is, whereas ridge regression reduces the value of the regression
coefficients <span class="math notranslate nohighlight">\(w_i\)</span>, they may remains small but still non-zero.
On the other hand, <span class="math notranslate nohighlight">\(L_1\)</span> regression can drive the coefficients to zero,
resulting in a more interpretable model, especially when there are many
predictor attributes.</p>
<p>The Lasso objective comprises two parts, the squared error term
<span class="math notranslate nohighlight">\(\lv\bar{Y}-\bar\D\w\rv^2\)</span> which is convex and differentiable, and the
<span class="math notranslate nohighlight">\(L_1\)</span> penalty term <span class="math notranslate nohighlight">\(\alpha\cd\lv\w\rv_1=\alpha\sum_{i=1}^d|w_i|\)</span>,
which is convex but unfortunately non-differentiable at <span class="math notranslate nohighlight">\(w_i=0\)</span>.
This means we cannot simply compute the gradient and set it to zero, as we did in the case of ridge regression.
However, these kinds of problems can be colved via the generalized approach of <em>subgradients</em>.</p>
</div>
<div class="section" id="subgradients-and-subdifferential">
<h3>23.6.1 Subgradients and Subdifferential<a class="headerlink" href="#subgradients-and-subdifferential" title="Permalink to this headline">Â¶</a></h3>
<p>Consider the absolute value function <span class="math notranslate nohighlight">\(f:\R\rightarrow\R\)</span></p>
<div class="math notranslate nohighlight">
\[f(w)=|w|\]</div>
<p>When <span class="math notranslate nohighlight">\(w&gt;0\)</span>, we can see that its derivative is <span class="math notranslate nohighlight">\(f\pr(w)=+1\)</span>, and when
<span class="math notranslate nohighlight">\(w&lt;0\)</span>, its derivative is <span class="math notranslate nohighlight">\(f\pr(w)=-1\)</span>.
On the other hand, there is a discontinuity at <span class="math notranslate nohighlight">\(w=0\)</span> where the derivative does not exist.</p>
<p>We use the concept of a <em>subgradient</em> that generalizes the notion of a derivative.
For the absolute value function, the slope <span class="math notranslate nohighlight">\(m\)</span> of any line that passes
through <span class="math notranslate nohighlight">\(w=0\)</span> that remains below or touches the graph of <span class="math notranslate nohighlight">\(f\)</span> is
called a subgradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(w=0\)</span>.
The set of all the subgradients at <span class="math notranslate nohighlight">\(w\)</span> is called the <em>subdifferential</em>, denoted as <span class="math notranslate nohighlight">\(\pd|w|\)</span>.
Thus, the subdifferential at <span class="math notranslate nohighlight">\(w=0\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\pd|w|=[-1,1]\]</div>
<p>since only lines with slope between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(+1\)</span> remain below or
(partially) coincide with the absolute value graph.</p>
<p>Considering all the cases, the subdifferential for the absolute value function is given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{\begin{array}{lr}1\quad\quad\quad\ \rm{iff\ }w&gt;0\\
-1\quad\quad\ \ \rm{iff\ }w&lt;0\\ [-1,1]\quad\rm{iff\ }w=0\end{array}\right.\end{split}\]</div>
<p>We can see that when the derivative exists, the subdifferential is unique and
corresponds to the derivative (or gradient), and when the derivative does not
exist the subdifferential corresponds to a set of subgradients.</p>
</div>
<div class="section" id="bivariate-l-1-regression">
<h3>23.6.2 Bivariate <span class="math notranslate nohighlight">\(L_1\)</span> Regression<a class="headerlink" href="#bivariate-l-1-regression" title="Permalink to this headline">Â¶</a></h3>
<p>The bivariate regression model is given as</p>
<div class="math notranslate nohighlight">
\[\hat{y_i}=w\cd\bar{x_i}\]</div>
<p>The Lasso objective can then be written as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_wJ(w)=\frac{1}{2}\sum_{i=1}^n(\bar{y_i}-w\cd\bar{x_i})^2+\alpha\cd|w|\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\pd J(w)&amp;=\frac{1}{2}\cd\sum_{i=1}^n2\cd(\bar{y_i}-w\cd\bar{x_i})\cd(-\bar{x_i})+\alpha\cd\pd|w|\\&amp;=-\sum_{i=1}^n\bar{x_i}\cd\bar{y_i}+w\cd\sum_{i=1}^n\bar{x_i}^2+\alpha\cd\pd|w|\\&amp;=-\bar{X}^T\bar{Y}+w\cd\lv\bar{X}\rv^2+\alpha\cd\pd|w|\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp;\quad\ \ \pd J(w)=0\\&amp;\Rightarrow w\cd\lv\bar{X}\rv^2+\alpha\cd\pd|w|=\bar{X}^T\bar{Y}\\&amp;\Rightarrow w+\eta\cd\alpha\cd\pd|w|=\eta\cd\bar{X}^T\bar{Y}\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\eta=1/\lv\bar{X}\rv^2&gt;0\)</span> is a scaling constant.</p>
<p><strong>Case I</strong> (<span class="math notranslate nohighlight">\(w&gt;0\)</span> and <span class="math notranslate nohighlight">\(\pd|w|=1\)</span>):</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[w=\eta\cd\bar{X}^T\bar{Y}-\eta\cd\alpha\]</div>
<p>Since <span class="math notranslate nohighlight">\(w&gt;0\)</span>, this implies <span class="math notranslate nohighlight">\(\eta\cd\bar{X}^T\bar{Y}&gt;\eta\cd\alpha\)</span>
or <span class="math notranslate nohighlight">\(|\eta\cd\bar{X}^T\bar{Y}&gt;\eta\cd\alpha\)</span>.</p>
</div></blockquote>
<p><strong>Case II</strong> (<span class="math notranslate nohighlight">\(w&lt;0\)</span> and <span class="math notranslate nohighlight">\(\pd|w|=-1\)</span>):</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[w=\eta\cd\bar{X}^T\bar{Y}+\eta\cd\alpha\]</div>
<p>Since <span class="math notranslate nohighlight">\(w&lt;0\)</span>, this implies
<span class="math notranslate nohighlight">\(\eta\cd\bar{X}^T\bar{Y}&lt;-\eta\cd\alpha\)</span> or
<span class="math notranslate nohighlight">\(|\eta\cd\bar{X}^T\bar{Y}|&gt;\eta\cd\alpha\)</span>.</p>
</div></blockquote>
<p><strong>Case III</strong> (<span class="math notranslate nohighlight">\(w=0\)</span> and <span class="math notranslate nohighlight">\(\pd|w|\in[-1,1]\)</span>):</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[w\in[\eta\cd\bar{X}^T\bar{Y}-\eta\cd\alpha,\eta\cd\bar{X}^T\bar{Y}+\eta\cd\alpha]\]</div>
<p>However, since <span class="math notranslate nohighlight">\(w=0\)</span>, this implies
<span class="math notranslate nohighlight">\(\eta\cd\bar{X}^T\bar{Y}-\eta\cd\alpha\leq 0\)</span> and
<span class="math notranslate nohighlight">\(\eta\cd\bar{X}^T\bar{Y}+\eta\cd\alpha\geq 0\)</span>.
In other words, <span class="math notranslate nohighlight">\(\eta\cd\bar{X}^T\bar{Y}\leq\eta\cd\alpha\)</span> and
<span class="math notranslate nohighlight">\(\eta\cd\bar{X}^T\bar{Y}\geq-\eta\cd\alpha\)</span>.
Therefore, <span class="math notranslate nohighlight">\(|\eta\cd\bar{X}^T\bar{Y}|\leq\eta\cd\alpha\)</span>.</p>
</div></blockquote>
<p>Let <span class="math notranslate nohighlight">\(\tau\geq 0\)</span> be some fixed value.
Define the <em>soft-threshold</em> function <span class="math notranslate nohighlight">\(S_\tau:\R\rightarrow\R\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[S_\tau(z)=\rm{sign}(z)\cd\max\{0,(|z|-\tau)\}\]</div>
<p>The the above three cases can be written compactly as:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(w=S_{\eta\cd\alpha}(\eta\cd\bar{X}^T\bar{Y})\)</span></p>
</div>
<p>with <span class="math notranslate nohighlight">\(\tau=\eta\cd\alpha\)</span>, where <span class="math notranslate nohighlight">\(w\)</span> is the optimal solution to the
bivariate <span class="math notranslate nohighlight">\(L_1\)</span> regression problem.</p>
</div>
<div class="section" id="multiple-l-1-regression">
<h3>23.6.3 Multiple <span class="math notranslate nohighlight">\(L_1\)</span> Regression<a class="headerlink" href="#multiple-l-1-regression" title="Permalink to this headline">Â¶</a></h3>
<p>Consider the <span class="math notranslate nohighlight">\(L_1\)</span> regression objective</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_\w J(\w)&amp;=\frac{1}{2}\cd\lv\bar{Y}-\sum_{i=1}^dw_i\cd\bar{X_i}\rv^2+\alpha\cd\lv\w\rv_1\\&amp;=\frac{1}{2}\cd\bigg(\bar{Y}^T\bar{Y}-2\sum_{i=1}^dw_i\cd\bar{X_i}^T\bar{Y}
+\sum_{i=1}^d\sum_{j=1}^dw_i\cd w_j\cd\bar{X_i}^T\bar{X_j})+\alpha\cd
\sum_{i=1}^d|w_i|\end{aligned}\end{align} \]</div>
<p>We generalize the bivariate solution to the multiple <span class="math notranslate nohighlight">\(L_1\)</span> formulation by
optimizing for each <span class="math notranslate nohighlight">\(w_k\)</span> individually, via the approach of
<em>cyclical coordinate descent</em>. We rewrite the <span class="math notranslate nohighlight">\(L_1\)</span> objective by focusing
only on the <span class="math notranslate nohighlight">\(w_k\)</span> terms, and ignoring all terms not involving <span class="math notranslate nohighlight">\(w_k\)</span>,
which are assumed to be fixed:</p>
<div class="math notranslate nohighlight">
\[\min_{w_k}=J(w_k)=-w_k\cd\bar{X_k}^T\bar{Y}+\frac{1}{2}w_k^2\cd\lv\bar{X_k}
\rv^2+w_k\cd\sum_{j\ne k}^dw_j\cd\bar{X_k}^T\bar{X_j}+\alpha\cd|w_k|\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&amp;\quad\ \ \pd J(w_k)=0\\&amp;\Rightarrow w_k\cd\lv\bar{X_k}\rv^2+\alpha\cd\pd|w_k|=\bar{X_k}^T\bar{Y}-\sum_{j\ne k}^dw_j\cd\bar{X_k}^T\bar{X_j}\\&amp;\Rightarrow w_k\cd\lv\bar{X_k}\rv^2+\alpha\cd\pd|w_k|=\bar{X_k}^T\bar{Y}-
\sum_{j=1}^dw_j\cd\bar{X_k}^T\bar{X_j}+w_k\bar{X_k}^T\bar{X_k}\\&amp;\Rightarrow w_k\cd\lv\bar{X_k}\rv^2+\alpha\cd\pd|w_k|=w_k\lv\bar{X_k}^T\rv^2+\bar{X_k}^T(\bar{Y}-\bar\D\w)\end{aligned}\end{align} \]</div>
<p>The new estimate for <span class="math notranslate nohighlight">\(w_k\)</span> at step <span class="math notranslate nohighlight">\(t+1\)</span> is then given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}w_k^{t+1}+\frac{1}{\bar{X_k}}^2\cd\alpha\cd\pd|w_k^{t+1}|&amp;=w_k^t+
\frac{1}{\lv\bar{X_k}\rv^2}\cd\bar{X_k}^T(\bar{Y}-\bar\D\w^t)\\w_k^{t+1}+\eta\cd\alpha\cd\pd|w_k^{t+1}|&amp;=w_k^t+\eta\cd\bar{X_k}^T(Y-\bar\D\w^t)\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\eta=1/\lv\bar{X_k}\rv^2&gt;0\)</span> is just a scaling constant.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(w_k^{t+1}=S_{\eta\cd\alpha}(w_k^t+\eta\cd\bar{X_k}^T(\bar{Y}-\bar\D\w^t))\)</span></p>
</div>
<img alt="../_images/Algo23.5.png" src="../_images/Algo23.5.png" />
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap24.html" class="btn btn-neutral float-right" title="Chapter 24 Logistic Regression" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index5.html" class="btn btn-neutral float-left" title="Part 5 Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>