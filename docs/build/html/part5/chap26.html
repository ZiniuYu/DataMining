

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 26 Deep Learning &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 27 Regression Evaluation" href="chap27.html" />
    <link rel="prev" title="Chapter 25 Neural Networks" href="chap25.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index5.html">Part 5 Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap23.html">Chapter 23 Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap24.html">Chapter 24 Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap25.html">Chapter 25 Neural Networks</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 26 Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#recurrent-neural-networks">26.1 Recurrent Neural Networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#feed-forward-in-time">26.1.1 Feed-forward in Time</a></li>
<li class="toctree-l4"><a class="reference internal" href="#backpropagation-in-time">26.1.2 backpropagation in Time</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-rnns">26.1.3 Training RNNs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bidirectional-rnns">26.1.4 Bidirectional RNNs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#gated-rnns-long-short-term-memory-networks">26.2 Gated RNNs: Long Short-Term Memory Networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#forget-gate">26.2.1 Forget Gate</a></li>
<li class="toctree-l4"><a class="reference internal" href="#computing-net-gradients">Computing Net Gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="#long-short-term-memory-lstm-networks">26.2.2 Long Short-Term Memory (LSTM) Networks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-lstms">26.2.3 Training LSTMs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#convolutional-neural-networks">26.3 Convolutional Neural Networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#convolutions">26.3.1 Convolutions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bias-and-activation-functions">26.3.2 Bias and Activation Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#padding-and-striding">26.3.3 Padding and Striding</a></li>
<li class="toctree-l4"><a class="reference internal" href="#generalized-aggregation-functions-pooling">26.3.4 Generalized Aggregation Functions: Pooling</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap27.html">Chapter 27 Regression Evaluation</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index5.html">Part 5 Regression</a> &raquo;</li>
        
      <li>Chapter 26 Deep Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part5/chap26.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}
\newcommand{\Z}{\boldsymbol{\mathrm{Z}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cE}{\mathcal{E}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-26-deep-learning">
<h1>Chapter 26 Deep Learning<a class="headerlink" href="#chapter-26-deep-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="recurrent-neural-networks">
<h2>26.1 Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>Multilayer perceptrons are feed-forward networks in which the information flows
in only one direction, namely from the input layer to the output layer via the
hidden layers.
In constrast, recurrent neural networks (RNNs) are dynamically driven, with a
<em>feedback</em> loop between two (or more) layers, which makes such networks ideal
for learning from sequence data.</p>
<p>Let <span class="math notranslate nohighlight">\(\cX=\lag\x_1,\x_2,\cds,\x_\tau\rag\)</span> denote a sequence of vectors,
where <span class="math notranslate nohighlight">\(\x_t\in\R^d\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector
<span class="math notranslate nohighlight">\((t=1,2\,\cds,\tau)\)</span>.
Thus, <span class="math notranslate nohighlight">\(\cX\)</span> is an input sequence of length <span class="math notranslate nohighlight">\(\tau\)</span>, with
<span class="math notranslate nohighlight">\(\x_t\)</span> denoting the input at time step <span class="math notranslate nohighlight">\(t\)</span>.
Let <span class="math notranslate nohighlight">\(\cY=\lag\y_1,\y_2,\cds,\y_\tau\rag\)</span> denote a sequence of vectors,
with <span class="math notranslate nohighlight">\(\y_t\in\R^p\)</span> a <span class="math notranslate nohighlight">\(p\)</span>-dimensional vector.
Here <span class="math notranslate nohighlight">\(\cY\)</span> is the desired target or response sequence, with
<span class="math notranslate nohighlight">\(\y_t\)</span> denoting the response vector at time <span class="math notranslate nohighlight">\(t\)</span>.
Finally, let <span class="math notranslate nohighlight">\(\cl{O}=\lag\o_1,\o_2,\cds,\o_\tau\rag\)</span> denote the predicted
or output sequence from the RNN.
Here <span class="math notranslate nohighlight">\(\o_t\in\R^p\)</span> is also a <span class="math notranslate nohighlight">\(p\)</span>-dimensional vector to match the corresponding true response <span class="math notranslate nohighlight">\(\y_t\)</span>.
The task of an RNN is to learn a function that predicts the target sequence
<span class="math notranslate nohighlight">\(\cY\)</span> given the input sequence <span class="math notranslate nohighlight">\(\cX\)</span>.
That is, the predicted output <span class="math notranslate nohighlight">\(\o_t\)</span> on input <span class="math notranslate nohighlight">\(\x_t\)</span> should be
similar or close to the target response <span class="math notranslate nohighlight">\(\y_t\)</span>, for each time point
<span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>To learn dependencies between elements of the input sequence, an RNN maintains a
sequence of <span class="math notranslate nohighlight">\(m\)</span>-dimensional hidden state vectors <span class="math notranslate nohighlight">\(\h_t\in\R^m\)</span>,
where <span class="math notranslate nohighlight">\(\h_t\)</span> captures the essential features of the input sequences up to
time <span class="math notranslate nohighlight">\(t\)</span>.
The hidden vector <span class="math notranslate nohighlight">\(\h_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> depends on the input vector
<span class="math notranslate nohighlight">\(\x_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and the previous hidden state vector
<span class="math notranslate nohighlight">\(\h_{t-1}\)</span> from time <span class="math notranslate nohighlight">\(t-1\)</span>, and it is computed as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\h_t=f^h(\W_i^T\x_t+\W_h^T\h_{t-1}+\b_h)\)</span></p>
</div>
<p>Here, <span class="math notranslate nohighlight">\(f^h\)</span> is the hidden state activation function, typically tanh or ReLU.
Also, we need an initial hidden state vector <span class="math notranslate nohighlight">\(\h_0\)</span> that serves as the prior state to compute <span class="math notranslate nohighlight">\(\h_1\)</span>.
This is uaually set to the zero vector, or seeded from a prior RNN prediction step.
The matrix <span class="math notranslate nohighlight">\(\W_i\in\R^{d\times m}\)</span> specifies the weights between the input vectors and the hidden state vectors.
The matrix <span class="math notranslate nohighlight">\(\W_h\in\R^{m\times m}\)</span> specifies the weight matrix between the
hidden state vectors at time <span class="math notranslate nohighlight">\(t-1\)</span> and <span class="math notranslate nohighlight">\(t\)</span>, with <span class="math notranslate nohighlight">\(\b_h\in\R^m\)</span>
specifying the bias terms associated with the hidden states.
Note that we need only one bias vector <span class="math notranslate nohighlight">\(\b_h\)</span> associated with the hidden
state neurons; we do not need a separate bias vector between the input and
hidden neurons.</p>
<p>Given the hidden state vector at time <span class="math notranslate nohighlight">\(t\)</span>, the output vector <span class="math notranslate nohighlight">\(\o_t\)</span>
at time <span class="math notranslate nohighlight">\(t\)</span> is computed as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\o_y=f^o(\W_o^T\h_t+\b_o)\)</span></p>
</div>
<p>Here, <span class="math notranslate nohighlight">\(\W_o\in\R^{m\times \p}\)</span> specifies the weights between the hidden
state and output vectors, with bias vector <span class="math notranslate nohighlight">\(\b_o\)</span>.
The output activation function <span class="math notranslate nohighlight">\(f^o\)</span> typically uses linear or identity
activation, or a softmax activation for one-hot encoded categorical output
values.</p>
<p>It is important to note that all the weight matrices and bias vectors are <em>independent</em> of the time <span class="math notranslate nohighlight">\(t\)</span>.
For example, for the hidden layer, the same weight matrix <span class="math notranslate nohighlight">\(\W_h\)</span> and bias
vector <span class="math notranslate nohighlight">\(\b_h\)</span> is used and updated while training the model, over all time
steps <span class="math notranslate nohighlight">\(t\)</span>.
This is an exmaple of <em>parameter sharing</em> or <em>weight tying</em> between different layers or components of a neural network.
Likewise, the input weight matrix <span class="math notranslate nohighlight">\(\W_i\)</span>, the output weight matrix
<span class="math notranslate nohighlight">\(\W_o\)</span> and the bias vector <span class="math notranslate nohighlight">\(\b_o\)</span> are all shared across time.
This greatly reduces the number of parameters that need to be learned by the
RNN, but it also relies on the assumption that all relevant sequential features
can be captured by the shared parameters.</p>
<p>The training data for the RNN is given as <span class="math notranslate nohighlight">\(\D=\{\cX_i,\cY_y\}_{i=1}^n\)</span>,
comprising <span class="math notranslate nohighlight">\(n\)</span> input sequences <span class="math notranslate nohighlight">\(\cX_i\)</span> and the corresponding target
response sequences <span class="math notranslate nohighlight">\(\cY_i\)</span>, with sequence length <span class="math notranslate nohighlight">\(\tau_i\)</span>.
Given each pair <span class="math notranslate nohighlight">\((\cX,\cY)\in\D\)</span>, with
<span class="math notranslate nohighlight">\(\cX=\lag\x_1,\x_2,\cds,\x_\tau\rag\)</span> and
<span class="math notranslate nohighlight">\(\cY=\lag\y_1,\y_2,\cds,\y_\tau\rag\)</span>, the RNN has to update the model
parameters <span class="math notranslate nohighlight">\(\W_i,\W_h,\b_h,\W_o,\b_o\)</span> for the input, hidden and output
layers, to learn the corresponding output sequence
<span class="math notranslate nohighlight">\(\cl{O}=\lag\o_1,\o_2,\cds,\o_\tau\rag\)</span>.
For training the network, we compute the error or <em>loss</em> between the predicted and response vectors over all time steps.
The squared error loss is given as</p>
<div class="math notranslate nohighlight">
\[\cE_\cX=\sum_{t=1}^\tau\cE_{\x_t}=\frac{1}{2}\cd\sum_{t=1}^\tau\lv\y_t-\o_t\rv^2\]</div>
<p>If we use a softmax activation at the output layer, then we use the cross-entropy loss, given as</p>
<div class="math notranslate nohighlight">
\[\cE_\cX=\sum_{t=1}^\tau\cE_{\x_t}=-\sum_{t=1}^\tau\sum_{i=1}^py_{ti}\cd\ln(o_{ti})\]</div>
<p>where <span class="math notranslate nohighlight">\(\y_t=(y_{t1},y_{t2},\cds,y_{tp})^T\in\R^p\)</span> and <span class="math notranslate nohighlight">\(\o_t=(o_{t1},o_{t2},\cds,o_{tp})^T\in\R^p\)</span>.
On training input of length <span class="math notranslate nohighlight">\(\tau\)</span> we first unfold the RNN for
<span class="math notranslate nohighlight">\(\tau\)</span> steps, following which the parameters can be learned via the
standard feed-forward and backpropagation steps, keeping in mind the connections
between the layers.</p>
<div class="section" id="feed-forward-in-time">
<h3>26.1.1 Feed-forward in Time<a class="headerlink" href="#feed-forward-in-time" title="Permalink to this headline">¶</a></h3>
<p>The feed-forward process starts at time <span class="math notranslate nohighlight">\(t=0\)</span>, taking as input the initial
hidden state vector <span class="math notranslate nohighlight">\(\h_0\)</span>, which us usually set to <span class="math notranslate nohighlight">\(\0\)</span> or it can
be user-specified, say from a previous prediction step.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\o_t&amp;=f^o(\W_o^T\h_t+\b_o)\\&amp;=f^o(\W_o^Tf^h(\W_i^T\x_t+\W_h^T\h_{t-1}+\b_h)+\b_o)\\&amp;=\vds\\&amp;=f^o(\W_o^Tf^h(\W_i^T\x_t+\W_h^Tf^h(\cds f^h(\W_i^T\x_1+\W_h^T\h_0+\b_h)+\cds)+\b_h)+\b_o)\end{aligned}\end{align} \]</div>
<p>We can observe that the RNN implicitly makes a prediction for every prefix of
the input sequence, since <span class="math notranslate nohighlight">\(\o_t\)</span> depends on all the previous input vectors
<span class="math notranslate nohighlight">\(\x_1,\x_2,\cds,\x_t\)</span> but not on any future inputs
<span class="math notranslate nohighlight">\(\x_{t+1},\cds,\x_\tau\)</span>.</p>
</div>
<div class="section" id="backpropagation-in-time">
<h3>26.1.2 backpropagation in Time<a class="headerlink" href="#backpropagation-in-time" title="Permalink to this headline">¶</a></h3>
<p>For the backpropagation step it is easier to view the RNN in terms of the
distinct layers based on the dependencies, as opposed to unfolding in time.</p>
<p>Let <span class="math notranslate nohighlight">\(\cE_{\x_t}\)</span> denote the loss on input vector <span class="math notranslate nohighlight">\(\x_t\)</span> from the
input sequence <span class="math notranslate nohighlight">\(\cX=\lag\x_1,\x_2,\cds,\x_\tau\rag\)</span>.
The unfolded feed-forward RNN for <span class="math notranslate nohighlight">\(\cX\)</span> has <span class="math notranslate nohighlight">\(l=\tau+1\)</span> layers.
Define <span class="math notranslate nohighlight">\(\bs\delta_t^o\)</span> as the net gradient vector for the output vector
<span class="math notranslate nohighlight">\(\o_t\)</span>, i.e., the derivative of the error function <span class="math notranslate nohighlight">\(\cE_{\x_t}\)</span> with
respect to the net value at each neuron in <span class="math notranslate nohighlight">\(\o_t\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^o=\bigg(\frac{\pd\cE_{\x_t}}{\pd net_{t1}^o},
\frac{\pd\cE_{\x_t}}{\pd net_{t2}^o},\cds,
\frac{\pd\cE_{\x_t}}{\pd net_{tp}^o}\bigg)^T\]</div>
<p>where <span class="math notranslate nohighlight">\(\o_t=(o_{t1},o_{t2},\cds,o_{tp})^T\in\R^p\)</span> is the <span class="math notranslate nohighlight">\(p\)</span>-dimensional output vector at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(net_{ti}^o\)</span> is the net
value at output neuron <span class="math notranslate nohighlight">\(o_{ti}\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.
Likewise, let <span class="math notranslate nohighlight">\(\bs\delta_t^h\)</span> denote the net gradient vector for the
hidden state neurons <span class="math notranslate nohighlight">\(\h_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span></p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^h=\bigg(\frac{\pd\cE_{\x_t}}{\pd net_{t1}^h},
\frac{\pd\cE_{\x_t}}{\pd net_{t2}^h},\cds,
\frac{\pd\cE_{\x_t}}{\pd net_{tm}^h}\bigg)^T\]</div>
<p>where <span class="math notranslate nohighlight">\(\h_t=(h_{t1},h_{t2},\cds,h_{tm})^T\in\R^m\)</span> is the <span class="math notranslate nohighlight">\(m\)</span>-dimensional hidden state vector at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(net_{ti}^h\)</span> is the
net value at hidden neuron <span class="math notranslate nohighlight">\(h_{ti}\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.
Let <span class="math notranslate nohighlight">\(f^h\)</span> and <span class="math notranslate nohighlight">\(f^o\)</span> denote the activation functions for the hidden
state and output neurons, and let <span class="math notranslate nohighlight">\(\pd\f_t^h\)</span> and <span class="math notranslate nohighlight">\(\pd\f_t^o\)</span> denote
the vector of the derivatives of the activation function with respect to the net
signal for the hidden and output neurons at time <span class="math notranslate nohighlight">\(t\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\pd\f_t^h\bigg(\frac{\pd f^h(net_{t1}^h)}{\pd net_{t1}^h},
\frac{\pd f^h(net_{t2}^h)}{\pd net_{t2}^h},\cds,
\frac{\pd f^h(net_{tm}^h)}{\pd net_{tm}^h}\bigg)^T\\\pd\f_t^o\bigg(\frac{\pd f^o(net_{t1}^o)}{\pd net_{t1}^o},
\frac{\pd f^o(net_{t2}^o)}{\pd net_{t2}^o},\cds,
\frac{\pd f^o(net_{tp}^o)}{\pd net_{tp}^o}\bigg)^T\end{aligned}\end{align} \]</div>
<p>Finally, let <span class="math notranslate nohighlight">\(\pd\bs\cE_{\x_t}\)</span> denote the vector of partial derivatives
of the error function with respect to <span class="math notranslate nohighlight">\(\o_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[\pd\bs\cE_{\x_t}=\bigg(\frac{\pd\cE_{\x_t}}{\pd o_{t1}},\frac{\pd\cE_{\x_t}}
{\pd o_{t2}},\cds,\frac{\pd\cE_{\x_t}}{\pd o_{tp}}\bigg)^T\]</div>
<p><strong>Computing Net Gradients</strong></p>
<p>The net gradient vector at the output <span class="math notranslate nohighlight">\(\o_t\)</span> can be computed as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\delta_t^o=\pd\f_t^o\od\pd\bs\cE_{\x_t}\)</span></p>
</div>
<p>For example, if <span class="math notranslate nohighlight">\(\cE_{\x_t}\)</span> is the squared error function, and the output
layer uses the identity funciton, then we have</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^o=\1\od(\o_t-\y_t)\]</div>
<p>On the other hand, the net gradients at each of the hidden layers need to
account for the incoming net gradients from <span class="math notranslate nohighlight">\(\o_t\)</span> and from
<span class="math notranslate nohighlight">\(\h_{t+1}\)</span>.
The net gradient vector for <span class="math notranslate nohighlight">\(\h_t(\rm{for\ }t=1,2,\cds,\tau-1)\)</span> is given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\delta_t^h=\pd\f_t^h\od((\W_o\cd\bs\delta_t^o)+(\W_h\cd\bs\delta_{t+1}^h))\)</span></p>
</div>
<p>Note that for <span class="math notranslate nohighlight">\(\h_\tau\)</span>, it depends only on <span class="math notranslate nohighlight">\(\o_\tau\)</span>, therefore</p>
<div class="math notranslate nohighlight">
\[\bs\delta_\tau^h=\pd\f_\tau^h\od(\W_o\cd\bs\delta_\tau^o)\]</div>
<p>For the tanh activation, which is commonly used in RNNs, the derivative of the
activation function with respect to the net values at <span class="math notranslate nohighlight">\(\h_t\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\pd\f_t^h=(\1-\h_t\od\h_t)\]</div>
<p>Finally, note that the net gradients do not have to be computed for <span class="math notranslate nohighlight">\(\h_0\)</span>
or for any of the input neurons <span class="math notranslate nohighlight">\(\x_t\)</span>, since these are leaf nodes in the
backpropagation graph, and thus do not backpropagate the gradients beyond those
neurons.</p>
<p><strong>Stochastic Gradient Descent</strong></p>
<p>The net gradients for the output <span class="math notranslate nohighlight">\(\bs\delta_t^o\)</span> and hidden
<span class="math notranslate nohighlight">\(\bs\delta_t^h\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> can be used to compute the gradients for
the weight matrices and bias vectors at each time point.
However, since an RNN uses parameter sharing across time, the gradients are
obtained by summing up all of the contributions from each time step <span class="math notranslate nohighlight">\(t\)</span>.
Define <span class="math notranslate nohighlight">\(\nabla_{\w_o}^t\)</span> and <span class="math notranslate nohighlight">\(\nabla_{\b_o}^t\)</span> as the gradients of
the weights and biases between the hidden neurons <span class="math notranslate nohighlight">\(\h_t\)</span> and output
neurons <span class="math notranslate nohighlight">\(\o_t\)</span> for time <span class="math notranslate nohighlight">\(t\)</span>.
Using the backpropagation equations, for deep multilayer perceptrons, these
gradients are computed as follows:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\b_o}=\sum_{t=1}^\tau\nabla_{\b_o}^t=\sum_{t=1}^\tau\bs\delta_t^o
\quad\quad\nabla_{\w_o}=\sum_{t=1}^\tau\nabla_{\w_o}^t=\sum_{t=1}^\tau\h_t
\cd(\bs\delta_t^o)^T\]</div>
<p>Likewise, the gradients of the other shared parameters between hidden layers
<span class="math notranslate nohighlight">\(\h_{t-1}\)</span> and <span class="math notranslate nohighlight">\(\h_t\)</span>, and between the input layer <span class="math notranslate nohighlight">\(\x_t\)</span> and
hidden layer <span class="math notranslate nohighlight">\(\h_t\)</span>, are obtained as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla_{\b_h}&amp;=\sum_{t=1}^\tau\nabla_{\b_h}^t=\sum_{t=1}^\tau\bs\delta_t^h
\quad\quad\nabla_{\w_h}=\sum_{t=1}^\tau\nabla_{\W_h}^t=\sum_{t=1}^\tau
\h_{t-1}\cd(\bs\delta_t^h)^T\\\nabla_{\w_i}&amp;=\sum_{t=1}^\tau\nabla_{\w_i}^t=\sum_{t=1}^\tau\x_t\cd(\bs
\delta_t^h)^T\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\W_i=\W_i-\eta\cd\nabla_{\w_i}\quad\W_h=\W_h-\eta\cd\nabla_{\w_h}\quad\b_h=\b_h-\eta\cd\nabla_{\b_h}\)</span></p>
<p><span class="math notranslate nohighlight">\(\W_o=\W_o-\eta\cd\nabla_{\w_o}\quad\b_o=\b_o-\eta\cd\nabla_{\b_o}\)</span></p>
</div>
</div>
<div class="section" id="training-rnns">
<h3>26.1.3 Training RNNs<a class="headerlink" href="#training-rnns" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/Algo26.1.png" src="../_images/Algo26.1.png" />
<p>Note that Line 15 shows the case where the output layer neurons are independent;
if they are not independent we can replace it by
<span class="math notranslate nohighlight">\(\pd\bs{\rm{F}}^o\cd\pd\bs\cE_{\x_t}\)</span>.</p>
<p>In practice, RNNs are trained using subsets or <em>minibatches</em> of input sequences instead of single sequences.
This helps to speed up the computation and convergence of gradient descent,
since minibatches provide better estimates of the bias and weight gradients and
allow the use of vectorized operations.</p>
</div>
<div class="section" id="bidirectional-rnns">
<h3>26.1.4 Bidirectional RNNs<a class="headerlink" href="#bidirectional-rnns" title="Permalink to this headline">¶</a></h3>
<p>A bidirectional RNN (BRNN) extends the RNN model to also include information from the future.
In particular, a BRNN maintains a backward hidden state vector
<span class="math notranslate nohighlight">\(\b_t\in\R^m\)</span> that depends on the next backward hidden state
<span class="math notranslate nohighlight">\(\b_{t+1}\)</span> and the current input <span class="math notranslate nohighlight">\(\x_t\)</span>.
The output at time <span class="math notranslate nohighlight">\(t\)</span> is a function of both <span class="math notranslate nohighlight">\(\h_t\)</span> and <span class="math notranslate nohighlight">\(\b_t\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\h_t=f^h(\W_{ih}^T\x_t+\W_h^T\h_{t-1}+\b_h)\)</span></p>
<p><span class="math notranslate nohighlight">\(\b_t=f^b(\W_{ib}^T\x_t+\W_b^T\b_{t+1}+\b_b)\)</span></p>
</div>
<p>Also, a BRNN needs two initial state vectors <span class="math notranslate nohighlight">\(\h_0\)</span> and
<span class="math notranslate nohighlight">\(\b_{\tau+1}\)</span> to compute <span class="math notranslate nohighlight">\(\b_1\)</span> and <span class="math notranslate nohighlight">\(\b_\tau\)</span>, respectively.
These are usually set to <span class="math notranslate nohighlight">\(\0\in\R^m\)</span>.
The forward and backward hidden states are computed independently, with the
forward hidden states omputed by considering the input sequence in the forward
direction, and with the backward hidden states computed by considering the
sequence in reverse order.
The output at time <span class="math notranslate nohighlight">\(t\)</span> is computed only when both <span class="math notranslate nohighlight">\(\h_t\)</span> and <span class="math notranslate nohighlight">\(\b_t\)</span> are available, and is given as</p>
<div class="math notranslate nohighlight">
\[\o_t=f^o(\W_{ho}^T\h_t+\W_{bo}^T\b_t+\b_o)\]</div>
</div>
</div>
<div class="section" id="gated-rnns-long-short-term-memory-networks">
<h2>26.2 Gated RNNs: Long Short-Term Memory Networks<a class="headerlink" href="#gated-rnns-long-short-term-memory-networks" title="Permalink to this headline">¶</a></h2>
<p>One of the problems in training RNNs is their susceptibility to either the
<em>vanishing gradient</em> or <em>exploding gradient</em> problem.
For example, consider the task of computing the net gradient vector
<span class="math notranslate nohighlight">\(\bs\delta_t^h\)</span> for the hidden layer at time <span class="math notranslate nohighlight">\(t\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^h=\pd\f_t^h\od((\W_o\cd\bs\delta_t^o)+(\W_h\cd\bs\delta_{t+1}^h))\]</div>
<p>Assume for simplicity that we use a linear activation function, i.e.,
<span class="math notranslate nohighlight">\(\pd\f_t^h=\1\)</span>, and let us ignore the net gradient vector for the output
layer, focusing only on the dependence on the hidden layers.
Then for an input sequence of length <span class="math notranslate nohighlight">\(\tau\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^h=\W_h\cd\bs\delta_{t+1}^h=\W_h(\W_h\cd\bs\delta_{t+2}^h)=
\W_h^2\cd\bs\delta_{t+2}^h=\cds=\W_h^{\tau-t}\cd\bs\delta_\tau^h\]</div>
<p>We can observe that the net gradient from time <span class="math notranslate nohighlight">\(\tau\)</span> affects the net
gradient vector at time <span class="math notranslate nohighlight">\(t\)</span> as a function of <span class="math notranslate nohighlight">\(\W_h^{\tau-t}\)</span>, i.e.,
as powers of the hidden weight matrix <span class="math notranslate nohighlight">\(\W_h\)</span>.
Let the <em>spectral radius</em> of <span class="math notranslate nohighlight">\(\W_h\)</span>, defined as the absolute value of its
largest eigenvalue, be given as <span class="math notranslate nohighlight">\(|\ld_1|\)</span>.
It turns out that if <span class="math notranslate nohighlight">\(|\ld_1|&lt;1\)</span>, then <span class="math notranslate nohighlight">\(\lv\W_h^k\rv\ra 0\)</span> as
<span class="math notranslate nohighlight">\(k\ra\infty\)</span>, that is, the gradients vanish as we train on long sequences.
On the other hand, if <span class="math notranslate nohighlight">\(|\ld_1|&gt;1\)</span>, the nat least one element of
<span class="math notranslate nohighlight">\(\W_h^k\)</span> becomes unbounded and thus <span class="math notranslate nohighlight">\(\lv\W_h^k\rv\ra\infty\)</span> as
<span class="math notranslate nohighlight">\(k\ra\infty\)</span>, that is, the gradients explode as we train on long
sequences.
Therefore, for the error to neither vanish nor explode, the spectral radius of
<span class="math notranslate nohighlight">\(\W_h\)</span> should remian 1 or very close to it.</p>
<p>Long short-term memory (LSTM) networks alleviate the vanishing gradients problem
by using <em>gate neurons</em> to control access to the hidden states.
Consider the <span class="math notranslate nohighlight">\(m\)</span>-dimensional hidden state vector <span class="math notranslate nohighlight">\(\h_t\in\R^m\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.
In a regular RNN, we update the hidden state as follows:</p>
<div class="math notranslate nohighlight">
\[\h_t=f^h(\W_i^T\x_t+\W_h^T\h_{t-1}+\b_h)\]</div>
<p>Let <span class="math notranslate nohighlight">\(\g\in\{0,1\}^m\)</span> be a binary vector.
If we take the element-wise product of <span class="math notranslate nohighlight">\(\g\)</span> and <span class="math notranslate nohighlight">\(\h_t\)</span>, namely,
<span class="math notranslate nohighlight">\(\g\od\h_t\)</span>, then elements of <span class="math notranslate nohighlight">\(\g\)</span> act as gates that either allow
the corresponding element of <span class="math notranslate nohighlight">\(\h_t\)</span> to be retained or set to zero.
The vector <span class="math notranslate nohighlight">\(\g\)</span> thus acts as logical gate that allows selected elements of
<span class="math notranslate nohighlight">\(\h_t\)</span> to be remembered or fogotten.
However, for backpropagation we need <em>differentiable gates</em>, for which we use
sigmoid activation on the gate neurons so that their value lies in the range
<span class="math notranslate nohighlight">\([0,1]\)</span>.
Like a logical gate, such neurons allow the inputs to be completely remembered
if the value is 1, or forgotten if the value is 0.
In addition, they allow a weighted memory, allowing partial remembrance of the
elements of <span class="math notranslate nohighlight">\(\h_t\)</span>, for values between 0 and 1.</p>
<div class="section" id="forget-gate">
<h3>26.2.1 Forget Gate<a class="headerlink" href="#forget-gate" title="Permalink to this headline">¶</a></h3>
<p>We consider an RNN with a <em>forget gate</em>.
Let <span class="math notranslate nohighlight">\(\h_t\in\R^m\)</span> be the hidden state vector, and let <span class="math notranslate nohighlight">\(\bs\phi_t\in\R^m\)</span> be a forget gate vector.
Both these vectors have the same number of neurons, <span class="math notranslate nohighlight">\(m\)</span>.</p>
<p>In a regular RNN, assuming tanh activation, the hidden state vector is updated unconditionally, as follows:</p>
<div class="math notranslate nohighlight">
\[\h_t=\tanh(\W_i^T\x_t+\W_h^T\h_{t-1}+\b_h)\]</div>
<p>Instead of directly updating <span class="math notranslate nohighlight">\(\h_t\)</span>, we will employ the forget gate
neurons to control how much of the prvious hidden state vector to forget when
computing its new value, and also to control how to update it in light of the
new input <span class="math notranslate nohighlight">\(\x_t\)</span>.</p>
<p>Given input <span class="math notranslate nohighlight">\(\x_t\)</span> and previous hidden state <span class="math notranslate nohighlight">\(\h_{t-1}\)</span>, we first
compute a candidate update vector <span class="math notranslate nohighlight">\(\u_t\)</span>, as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\u_t=\tanh(\W_u^T\x_t+\W_{hu}^T\h_{t-1}+\b_u)\)</span></p>
</div>
<p>The candidate update vector <span class="math notranslate nohighlight">\(\u_t\)</span> is essentially the unmodified hidden state vector, as in a regular RNN.</p>
<p>Using the forget gate, we can compute the new hidden state vector as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\h_t=\bs\phi_t\od\h_{t-1}+(1-\bs\phi_t)\od\u_t\)</span></p>
</div>
<p>We can see that the new hidden state vector retains a fraction of the previous
hidden state values, and a (complementary) fraction of the candidate update
values.
Observe that if <span class="math notranslate nohighlight">\(\bs\phi_t=\0\)</span>, i.e., if we want to entirely forget the
previous hidden state, then <span class="math notranslate nohighlight">\(\1-\bs\phi_t=\1\)</span>, which means that the hidden
state will be updated completely at each time step just like in a regular RNN.
Finally, given the hidden state <span class="math notranslate nohighlight">\(\h_t\)</span>, we can compute the output vector <span class="math notranslate nohighlight">\(\o_t\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[\o_t=f^o(\W_o^T\h_t+\b_o)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\phi_t=\sg(\W_\phi^T\x_t+\W_{h\phi}^T\h_{t-1}+\b_\phi)\)</span></p>
</div>
<p>where we use a sigmoid activation function, denoted <span class="math notranslate nohighlight">\(\sg\)</span>, to ensure that
all the neuron values are in the range <span class="math notranslate nohighlight">\([0,1]\)</span>, denoting the extent to
which the corresponding previous hidden state values should be forgotten.</p>
<p>A forget gate vector <span class="math notranslate nohighlight">\(\bs\phi_t\)</span> is a layer that depends on the previous
hidden state layer <span class="math notranslate nohighlight">\(\h_{t-1}\)</span> and the current input layer <span class="math notranslate nohighlight">\(\x_t\)</span>;
these connections are fully connected, and are specified by the corresponding
weight matrices <span class="math notranslate nohighlight">\(\W_{h\phi}\)</span> and <span class="math notranslate nohighlight">\(\W_{\phi}\)</span>, and the bias vector
<span class="math notranslate nohighlight">\(\b_\phi\)</span>.
On the other hand, the output of the forget gate layer <span class="math notranslate nohighlight">\(\bs\phi_t\)</span> needs
to modify the previous hidden state layer <span class="math notranslate nohighlight">\(\h_{t-1}\)</span>, and therefore, both
<span class="math notranslate nohighlight">\(\bs\phi_t\)</span> and <span class="math notranslate nohighlight">\(\h_{t-1}\)</span> feed into what is essentially a new
<em>element-wise</em> product layer.
Finally, the output of this element-wise product layer is used as input to the
new hidden layer <span class="math notranslate nohighlight">\(\h_t\)</span> that also takes input from another element-wise
gate that computes the output from the candidate update vector <span class="math notranslate nohighlight">\(\u_t\)</span> and
the complemented forget gate, <span class="math notranslate nohighlight">\(\1-\bs\phi_t\)</span>.
Thus, unlike regular layers that are fully connected and have a weight matrix
and bias vector between the layers, the connections between <span class="math notranslate nohighlight">\(\bs\phi_t\)</span>
and <span class="math notranslate nohighlight">\(\h_t\)</span> via the element-wise layer are all one-to-one, and the weights
are fixed at the value 1 with bias 0.
Likewise the connections between <span class="math notranslate nohighlight">\(\u_t\)</span> and <span class="math notranslate nohighlight">\(\h_t\)</span> via the other
element-wise layer are also one-to-one, with weights fixed at 1 and bias at 0.</p>
</div>
<div class="section" id="computing-net-gradients">
<h3>Computing Net Gradients<a class="headerlink" href="#computing-net-gradients" title="Permalink to this headline">¶</a></h3>
<p>An RNN with a forget gate has the following parameters it needs to learn, namely
the weight matrices <span class="math notranslate nohighlight">\(\W_u,\W_{hu},\W_\phi,\W_{h\phi},\W_o\)</span>, and the bias
vectors <span class="math notranslate nohighlight">\(\b_u,\b_\phi,\b_o\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\bs\delta_t^o, \bs\delta_t^h, \bs\delta_t^\phi, \bs\delta_t^u\)</span> denote
the net gradient vectors at the output, hidden, forget gate, and candidate
update layers, respectively.
During backpropagation, we need to compute the net gradients at each layer.
The net gradients at the outputs are computed by considering the partial
derivatives of the activation function <span class="math notranslate nohighlight">\(\pd\f_t^o\)</span> and the error function
<span class="math notranslate nohighlight">\(\pd\bs\cE_{\x_t}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^o=\pd\f_t^o\od\pd\bs\cE_{\x_t}\]</div>
<p>For the other layers, we can reverse all the arrows to determine the dependencies between the layers.
Therefore, to compute the net gradient for the update layer
<span class="math notranslate nohighlight">\(\bs\delta_t^u\)</span>, notice that in backpropagation it has only one incoming
edge from <span class="math notranslate nohighlight">\(\h_t\)</span> via the element-wise product
<span class="math notranslate nohighlight">\((\1-\bs\phi_t)\od\u_t\)</span>.
The net gradient <span class="math notranslate nohighlight">\(\delta_{ti}^u\)</span> at update layer neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\delta_{ti}^u=\frac{\pd\cE_\x}{\pd net_{ti}^u}=\frac{\pd\cE_\x}
{\pd net_{ti}h}\cd\frac{\pd net_{ti}^h}{\pd u_{ti}}\cd\frac{\pd u_{ti}}
{\pd net_{ti}^u}=\delta_{ti}^h\cd(1-\phi_{ti})\cd(1-u_{ti}^2)\]</div>
<p>where <span class="math notranslate nohighlight">\(\frac{\pd net_{ti}^h}{\pd u_{ti}}=\frac{\pd}{\pd u_{ti}}\)</span>
<span class="math notranslate nohighlight">\(\{\phi_{ti}\cd h_{t-1,i}+(1-\phi_{ti})\cd u_{ti}\}=1-\phi_{ti}\)</span>, and we
use the fact that the update layer uses a tanh activation function.
Across all neurons, we obtain the net gradient at <span class="math notranslate nohighlight">\(\u_t\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^u=\bs\delta_t^h\od(\1-\bs\phi_t)\od(\1-\u_t\od\u_t)\]</div>
<p>To compute the net gradient vector for the forget gate, we observe that there
are two incoming flows into <span class="math notranslate nohighlight">\(\bs\phi_t\)</span> during backpropagation-one from
<span class="math notranslate nohighlight">\(\h_t\)</span> via the element-wise product <span class="math notranslate nohighlight">\(\bs\phi_t\od\h_{t-1}\)</span>, and the
other also from <span class="math notranslate nohighlight">\(\bs\h_t\)</span> via the element-wise product
<span class="math notranslate nohighlight">\((\1-\bs\phi_t)\od\u_t\)</span>.
Therefore, the net gradient <span class="math notranslate nohighlight">\(\delta_{ti}^\phi\)</span> at forget gate neuron <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\delta_{ti}^\phi=\frac{\pd\cE_\x}{\pd net_{ti}^\phi}=\frac{\pd\cE_\x}
{\pd net_{ti}^h}\cd\frac{\pd net_{ti}^h}{\pd\phi_{ti}}\cd\frac{\pd\phi_{ti}}
{\pd net_{ti}^\phi}=\delta_{ti}^h\cd(h_{t-1,i}-u_{ti})\cd\phi_{ti}
(1-\phi_{ti})\]</div>
<p>where <span class="math notranslate nohighlight">\(\frac{\pd net_{ti}^h}{\pd\phi_{ti}}=\frac{\pd}{\pd\phi_{ti}}\)</span>
<span class="math notranslate nohighlight">\(\{\phi_{ti}\cd h_{t-1,i}+(1-\phi_{ti})\cd u_{ti}\}=h_{t-1,i}-u_{ti}\)</span>, and
we use the fact that the forget gate uses a sigmoid activation function.
Across all neurons, we obtain the net gradient at <span class="math notranslate nohighlight">\(\bs\phi_t\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^\phi=\bs\delta_t^h\od(\h_{t-1}-\u_t)\od\bs\phi_t\od(\1-\bs\phi_t)\]</div>
<p>We can observe that if we reverse the arrows, <span class="math notranslate nohighlight">\(\bs\delta_t^h\)</span> depends on
the gradients at the output layer <span class="math notranslate nohighlight">\(\o_t\)</span>, the forget gate layer
<span class="math notranslate nohighlight">\(\bs\phi_{t+1}\)</span>, the update layer <span class="math notranslate nohighlight">\(\u_{t+1}\)</span>, and on the hidden
layer <span class="math notranslate nohighlight">\(\h_{t+1}\)</span> via the element-wise product
<span class="math notranslate nohighlight">\(\h_{t+1}\od\bs\phi_{t+1}\)</span>.
The output, forget and update layers are treated as in a regular RNN.
However, due to the element-wise layer, the flow from <span class="math notranslate nohighlight">\(\h_{t+1}\)</span> is handled as follows:</p>
<div class="math notranslate nohighlight">
\[\frac{\pd\cE_{\x_t}}{\pd net_{t+1,i}^h}\cd\frac{\pd net_{t+1,i}^h}
{\pd h_{ti}}\cd\frac{\pd h_{ti}}{\pd net_{ti}^h}=\delta_{t+1,i}^h\cd
\phi_{t+1,i}\cd 1=\delta_{t+1,i}^h\cd\phi_{t+1,i}\]</div>
<p>where <span class="math notranslate nohighlight">\(\frac{\pd net_{t+1,i}^h}{\pd h_{ti}}=\frac{\pd}{\pd h_{ti}}\)</span>
<span class="math notranslate nohighlight">\(\{\phi_{t+1,i}\cd h_{ti}+(1-\phi_{t+1,i})\cd u_{t+1,i}\}=\phi_{t+1,i}\)</span>,
and we used the fact that <span class="math notranslate nohighlight">\(\h_t\)</span> implicitly uses an identity activation
function.
Across all the hidden neurons at time <span class="math notranslate nohighlight">\(t\)</span>, the net gradient vector
component from <span class="math notranslate nohighlight">\(\h_{t+1}\)</span> is given as
<span class="math notranslate nohighlight">\(\bs\delta_{t+1}^h\od\bs\phi_{t+1}\)</span>.
Considering all the layers, including the output, forget, update and element-wise layers, the complete net gradient vector at the hidden layer at time
<span class="math notranslate nohighlight">\(t\)</span> is given as:</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^h=\W_o\bs\delta_t^o+\W_{h\phi}\bs\delta_{t+1}^\phi+\W_{hu}\bs
\delta_{t+1}^u+(\bs\delta_{t+1}^h\od\bs\phi_{t+1})\]</div>
</div>
<div class="section" id="long-short-term-memory-lstm-networks">
<h3>26.2.2 Long Short-Term Memory (LSTM) Networks<a class="headerlink" href="#long-short-term-memory-lstm-networks" title="Permalink to this headline">¶</a></h3>
<p>LSTMs use differentiable gate vectors to control the hidden state vector
<span class="math notranslate nohighlight">\(\h_t\)</span>, as well as another vector <span class="math notranslate nohighlight">\(\c_t\in\R^m\)</span> called the
<em>internal memory</em> vector.
In particular, LSTMs utilize three <em>gate vectors</em>: an input gate vector
<span class="math notranslate nohighlight">\(\bs\kappa_t\in\R^m\)</span>, a forget gate vector <span class="math notranslate nohighlight">\(\bs\phi_t\in\R^m\)</span>, and
an output get vector <span class="math notranslate nohighlight">\(\bs\omega_t\in\R^m\)</span>.
Like a regular RNN, an  LSTM also maintains a hidden state vecftor for each time step.
However, the content of the hidden vector is selectively copied from the
internal memory vector via the output gate, with the internal memory being
updated via the input gate and parts of it forgotten via the forget gate.</p>
<p>Let <span class="math notranslate nohighlight">\(\cX=\lag\x_1,\x_2,\cds,\x_\tau\rag\)</span> denote a sequence of <span class="math notranslate nohighlight">\(d\)</span>-dimensional input vectors of length <span class="math notranslate nohighlight">\(\tau\)</span>,
<span class="math notranslate nohighlight">\(\cY=\lag\y_1,\y_2,\cds,\y_\tau\rag\)</span> the sequence of <span class="math notranslate nohighlight">\(p\)</span>-dimensional
response vectors, and <span class="math notranslate nohighlight">\(\cl{O}=\lag\o_1,\o_2,\cds,\o_\tau\rag\)</span> the
<span class="math notranslate nohighlight">\(p\)</span>-dimensional output sequence from an LSTM.
At each time step <span class="math notranslate nohighlight">\(t\)</span>, the three gate vectors are updated as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\kappa_t=\sigma(\W_\kappa^T\x_t+\W_{h\kappa}^T\h_{t-1}+\b_\kappa)\)</span></p>
<p><span class="math notranslate nohighlight">\(\bs\phi_t=\sigma(\W_\phi^T\x_t+\W_{h\phi}^T\h_{t-1}+\b_\phi)\)</span></p>
<p><span class="math notranslate nohighlight">\(\bs\omega_t=\sigma(\W_\omega^T\x_t+\W_{h\omega}^T\h_{t-1}+\b_\omega)\)</span></p>
</div>
<p>Here <span class="math notranslate nohighlight">\(\sigma(\cd)\)</span> denotes the sigmoid activation function.
The input gate vector <span class="math notranslate nohighlight">\(\bs\kappa_t\)</span> controls how much of the input vector,
via the candidate update vector <span class="math notranslate nohighlight">\(\u_t\)</span>, is allowed to influence the memory
vector <span class="math notranslate nohighlight">\(\c_t\)</span>.
The forget gate vector <span class="math notranslate nohighlight">\(\bs\phi_t\)</span> controls how much of the previous
memory vector to feget, and finally the output gate vector <span class="math notranslate nohighlight">\(\bs\omega_t\)</span>
controls how much of the memory state is retained for the hidden state.</p>
<p>Given the current input <span class="math notranslate nohighlight">\(\x_t\)</span> and the previous hidden state
<span class="math notranslate nohighlight">\(\h_{t-1}\)</span>, an LSTM first computes a candidate update vector <span class="math notranslate nohighlight">\(\u_t\)</span>
after applying the tanh activation:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\u_t=\tanh(\W_u^T\x_t+\W_{hu}^T\h_{t-1}+\b_u)\)</span></p>
</div>
<p>It then applies the different gates to compute the internal memory and hidden state vectors:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\c_t=\bs\kappa_t\od\u_t+\bs\phi_t\od\c_{t-1}\)</span></p>
<p><span class="math notranslate nohighlight">\(\h_t=\bs\omega_t\od\tanh(\c_t)\)</span></p>
</div>
<p>Finally, the output of the network <span class="math notranslate nohighlight">\(\o_t\)</span> is obtained by applying the
output activaton function <span class="math notranslate nohighlight">\(f^o\)</span> to an affine combination of the hidden
state neuron values:</p>
<div class="math notranslate nohighlight">
\[\o_t=f^o(\W_o^T\h_t+\b_o)\]</div>
<p>LSTMs can typically handle long sequences since the net gradients for the
internal memory states do not vanish over long time steps.
This is because, by design, the memory state <span class="math notranslate nohighlight">\(\c_{t-1}\)</span> at time
<span class="math notranslate nohighlight">\(t-1\)</span> is linked to the memory state <span class="math notranslate nohighlight">\(\c_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> via
implicit weights fixed at 1 and biases fixed at 0, with linear activation.
This allows the error to flow across time steps without vanishing or exploding.</p>
</div>
<div class="section" id="training-lstms">
<h3>26.2.3 Training LSTMs<a class="headerlink" href="#training-lstms" title="Permalink to this headline">¶</a></h3>
<p>During backpropagation the net gradient vector at the output layer at time
<span class="math notranslate nohighlight">\(t\)</span> is computed by considering the partial derivatives of the activation
funciton, <span class="math notranslate nohighlight">\(\pd\f_t^o\)</span> and the error function, <span class="math notranslate nohighlight">\(\pd\bs\cE_{\x_t}\)</span> as
follows:</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^o=\pd\f_t^o\od\pd\bs\cE_{\x_t}\]</div>
<p>where we assume that the output neurons are independent.</p>
<p>In backpropagation there are two incoming connections to the internal memory
vector <span class="math notranslate nohighlight">\(\c_t\)</span>, one from <span class="math notranslate nohighlight">\(\h_t\)</span> and the other from <span class="math notranslate nohighlight">\(\c_{t+1}\)</span>.
Therefore, the net gradient <span class="math notranslate nohighlight">\(\delta_{ti}^c\)</span> at the internal memory neuron
<span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\delta_{ti}^c=\frac{\pd\cE_\x}{\pd net_{ti}^c}&amp;=\frac{\pd\cE_\x}
{\pd net_{ti}^h}\cd\frac{\pd net_{ti}^h}{\pd c_{ti}}\cd\frac{\pd c_{ti}}
{\pd net_{ti}^c}+\frac{\pd\cE_\x}{\pd net_{t+1,i}^c}\cd
\frac{\pd net_{t+1,i}^c}{\pd c_{ti}}\cd\frac{\pd c_{ti}}{\pd net_{ti}^c}\\&amp;=\delta_{ti}^h\cd\omega_{ti}(1-c_{ti}^2)+\delta_{t+1,i}^c\cd\phi_{t+1,i}\end{aligned}\end{align} \]</div>
<p>where we use the fact that the internal memory vector implicitly uses an identity activation function, and furthermore</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd net_{ti}^h}{\pd c_{ti}}&amp;=\frac{\pd}{\pd c_{ti}}\{\omega_{ti}\cd\tanh(c_{ti})\}=\omega_{ti}(1-c_{ti}^2)\\\frac{\pd net_{t+1,i}^c}{\pd c_{ti}}&amp;=\frac{\pd}{\pd c_{ti}}\{\kappa_{t+1,i}
\cd u_{t+1,i}+\phi_{t+1,i}\cd c_{ti}\}=\phi_{t+1,i}\end{aligned}\end{align} \]</div>
<p>The net gradient vector <span class="math notranslate nohighlight">\(\bs\delta_t^c\)</span> at <span class="math notranslate nohighlight">\(\bs\c_t\)</span> is therefore given as:</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^c=\bs\delta_t^h\od\omega_t\od(\1-\c_t\od\c_t)+\bs\delta_{t+1}^c\od\bs\phi_{t+1}\]</div>
<p>The forget gate has only one incoming edge in backpropagation, from
<span class="math notranslate nohighlight">\(\c_t\)</span>, via the element-wise multiplication <span class="math notranslate nohighlight">\(\bs\phi_t\od\c_{t-1}\)</span>,
with sigmoid activation, therefore the net gradient is:</p>
<div class="math notranslate nohighlight">
\[\delta_{ti}^\phi=\frac{\pd\cE_\x}{\pd net_{ti}^\phi}=\frac{\pd\cE_\x}
{\pd net_{ti}^c}\cd\frac{\pd net_{ti}^c}{\pd\phi_{ti}}\cd\frac{\pd\phi_{ti}}
{\pd net_{ti}^\phi}=\delta_{ti}^c\cd c_{t-1,i}\cd\phi_{ti}(1-\phi_{ti})\]</div>
<p>where we used the fact that the forget gate uses sigmoid activation and</p>
<div class="math notranslate nohighlight">
\[\frac{\pd net_{ti}^c}{\pd\phi_{ti}}=\frac{\pd}{\pd\phi_{ti}}\{\kappa_{ti}\cd
u_{ti}+\phi_{ti}\cd c_{t-1,i}\}=c_{t-1,i}\]</div>
<p>Across all forget gate neurons, the net gradient vector is therefore given as</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^phi=\bs\delta_t^c\od\c_{t-1}\od(\1-\bs\phi_t)\od\bs\phi_t\]</div>
<p>The input gate also has only one incoming edge in backpropagation, from
<span class="math notranslate nohighlight">\(\c_t\)</span>, via the element-wise multiplication <span class="math notranslate nohighlight">\(\bs\kappa_t\od\u_t\)</span>,
with sigmoid activation.
In a similar manner, as outlined above for <span class="math notranslate nohighlight">\(\bs\delta_t^\phi\)</span>, the net
gradient <span class="math notranslate nohighlight">\(\bs\delta_t^\kappa\)</span> at the input gate <span class="math notranslate nohighlight">\(\bs\kappa_t\)</span> is
given as:</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^\kappa=\bs\delta_t^c\od\u_t\od(\1-\bs\kappa_t)\od\bs\kappa_t\]</div>
<p>The same reasoning applies to the update candidate <span class="math notranslate nohighlight">\(\u_t\)</span>, which also has
an incoming edge from <span class="math notranslate nohighlight">\(\c_t\)</span> via <span class="math notranslate nohighlight">\(\bs\kappa_t\od\u_t\)</span> and tanh
activation, so the net gradient vector <span class="math notranslate nohighlight">\(\bs\delta_t^u\)</span> at the update layer
is</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^u=\bs\delta_t^c\od\bs\kappa_t\od(\1-\u_t\od\u_t)\]</div>
<p>Likewise, in backpropagation, there is one incoming connection to the output
gate from <span class="math notranslate nohighlight">\(\h_t\)</span> via <span class="math notranslate nohighlight">\(\bs\omega_t\od\tanh(\c)\)</span> with sigmoid
activation, therefore</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^\omega=\bs\delta_t^h\od\tanh(\c_t)\od(\1-\omega_t)\od\omega_t\]</div>
<p>Finally, to compute the net gradients at the hidden layer, note that gradients
flow back to <span class="math notranslate nohighlight">\(\h_t\)</span> from the following layers:
<span class="math notranslate nohighlight">\(\bs\u_{t+1},\bs\kappa_{t+1},\bs\phi_{t+1},\bs\omega_{t+1},\o_t\)</span>.
Therefore, the net gradient vector at the hidden state vector <span class="math notranslate nohighlight">\(\bs\delta_t^h\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^h=\W_o\bs\delta_t^o+\W_{h\kappa}\delta_{t+1}^\kappa+\W_{h\phi}
\bs\delta_{t+1}^\phi+\W_{h\omega}\bs\delta_{t+1}^\omega+\W_{hu}\bs
\delta_{t+1}^u\]</div>
<p>The gradients for the weight matrix and bias vector at the output layer are given as:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\b_o}=\sum_{t=1}^\tau\bs\delta_t^o\quad\quad\nabla_{\w_o}=\sum_{t=1}^\tau\h_t\cd(\bs\delta_t^o)^T\]</div>
<p>Likewise, the gradients for the weight matrices and bias vectors for the other layers are given as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla_{\b_\kappa}=\sum_{t=1}^\tau\bs\delta_t^\kappa\quad\quad
\nabla_{\W_\kappa}=\sum_{t=1}^\tau\x_t\cd(\bs\delta_t^\kappa)^T\quad\quad
\nabla_{\W_{h\kappa}}=\sum_{t=1}^\tau\h_{t-1}\cd(\bs\delta_t^\kappa)^T\\\nabla_{\b_\phi}=\sum_{t=1}^\tau\bs\delta_t^\phi\quad\quad
\nabla_{\W_\phi}=\sum_{t=1}^\tau\x_t\cd(\bs\delta_t^\phi)^T\quad\quad
\nabla_{\W_{h\phi}}=\sum_{t=1}^\tau\h_{t-1}\cd(\bs\delta_t^\phi)^T\\\nabla_{\b_\omega}=\sum_{t=1}^\tau\bs\delta_t^\omega\quad\quad
\nabla_{\W_\omega}=\sum_{t=1}^\tau\x_t\cd(\bs\delta_t^\omega)^T\quad\quad
\nabla_{\W_{h\omega}}=\sum_{t=1}^\tau\h_{t-1}\cd(\bs\delta_t^\omega)^T\\\nabla_{\b_u}=\sum_{t=1}^\tau\bs\delta_t^u\quad\quad
\nabla_{\W_u}=\sum_{t=1}^\tau\x_t\cd(\bs\delta_t^u)^T\quad\quad
\nabla_{\W_{hu}}=\sum_{t=1}^\tau\h_{t-1}\cd(\bs\delta_t^u)^T\end{aligned}\end{align} \]</div>
<p>Given these gradients, we can use the stochastic gradient descent approach to train the network.</p>
</div>
</div>
<div class="section" id="convolutional-neural-networks">
<h2>26.3 Convolutional Neural Networks<a class="headerlink" href="#convolutional-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>A convolutional neural network (CNN) is essentially a <em>localized</em> and sparse
feedforward MLP that is designed to exploit spatial and/or temporal structure in
the input data.
In a regular MLP all of the neurons in layer <span class="math notranslate nohighlight">\(l\)</span> are connected to all of the neurons in layer <span class="math notranslate nohighlight">\(l+1\)</span>.
In contrast, a CNN connects a contiguous or adjacent subset of neurons in layer
<span class="math notranslate nohighlight">\(l\)</span> to a single neuron in the next layer <span class="math notranslate nohighlight">\(l+1\)</span>.
Different sliding windows comprising contiguous subsets of neurons in layer
<span class="math notranslate nohighlight">\(l\)</span> connect to different neurons in layer <span class="math notranslate nohighlight">\(l+1\)</span>.
Furthermore, all of these sliding windows use <em>parameter sharing</em>, that is, the
same set of weights, called a <em>ﬁlter</em>, is used for all sliding windows.
Finally, different ﬁlters are used to automatically extract features from layer <span class="math notranslate nohighlight">\(l\)</span> for use by layer <span class="math notranslate nohighlight">\(l+1\)</span>.</p>
<div class="section" id="convolutions">
<h3>26.3.1 Convolutions<a class="headerlink" href="#convolutions" title="Permalink to this headline">¶</a></h3>
<p><strong>1D Convolution</strong></p>
<p>Let <span class="math notranslate nohighlight">\(\x=(x_1,x_2,\cds,x_n)^T\)</span> be an input vector (a one-way or 1D input) with <span class="math notranslate nohighlight">\(n\)</span> points.
It is assumed that the input points <span class="math notranslate nohighlight">\(x_i\)</span> are not independent, but rather,
there are dependencies between successive points.
Let <span class="math notranslate nohighlight">\(\w=(w_1,w_2,\cds,w_k)^T\)</span> be a vector of weights, called a <em>1D filter</em>, with <span class="math notranslate nohighlight">\(k\leq n\)</span>.
Here <span class="math notranslate nohighlight">\(k\)</span> is also called the <em>window size</em>.
Let <span class="math notranslate nohighlight">\(\x_k(i)\)</span> denote the window of <span class="math notranslate nohighlight">\(\x\)</span> of length <span class="math notranslate nohighlight">\(k\)</span> starting at position <span class="math notranslate nohighlight">\(i\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[\x_k(i)=(x_i,x_{i+1},x_{i+2},\cds,x_{i+k-1})^T\]</div>
<p>with <span class="math notranslate nohighlight">\(1\leq i\leq n-k+1\)</span>.
Given a vector <span class="math notranslate nohighlight">\(\a\in\R^k\)</span>, define the summation operator as one that adds all the elements of the vector.
That is,</p>
<div class="math notranslate nohighlight">
\[\rm{sum}(\a)=\sum_{i=1}^ka_i\]</div>
<p>A <em>1D convolution</em> between <span class="math notranslate nohighlight">\(\x\)</span> and <span class="math notranslate nohighlight">\(\w\)</span>, denoted by the asterisk symbol <span class="math notranslate nohighlight">\(*\)</span>, is defined as</p>
<div class="math notranslate nohighlight">
\[\x*\w=\bp\rm{sum}(\x_k(1)\od\w)&amp;\cds&amp;\rm{sum}(\x_k(n-k+1)\od\w)\ep^T\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\sum(\x_k(i)\od\w)=\sum_{j=1}^kx_{i+j-1}\cd w_j\)</span></p>
</div>
<p>for <span class="math notranslate nohighlight">\(i=1,2,\cds,n-k+1\)</span>.
We can see that the convolution of <span class="math notranslate nohighlight">\(\x\in\R^n\)</span> and <span class="math notranslate nohighlight">\(\w\in\R^k\)</span> results in a vector of length <span class="math notranslate nohighlight">\(n-k+1\)</span>.</p>
<p><strong>2D Convolution</strong></p>
<p>Let <span class="math notranslate nohighlight">\(\X\)</span> be an <span class="math notranslate nohighlight">\(n\times n\)</span> input matrix, and let <span class="math notranslate nohighlight">\(\W\)</span> be a
<span class="math notranslate nohighlight">\(k\times k\)</span> matrix of weights, called a <em>2D filter</em>, with <span class="math notranslate nohighlight">\(k\leq n\)</span>.
Here <span class="math notranslate nohighlight">\(k\)</span> is called the window size.
Let <span class="math notranslate nohighlight">\(\X_k(i,j)\)</span> denote the <span class="math notranslate nohighlight">\(k\times k\)</span> submatrix of <span class="math notranslate nohighlight">\(\X\)</span>
starting at row <span class="math notranslate nohighlight">\(i\)</span> and column <span class="math notranslate nohighlight">\(j\)</span>, defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\X_k(i,j)=\bp x_{i,j}&amp;x_{i,j+1}&amp;\cds&amp;x_{i,j+k-1}\\x_{i+1,j}&amp;x_{i+1,j+1}&amp;\cds
&amp;x_{i+1,j+k-1}\\\vds&amp;\vds&amp;\dds&amp;\vds\\x_{i+k-1,j}&amp;x_{i+k-1,j+1}&amp;\cds&amp;
x_{i+k-1,j+k-1}\ep\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(1\leq i,j\leq n-k+1\)</span>.
Given a <span class="math notranslate nohighlight">\(k\times k\)</span> matrix <span class="math notranslate nohighlight">\(\A\in\R^{k\times k}\)</span>, define the
summation operator as one that adds all the elements of the matrix.</p>
<div class="math notranslate nohighlight">
\[\rm{sum}(\A)=\sum_{i=1}^k\sum_{j=1}^ka_{i,j}\]</div>
<p>where <span class="math notranslate nohighlight">\(a_{i,j}\)</span> is the element of <span class="math notranslate nohighlight">\(\A\)</span> at row <span class="math notranslate nohighlight">\(i\)</span> and column <span class="math notranslate nohighlight">\(j\)</span>.
The <em>2D convolution</em> of <span class="math notranslate nohighlight">\(\X\)</span> and <span class="math notranslate nohighlight">\(\W\)</span>, denoted <span class="math notranslate nohighlight">\(\X*\W\)</span>, is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\X*\W=\bp\rm{sum}(\X_k(1,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(1,n-k+1)\od\W)\\
\rm{sum}(\X_k(2,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(2,n-k+1)\od\W)\\\vds&amp;\dds&amp;\vds\\
\rm{sum}(\X_k(n-k+1,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(n-k+1,n-k+1)\od\W)\ep\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\rm{\sum}(\X_k(i,j)\od\W)=\sum_{a=1}^k\sum_{b=1}^kx_{i+a-1,j+b-1}\cd w_{a,b}\)</span></p>
</div>
<p>for <span class="math notranslate nohighlight">\(i,j=1,2,\cds,n-k+1\)</span>.
The convolution of <span class="math notranslate nohighlight">\(\X\in\R^{n\times n}\)</span> and <span class="math notranslate nohighlight">\(\W\in\R^{k\times k}\)</span>
results in a <span class="math notranslate nohighlight">\((n-k+1)\times(n-k+1)\)</span> matrix.</p>
<p><strong>3D Convolution</strong></p>
<p>The three-dimensional matrix is also called a <em>3D tensor</em>.
The first dimension comprises the rows, the second the columns, and the third the <em>channels</em>.
Let <span class="math notranslate nohighlight">\(\X\)</span> be an <span class="math notranslate nohighlight">\(n\times n\times m\)</span> tensor, with <span class="math notranslate nohighlight">\(n\)</span> rows, <span class="math notranslate nohighlight">\(n\)</span> columns and <span class="math notranslate nohighlight">\(m\)</span> channels.
The assumption is that the input <span class="math notranslate nohighlight">\(\X\)</span> is a collection of <span class="math notranslate nohighlight">\(n\times n\)</span>
matrices obtained by applying <span class="math notranslate nohighlight">\(m\)</span> <em>filters</em>, which specify the <span class="math notranslate nohighlight">\(m\)</span>
channels.</p>
<p>Let <span class="math notranslate nohighlight">\(\W\)</span> be a <span class="math notranslate nohighlight">\(k\times k\times r\)</span> tensor of weights, called a
<em>3D filter</em>, with <span class="math notranslate nohighlight">\(k\leq n\)</span> and <span class="math notranslate nohighlight">\(r\leq m\)</span>.
Let <span class="math notranslate nohighlight">\(\X_k(i,j,q)\)</span> denote the <span class="math notranslate nohighlight">\(k\times k\times r\)</span> subtensor of
<span class="math notranslate nohighlight">\(\X\)</span> starting at row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span> and channel <span class="math notranslate nohighlight">\(q\)</span>,
with <span class="math notranslate nohighlight">\(1\leq i,j\leq n-k+1\)</span> and <span class="math notranslate nohighlight">\(1\leq q\leq m-r+1\)</span>.</p>
<p>Given a <span class="math notranslate nohighlight">\(k\times k\times r\)</span> tensor <span class="math notranslate nohighlight">\(\A\in\R^{k\times k\times r}\)</span>,
define the summation operator as one that adds all the elements of the tensor.</p>
<div class="math notranslate nohighlight">
\[\rm{sum}(\A)=\sum_{i=1}^k\sum_{j=1}^k\sum_{q=1}^ra_{i,j,q}\]</div>
<p>where <span class="math notranslate nohighlight">\(a_{i,j,q}\)</span> is the element of <span class="math notranslate nohighlight">\(\A\)</span> at row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span> and channel <span class="math notranslate nohighlight">\(q\)</span>.
The <em>3D convolution</em> of <span class="math notranslate nohighlight">\(\X\)</span> and <span class="math notranslate nohighlight">\(\W\)</span>, denoted <span class="math notranslate nohighlight">\(\X*\W\)</span>, is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\scriptsize
\X*\W=\left(\begin{array}{ccc}
\rm{sum}(\X_k(1,1,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(1,n-k+1,1)\od\W)\\
\rm{sum}(\X_k(2,1,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(2,n-k+1,1)\od\W)\\
\vds&amp;\dds&amp;\vds\\
\rm{sum}(\X_k(n-k+1,1,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(n-k+1,n-k+1,1)\od\W)\\
\hline
\rm{sum}(\X_k(1,1,2)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(1,n-k+1,2)\od\W)\\
\rm{sum}(\X_k(2,1,2)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(2,n-k+1,2)\od\W)\\
\vds&amp;\dds&amp;\vds\\
\rm{sum}(\X_k(n-k+1,1,2)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(n-k+1,n-k+1,2)\od\W)\\
\hline
\vds&amp;\vds&amp;\vds\\\hline
\rm{sum}(\X_k(1,1,m-r+1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(1,n-k+1,m-r+1)\od\W)\\
\rm{sum}(\X_k(2,1,m-r+1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(2,n-k+1,m-r+1)\od\W)\\
\vds&amp;\dds&amp;\vds\\
\rm{sum}(\X_k(n-k+1,1,m-r+1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(n-k+1,n-k+1,m-r+1)
\od\W)\\\hline
\end{array}\right)\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\rm{\sum}(\X_k(i,j,q)\od\W)=\sum_{a=1}^k\sum_{b=1}^k\sum_{c=1}^rx_{i+a-1,j+b-1,q+c-1}\cd w_{a,b,c}\)</span></p>
</div>
<p>for <span class="math notranslate nohighlight">\(i,j=1,2,\cds,n-k+1\)</span> and <span class="math notranslate nohighlight">\(q=1,2,\cds,m-r+1\)</span>.
We can see that the convolution of <span class="math notranslate nohighlight">\(\X\in\R^{n\times n\times m}\)</span> and
<span class="math notranslate nohighlight">\(\W\in\R^{k\times k\times r}\)</span> results in a
<span class="math notranslate nohighlight">\((n-k+1)\times(n-k+1)\times(m-r+1)\)</span> tensor.</p>
<p><strong>3D Convolutions in CNNs</strong></p>
<p>Typically in CNNs, we use a 3D filter <span class="math notranslate nohighlight">\(\W\)</span> of size
<span class="math notranslate nohighlight">\(k\times k\times m\)</span>, with the number of channels <span class="math notranslate nohighlight">\(r=m\)</span>, the same as
the number of channels in <span class="math notranslate nohighlight">\(\X\in\R^{n\times n\times m}\)</span>.
Let <span class="math notranslate nohighlight">\(\X_k(i,j)\)</span> be the <span class="math notranslate nohighlight">\(k\times k\times m\)</span> subtensor of <span class="math notranslate nohighlight">\(\X\)</span>
starting at row <span class="math notranslate nohighlight">\(i\)</span> and column <span class="math notranslate nohighlight">\(j\)</span>.
Then the 3D convolution of <span class="math notranslate nohighlight">\(\X\)</span> and <span class="math notranslate nohighlight">\(\W\)</span> is given as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\X*\W=\bp\rm{sum}(\X_k(1,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(1,n-k+1)\od\W)\\
\rm{sum}(\X_k(2,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(2,n-k+1)\od\W)\\\vds&amp;\dds&amp;\vds\\
\rm{sum}(\X_k(n-k+1,1)\od\W)&amp;\cds&amp;\rm{sum}(\X_k(n-k+1,n-k+1)\od\W)\ep\end{split}\]</div>
<p>We can see that when <span class="math notranslate nohighlight">\(\W\in\R^{k\times k\times m\)</span>, its 3D convolution with
<span class="math notranslate nohighlight">\(\X\in\R^{n\times n\times m}\)</span> results in a <span class="math notranslate nohighlight">\((n-k+1)\times(n-k+1)\)</span>
matrix, since there is no freedom to move in the third dimension.
Henceforth, we will always assume that a 3D filter
<span class="math notranslate nohighlight">\(\W\in\R^{k\times k\times m}\)</span> has the same number of channels as the
tensor <span class="math notranslate nohighlight">\(\X\)</span> on which it is applied.
Since the number of channels is fixed based on <span class="math notranslate nohighlight">\(\X\)</span>, the only parameter
needed to fully specify <span class="math notranslate nohighlight">\(\W\)</span> is the window size <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div>
<div class="section" id="bias-and-activation-functions">
<h3>26.3.2 Bias and Activation Functions<a class="headerlink" href="#bias-and-activation-functions" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\Z^l\)</span> be an <span class="math notranslate nohighlight">\(n_l\times n_l\times m_l\)</span> tensor of neurons at
layer <span class="math notranslate nohighlight">\(l\)</span> so that <span class="math notranslate nohighlight">\(z_{i,j,q}^l\)</span> denotes the value of the neuron at
row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span> and channel <span class="math notranslate nohighlight">\(q\)</span> for layer <span class="math notranslate nohighlight">\(l\)</span>, with
<span class="math notranslate nohighlight">\(1\leq i,j\leq n_l\)</span> and <span class="math notranslate nohighlight">\(1\leq q\leq m_l\)</span>.</p>
<p><strong>Filter Bias</strong></p>
<p>Let <span class="math notranslate nohighlight">\(\W\)</span> be a <span class="math notranslate nohighlight">\(k\times k\times m_l\)</span> 3D filter.
Let <span class="math notranslate nohighlight">\(b\in\R\)</span> be a scalar bias value for <span class="math notranslate nohighlight">\(\W\)</span>, and let
<span class="math notranslate nohighlight">\(\Z_k^l(i,j)\)</span> denote the <span class="math notranslate nohighlight">\(k\times k\times m_l\)</span> subtensor of
<span class="math notranslate nohighlight">\(\Z^l\)</span> at position <span class="math notranslate nohighlight">\((i,j)\)</span>.
Then, the net signal at neuron <span class="math notranslate nohighlight">\(z_{i,j}^{l+1}\)</span> in layer <span class="math notranslate nohighlight">\(l+1\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[net_{i,j}^{l+1}=\rm{sum}(\Z_k^l(i,j)\od\W+b)\]</div>
<p>and the value of the neuron <span class="math notranslate nohighlight">\(\z_{i,j}^{l+1}\)</span> is obtained by applying some
activation function <span class="math notranslate nohighlight">\(f\)</span> to the net signal</p>
<div class="math notranslate nohighlight">
\[z_{i,j}^{l+1}=f(\rm{sum}(\Z_k^l(i,j)\od\W+b))\]</div>
<p>The activation function can be any of the ones typically used in neural networks.
In the language of convolutions, the values of the neurons in layer <span class="math notranslate nohighlight">\(l+1\)</span> is given as follows:</p>
<div class="math notranslate nohighlight">
\[\Z^{l+1}=f((\Z^l*\W)\oplus b)\]</div>
<p><strong>Multiple 3D Filters</strong></p>
<p>We can observe that one 3D filter <span class="math notranslate nohighlight">\(\W\)</span> with a corresponding bias term
<span class="math notranslate nohighlight">\(b\)</span> results in a <span class="math notranslate nohighlight">\((n_l-k+1)\times(n_l-k+1)\)</span> matrix of neurons in
layer <span class="math notranslate nohighlight">\(l+1\)</span>.
Therefore, if we desire <span class="math notranslate nohighlight">\(m_{k+1}\)</span> channels in layer <span class="math notranslate nohighlight">\(l+1\)</span>, then we
need <span class="math notranslate nohighlight">\(m_{l+1}\)</span> different <span class="math notranslate nohighlight">\(k\times k\times m_l\)</span> filters <span class="math notranslate nohighlight">\(\W_q\)</span>
with a corresponding bias term <span class="math notranslate nohighlight">\(b_q\)</span>, to obtain the
<span class="math notranslate nohighlight">\((n_l-k+1)\times(n_l-k+1)\times m_{l+1}\)</span> tensor of neuron values at layer
<span class="math notranslate nohighlight">\(l+1\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[\Z^{l+1}=\{z_{i,j,q}^{l+1}=f(\rm{sum}(\Z_k^l(i,j)\od\W_q)+b_q)\}_{i,j=1,2,
\cds,n_l-k+1\rm{\ and\ }q=1,2,\cds,m_{l+1}}\]</div>
<p>which can be written more compactly as</p>
<div class="math notranslate nohighlight">
\[\Z^{l+1}=f((\Z^l*\W_1)\oplus b_1,(\Z^l*\W_2)\oplus b_2,\cds,(\Z^l*\W_{m_{l+1}})\oplus b_{m_{l+1}})\]</div>
<p>where the activation function <span class="math notranslate nohighlight">\(f\)</span> distributes over all of its arguments.</p>
<p>In summary, a convolution layer takes as input the
<span class="math notranslate nohighlight">\(n_l\times n_l\times m_l\)</span> tensor <span class="math notranslate nohighlight">\(\Z^l\)</span> of neurons from layer
<span class="math notranslate nohighlight">\(l\)</span>, and then computes the <span class="math notranslate nohighlight">\(n_{l+1}\times n_{l+1}\times m_{l+1}\)</span>
tensor <span class="math notranslate nohighlight">\(\Z^{l+1}\)</span> of neurons for the next layer <span class="math notranslate nohighlight">\(l+1\)</span> via the
convolution of <span class="math notranslate nohighlight">\(\Z^l\)</span> with a set of <span class="math notranslate nohighlight">\(m_{l+1}\)</span> different 3D filters
of size <span class="math notranslate nohighlight">\(k\times k\times m_l\)</span>, followed by adding the bias and applying
some non-linear activation function <span class="math notranslate nohighlight">\(f\)</span>.
Note that each 3D filter applied to <span class="math notranslate nohighlight">\(\Z^l\)</span> results in a new channel in layer <span class="math notranslate nohighlight">\(l+1\)</span>.
Therefore, <span class="math notranslate nohighlight">\(m_{l+1}\)</span> filters are used to yield <span class="math notranslate nohighlight">\(m_{l+1}\)</span> channels at layer <span class="math notranslate nohighlight">\(l+1\)</span>.</p>
</div>
<div class="section" id="padding-and-striding">
<h3>26.3.3 Padding and Striding<a class="headerlink" href="#padding-and-striding" title="Permalink to this headline">¶</a></h3>
<p>One of the issues with the convolution operation is that the size of the tensors
will necessarily decrease in each successive CNN layer.
If layer <span class="math notranslate nohighlight">\(l\)</span> has size <span class="math notranslate nohighlight">\(n_l\times n_l\times m_l\)</span>, and we use filters
of size <span class="math notranslate nohighlight">\(k\times k\times m_l\)</span>, then each channel in layer <span class="math notranslate nohighlight">\(l+1\)</span> will
have size <span class="math notranslate nohighlight">\((n_l-k+1)\times(n_l-k+1)\)</span>.
That is the number of rows and columns for each successive tensor will shrink by
<span class="math notranslate nohighlight">\(k-1\)</span> and that will limit the number of layers the CNN can have.</p>
<p><strong>Padding</strong></p>
<p>To get around this limitation, a simple solution is to pad each tensor along
both the rows and columns in each channel by some default value, typically zero.
Assume that we add <span class="math notranslate nohighlight">\(p\)</span> rows both on top and bottom, and <span class="math notranslate nohighlight">\(p\)</span> columns both on the left and right.
With padding <span class="math notranslate nohighlight">\(p\)</span>, the implicit size of layer <span class="math notranslate nohighlight">\(l\)</span> tensor is then <span class="math notranslate nohighlight">\((n_l+2p)\times(n_l+2p)\times m_l\)</span>.
Assume that each filter is of size <span class="math notranslate nohighlight">\(k\times k\times m_l\)</span>, and assume there
are <span class="math notranslate nohighlight">\(m_{l+1}\)</span> filters, then the size of the layer <span class="math notranslate nohighlight">\(l+1\)</span> tensor will
be <span class="math notranslate nohighlight">\((n_l+2p-k+1)\times(n_l+2p-k+1)\times m_{l+1}\)</span>.
Since we want to preserve the size of the resulting tensor, we need to have</p>
<div class="math notranslate nohighlight">
\[n_l+2p-k+1\geq n_l\Rightarrow p=\lceil\frac{k-1}{2}\rceil\]</div>
<p>With padding, we can have arbitrarily deep convolutional layers in a CNN.</p>
<p><strong>Striding</strong></p>
<p>Striding is often used to sparsify the number of sliding windows used in the convolutions.
That is, instead of considering all possible windows we increment the index
along both rows and columns by an integer value <span class="math notranslate nohighlight">\(s\geq 1\)</span> called the
<em>stride</em>.
A 3D convolution of <span class="math notranslate nohighlight">\(\Z_l\)</span> of size <span class="math notranslate nohighlight">\(n_l\times n_l\times m_l\)</span> with a
filter <span class="math notranslate nohighlight">\(\W\)</span> of size <span class="math notranslate nohighlight">\(k\times k\times m_l\)</span>, using stride <span class="math notranslate nohighlight">\(s\)</span>,
is given as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\scriptsize
\Z^l*\W=\bp\rm{sum}(\Z_k^l(1,1)\od\W)&amp;\rm{sum}(\Z_k^l(1,1+s)\od\W)&amp;\cds&amp;
\rm{sum}(\Z_k^l(1,1+t\cd s)\od\W)\\\rm{sum}(\Z_k^l(1+s,1)\od\W)&amp;\rm{sum}
(\Z_k^l(1+s,1+s)\od\W)&amp;\cds&amp;\rm{sum}(\Z_k^l(1+s,1+t\cd s)\od\W)\\
\vds&amp;\vds&amp;\dds&amp;\vds\\\rm{sum}(\Z_k^l(1+t\cd s,1)\od\W)&amp;\rm{sum}
(\Z_k^l(1+t\cd s,1+s)\od\W)&amp;\cds&amp;\rm{sum}(\Z_k^l(1+t\cd s,1+t\cd s)\od\W)\ep\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(t=\lfloor\frac{n_l-k}{s}\rfloor\)</span>.
We can observe that using stride <span class="math notranslate nohighlight">\(s\)</span>, the convolution of
<span class="math notranslate nohighlight">\(\Z_l\in\R^{n_l\times n_l\times m_l}\)</span> with
<span class="math notranslate nohighlight">\(\W\in\R^{k\times k\times m_l}\)</span> results in a <span class="math notranslate nohighlight">\((t+1)\times(t+1)\)</span>
matrix.</p>
</div>
<div class="section" id="generalized-aggregation-functions-pooling">
<h3>26.3.4 Generalized Aggregation Functions: Pooling<a class="headerlink" href="#generalized-aggregation-functions-pooling" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\Z^l\)</span> be a <span class="math notranslate nohighlight">\(n_l\times n_l\times m_l\)</span> tensor at layer <span class="math notranslate nohighlight">\(l\)</span>.</p>
<p><strong>Avg-Pooling</strong></p>
<p>If we replace the summation with the average value over the element-wise product
of <span class="math notranslate nohighlight">\(\Z_k^l(i,j,q)\)</span> and <span class="math notranslate nohighlight">\(\W\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[\avg(\Z_k^l(i,j,q)\od\W)=\avg\{\]</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap27.html" class="btn btn-neutral float-right" title="Chapter 27 Regression Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap25.html" class="btn btn-neutral float-left" title="Chapter 25 Neural Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>