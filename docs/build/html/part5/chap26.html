

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 26 Deep Learning &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 27 Regression Evaluation" href="chap27.html" />
    <link rel="prev" title="Chapter 25 Neural Networks" href="chap25.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index5.html">Part 5 Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap23.html">Chapter 23 Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap24.html">Chapter 24 Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap25.html">Chapter 25 Neural Networks</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 26 Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#recurrent-neural-networks">26.1 Recurrent Neural Networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#feed-forward-in-time">26.1.1 Feed-forward in Time</a></li>
<li class="toctree-l4"><a class="reference internal" href="#backpropagation-in-time">26.1.2 backpropagation in Time</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-rnns">26.1.3 Training RNNs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bidirectional-rnns">26.1.4 Bidirectional RNNs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap27.html">Chapter 27 Regression Evaluation</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index5.html">Part 5 Regression</a> &raquo;</li>
        
      <li>Chapter 26 Deep Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part5/chap26.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cE}{\mathcal{E}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-26-deep-learning">
<h1>Chapter 26 Deep Learning<a class="headerlink" href="#chapter-26-deep-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="recurrent-neural-networks">
<h2>26.1 Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>Multilayer perceptrons are feed-forward networks in which the information flows
in only one direction, namely from the input layer to the output layer via the
hidden layers.
In constrast, recurrent neural networks (RNNs) are dynamically driven, with a
<em>feedback</em> loop between two (or more) layers, which makes such networks ideal
for learning from sequence data.</p>
<p>Let <span class="math notranslate nohighlight">\(\cX=\lag\x_1,\x_2,\cds,\x_\tau\rag\)</span> denote a sequence of vectors,
where <span class="math notranslate nohighlight">\(\x_t\in\R^d\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector
<span class="math notranslate nohighlight">\((t=1,2\,\cds,\tau)\)</span>.
Thus, <span class="math notranslate nohighlight">\(\cX\)</span> is an input sequence of length <span class="math notranslate nohighlight">\(\tau\)</span>, with
<span class="math notranslate nohighlight">\(\x_t\)</span> denoting the input at time step <span class="math notranslate nohighlight">\(t\)</span>.
Let <span class="math notranslate nohighlight">\(\cY=\lag\y_1,\y_2,\cds,\y_\tau\rag\)</span> denote a sequence of vectors,
with <span class="math notranslate nohighlight">\(\y_t\in\R^p\)</span> a <span class="math notranslate nohighlight">\(p\)</span>-dimensional vector.
Here <span class="math notranslate nohighlight">\(\cY\)</span> is the desired target or response sequence, with
<span class="math notranslate nohighlight">\(\y_t\)</span> denoting the response vector at time <span class="math notranslate nohighlight">\(t\)</span>.
Finally, let <span class="math notranslate nohighlight">\(\cl{O}=\lag\o_1,\o_2,\cds,\o_\tau\rag\)</span> denote the predicted
or output sequence from the RNN.
Here <span class="math notranslate nohighlight">\(\o_t\in\R^p\)</span> is also a <span class="math notranslate nohighlight">\(p\)</span>-dimensional vector to match the corresponding true response <span class="math notranslate nohighlight">\(\y_t\)</span>.
The task of an RNN is to learn a function that predicts the target sequence
<span class="math notranslate nohighlight">\(\cY\)</span> given the input sequence <span class="math notranslate nohighlight">\(\cX\)</span>.
That is, the predicted output <span class="math notranslate nohighlight">\(\o_t\)</span> on input <span class="math notranslate nohighlight">\(\x_t\)</span> should be
similar or close to the target response <span class="math notranslate nohighlight">\(\y_t\)</span>, for each time point
<span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>To learn dependencies between elements of the input sequence, an RNN maintains a
sequence of <span class="math notranslate nohighlight">\(m\)</span>-dimensional hidden state vectors <span class="math notranslate nohighlight">\(\h_t\in\R^m\)</span>,
where <span class="math notranslate nohighlight">\(\h_t\)</span> captures the essential features of the input sequences up to
time <span class="math notranslate nohighlight">\(t\)</span>.
The hidden vector <span class="math notranslate nohighlight">\(\h_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> depends on the input vector
<span class="math notranslate nohighlight">\(\x_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and the previous hidden state vector
<span class="math notranslate nohighlight">\(\h_{t-1}\)</span> from time <span class="math notranslate nohighlight">\(t-1\)</span>, and it is computed as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\h_t=f^h(\W_i^T\x_t+\W_h^T\h_{t-1}+\b_h)\)</span></p>
</div>
<p>Here, <span class="math notranslate nohighlight">\(f^h\)</span> is the hidden state activation function, typically tanh or ReLU.
Also, we need an initial hidden state vector <span class="math notranslate nohighlight">\(\h_0\)</span> that serves as the prior state to compute <span class="math notranslate nohighlight">\(\h_1\)</span>.
This is uaually set to the zero vector, or seeded from a prior RNN prediction step.
The matrix <span class="math notranslate nohighlight">\(\W_i\in\R^{d\times m}\)</span> specifies the weights between the input vectors and the hidden state vectors.
The matrix <span class="math notranslate nohighlight">\(\W_h\in\R^{m\times m}\)</span> specifies the weight matrix between the
hidden state vectors at time <span class="math notranslate nohighlight">\(t-1\)</span> and <span class="math notranslate nohighlight">\(t\)</span>, with <span class="math notranslate nohighlight">\(\b_h\in\R^m\)</span>
specifying the bias terms associated with the hidden states.
Note that we need only one bias vector <span class="math notranslate nohighlight">\(\b_h\)</span> associated with the hidden
state neurons; we do not need a separate bias vector between the input and
hidden neurons.</p>
<p>Given the hidden state vector at time <span class="math notranslate nohighlight">\(t\)</span>, the output vector <span class="math notranslate nohighlight">\(\o_t\)</span>
at time <span class="math notranslate nohighlight">\(t\)</span> is computed as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\o_y=f^o(\W_o^T\h_t+\b_o)\)</span></p>
</div>
<p>Here, <span class="math notranslate nohighlight">\(\W_o\in\R^{m\times \p}\)</span> specifies the weights between the hidden
state and output vectors, with bias vector <span class="math notranslate nohighlight">\(\b_o\)</span>.
The output activation function <span class="math notranslate nohighlight">\(f^o\)</span> typically uses linear or identity
activation, or a softmax activation for one-hot encoded categorical output
values.</p>
<p>It is important to note that all the weight matrices and bias vectors are <em>independent</em> of the time <span class="math notranslate nohighlight">\(t\)</span>.
For example, for the hidden layer, the same weight matrix <span class="math notranslate nohighlight">\(\W_h\)</span> and bias
vector <span class="math notranslate nohighlight">\(\b_h\)</span> is used and updated while training the model, over all time
steps <span class="math notranslate nohighlight">\(t\)</span>.
This is an exmaple of <em>parameter sharing</em> or <em>weight tying</em> between different layers or components of a neural network.
Likewise, the input weight matrix <span class="math notranslate nohighlight">\(\W_i\)</span>, the output weight matrix
<span class="math notranslate nohighlight">\(\W_o\)</span> and the bias vector <span class="math notranslate nohighlight">\(\b_o\)</span> are all shared across time.
This greatly reduces the number of parameters that need to be learned by the
RNN, but it also relies on the assumption that all relevant sequential features
can be captured by the shared parameters.</p>
<p>The training data for the RNN is given as <span class="math notranslate nohighlight">\(\D=\{\cX_i,\cY_y\}_{i=1}^n\)</span>,
comprising <span class="math notranslate nohighlight">\(n\)</span> input sequences <span class="math notranslate nohighlight">\(\cX_i\)</span> and the corresponding target
response sequences <span class="math notranslate nohighlight">\(\cY_i\)</span>, with sequence length <span class="math notranslate nohighlight">\(\tau_i\)</span>.
Given each pair <span class="math notranslate nohighlight">\((\cX,\cY)\in\D\)</span>, with
<span class="math notranslate nohighlight">\(\cX=\lag\x_1,\x_2,\cds,\x_\tau\rag\)</span> and
<span class="math notranslate nohighlight">\(\cY=\lag\y_1,\y_2,\cds,\y_\tau\rag\)</span>, the RNN has to update the model
parameters <span class="math notranslate nohighlight">\(\W_i,\W_h,\b_h,\W_o,\b_o\)</span> for the input, hidden and output
layers, to learn the corresponding output sequence
<span class="math notranslate nohighlight">\(\cl{O}=\lag\o_1,\o_2,\cds,\o_\tau\rag\)</span>.
For training the network, we compute the error or <em>loss</em> between the predicted and response vectors over all time steps.
The squared error loss is given as</p>
<div class="math notranslate nohighlight">
\[\cE_\cX=\sum_{t=1}^\tau\cE_{\x_t}=\frac{1}{2}\cd\sum_{t=1}^\tau\lv\y_t-\o_t\rv^2\]</div>
<p>If we use a softmax activation at the output layer, then we use the cross-entropy loss, given as</p>
<div class="math notranslate nohighlight">
\[\cE_\cX=\sum_{t=1}^\tau\cE_{\x_t}=-\sum_{t=1}^\tau\sum_{i=1}^py_{ti}\cd\ln(o_{ti})\]</div>
<p>where <span class="math notranslate nohighlight">\(\y_t=(y_{t1},y_{t2},\cds,y_{tp})^T\in\R^p\)</span> and <span class="math notranslate nohighlight">\(\o_t=(o_{t1},o_{t2},\cds,o_{tp})^T\in\R^p\)</span>.
On training input of length <span class="math notranslate nohighlight">\(\tau\)</span> we first unfold the RNN for
<span class="math notranslate nohighlight">\(\tau\)</span> steps, following which the parameters can be learned via the
standard feed-forward and backpropagation steps, keeping in mind the connections
between the layers.</p>
<div class="section" id="feed-forward-in-time">
<h3>26.1.1 Feed-forward in Time<a class="headerlink" href="#feed-forward-in-time" title="Permalink to this headline">¶</a></h3>
<p>The feed-forward process starts at time <span class="math notranslate nohighlight">\(t=0\)</span>, taking as input the initial
hidden state vector <span class="math notranslate nohighlight">\(\h_0\)</span>, which us usually set to <span class="math notranslate nohighlight">\(\0\)</span> or it can
be user-specified, say from a previous prediction step.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\o_t&amp;=f^o(\W_o^T\h_t+\b_o)\\&amp;=f^o(\W_o^Tf^h(\W_i^T\x_t+\W_h^T\h_{t-1}+\b_h)+\b_o)\\&amp;=\vds\\&amp;=f^o(\W_o^Tf^h(\W_i^T\x_t+\W_h^Tf^h(\cds f^h(\W_i^T\x_1+\W_h^T\h_0+\b_h)+\cds)+\b_h)+\b_o)\end{aligned}\end{align} \]</div>
<p>We can observe that the RNN implicitly makes a prediction for every prefix of
the input sequence, since <span class="math notranslate nohighlight">\(\o_t\)</span> depends on all the previous input vectors
<span class="math notranslate nohighlight">\(\x_1,\x_2,\cds,\x_t\)</span> but not on any future inputs
<span class="math notranslate nohighlight">\(\x_{t+1},\cds,\x_\tau\)</span>.</p>
</div>
<div class="section" id="backpropagation-in-time">
<h3>26.1.2 backpropagation in Time<a class="headerlink" href="#backpropagation-in-time" title="Permalink to this headline">¶</a></h3>
<p>For the backpropagation step it is easier to view the RNN in terms of the
distinct layers based on the dependencies, as opposed to unfolding in time.</p>
<p>Let <span class="math notranslate nohighlight">\(\cE_{\x_t}\)</span> denote the loss on input vector <span class="math notranslate nohighlight">\(\x_t\)</span> from the
input sequence <span class="math notranslate nohighlight">\(\cX=\lag\x_1,\x_2,\cds,\x_\tau\rag\)</span>.
The unfolded feed-forward RNN for <span class="math notranslate nohighlight">\(\cX\)</span> has <span class="math notranslate nohighlight">\(l=\tau+1\)</span> layers.
Define <span class="math notranslate nohighlight">\(\bs\delta_t^o\)</span> as the net gradient vector for the output vector
<span class="math notranslate nohighlight">\(\o_t\)</span>, i.e., the derivative of the error function <span class="math notranslate nohighlight">\(\cE_{\x_t}\)</span> with
respect to the net value at each neuron in <span class="math notranslate nohighlight">\(\o_t\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^o=\bigg(\frac{\pd\cE_{\x_t}}{\pd net_{t1}^o},
\frac{\pd\cE_{\x_t}}{\pd net_{t2}^o},\cds,
\frac{\pd\cE_{\x_t}}{\pd net_{tp}^o}\bigg)^T\]</div>
<p>where <span class="math notranslate nohighlight">\(\o_t=(o_{t1},o_{t2},\cds,o_{tp})^T\in\R^p\)</span> is the <span class="math notranslate nohighlight">\(p\)</span>-dimensional output vector at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(net_{ti}^o\)</span> is the net
value at output neuron <span class="math notranslate nohighlight">\(o_{ti}\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.
Likewise, let <span class="math notranslate nohighlight">\(\bs\delta_t^h\)</span> denote the net gradient vector for the
hidden state neurons <span class="math notranslate nohighlight">\(\h_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span></p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^h=\bigg(\frac{\pd\cE_{\x_t}}{\pd net_{t1}^h},
\frac{\pd\cE_{\x_t}}{\pd net_{t2}^h},\cds,
\frac{\pd\cE_{\x_t}}{\pd net_{tm}^h}\bigg)^T\]</div>
<p>where <span class="math notranslate nohighlight">\(\h_t=(h_{t1},h_{t2},\cds,h_{tm})^T\in\R^m\)</span> is the <span class="math notranslate nohighlight">\(m\)</span>-dimensional hidden state vector at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(net_{ti}^h\)</span> is the
net value at hidden neuron <span class="math notranslate nohighlight">\(h_{ti}\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.
Let <span class="math notranslate nohighlight">\(f^h\)</span> and <span class="math notranslate nohighlight">\(f^o\)</span> denote the activation functions for the hidden
state and output neurons, and let <span class="math notranslate nohighlight">\(\pd\f_t^h\)</span> and <span class="math notranslate nohighlight">\(\pd\f_t^o\)</span> denote
the vector of the derivatives of the activation function with respect to the net
signal for the hidden and output neurons at time <span class="math notranslate nohighlight">\(t\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\pd\f_t^h\bigg(\frac{\pd f^h(net_{t1}^h)}{\pd net_{t1}^h},
\frac{\pd f^h(net_{t2}^h)}{\pd net_{t2}^h},\cds,
\frac{\pd f^h(net_{tm}^h)}{\pd net_{tm}^h}\bigg)^T\\\pd\f_t^o\bigg(\frac{\pd f^o(net_{t1}^o)}{\pd net_{t1}^o},
\frac{\pd f^o(net_{t2}^o)}{\pd net_{t2}^o},\cds,
\frac{\pd f^o(net_{tp}^o)}{\pd net_{tp}^o}\bigg)^T\end{aligned}\end{align} \]</div>
<p>Finally, let <span class="math notranslate nohighlight">\(\pd\bs\cE_{\x_t}\)</span> denote the vector of partial derivatives
of the error function with respect to <span class="math notranslate nohighlight">\(\o_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[\pd\bs\cE_{\x_t}=\bigg(\frac{\pd\cE_{\x_t}}{\pd o_{t1}},\frac{\pd\cE_{\x_t}}
{\pd o_{t2}},\cds,\frac{\pd\cE_{\x_t}}{\pd o_{tp}}\bigg)^T\]</div>
<p><strong>Computing Net Gradients</strong></p>
<p>The net gradient vector at the output <span class="math notranslate nohighlight">\(\o_t\)</span> can be computed as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\delta_t^o=\pd\f_t^o\od\pd\bs\cE_{\x_t}\)</span></p>
</div>
<p>For example, if <span class="math notranslate nohighlight">\(\cE_{\x_t}\)</span> is the squared error function, and the output
layer uses the identity funciton, then we have</p>
<div class="math notranslate nohighlight">
\[\bs\delta_t^o=\1\od(\o_t-\y_t)\]</div>
<p>On the other hand, the net gradients at each of the hidden layers need to
account for the incoming net gradients from <span class="math notranslate nohighlight">\(\o_t\)</span> and from
<span class="math notranslate nohighlight">\(\h_{t+1}\)</span>.
The net gradient vector for <span class="math notranslate nohighlight">\(\h_t(\rm{for\ }t=1,2,\cds,\tau-1)\)</span> is given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\delta_t^h=\pd\f_t^h\od((\W_o\cd\bs\delta_t^o)+(\W_h\cd\bs\delta_{t+1}^h))\)</span></p>
</div>
<p>Note that for <span class="math notranslate nohighlight">\(\h_\tau\)</span>, it depends only on <span class="math notranslate nohighlight">\(\o_\tau\)</span>, therefore</p>
<div class="math notranslate nohighlight">
\[\bs\delta_\tau^h=\pd\f_\tau^h\od(\W_o\cd\bs\delta_\tau^o)\]</div>
<p>For the tanh activation, which is commonly used in RNNs, the derivative of the
activation function with respect to the net values at <span class="math notranslate nohighlight">\(\h_t\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\pd\f_t^h=(\1-\h_t\od\h_t)\]</div>
<p>Finally, note that the net gradients do not have to be computed for <span class="math notranslate nohighlight">\(\h_0\)</span>
or for any of the input neurons <span class="math notranslate nohighlight">\(\x_t\)</span>, since these are leaf nodes in the
backpropagation graph, and thus do not backpropagate the gradients beyond those
neurons.</p>
<p><strong>Stochastic Gradient Descent</strong></p>
<p>The net gradients for the output <span class="math notranslate nohighlight">\(\bs\delta_t^o\)</span> and hidden
<span class="math notranslate nohighlight">\(\bs\delta_t^h\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> can be used to compute the gradients for
the weight matrices and bias vectors at each time point.
However, since an RNN uses parameter sharing across time, the gradients are
obtained by summing up all of the contributions from each time step <span class="math notranslate nohighlight">\(t\)</span>.
Define <span class="math notranslate nohighlight">\(\nabla_{\w_o}^t\)</span> and <span class="math notranslate nohighlight">\(\nabla_{\b_o}^t\)</span> as the gradients of
the weights and biases between the hidden neurons <span class="math notranslate nohighlight">\(\h_t\)</span> and output
neurons <span class="math notranslate nohighlight">\(\o_t\)</span> for time <span class="math notranslate nohighlight">\(t\)</span>.
Using the backpropagation equations, for deep multilayer perceptrons, these
gradients are computed as follows:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\b_o}=\sum_{t=1}^\tau\nabla_{\b_o}^t=\sum_{t=1}^\tau\bs\delta_t^o
\quad\quad\nabla_{\w_o}=\sum_{t=1}^\tau\nabla_{\w_o}^t=\sum_{t=1}^\tau\h_t
\cd(\bs\delta_t^o)^T\]</div>
<p>Likewise, the gradients of the other shared parameters between hidden layers
<span class="math notranslate nohighlight">\(\h_{t-1}\)</span> and <span class="math notranslate nohighlight">\(\h_t\)</span>, and between the input layer <span class="math notranslate nohighlight">\(\x_t\)</span> and
hidden layer <span class="math notranslate nohighlight">\(\h_t\)</span>, are obtained as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla_{\b_h}&amp;=\sum_{t=1}^\tau\nabla_{\b_h}^t=\sum_{t=1}^\tau\bs\delta_t^h
\quad\quad\nabla_{\w_h}=\sum_{t=1}^\tau\nabla_{\W_h}^t=\sum_{t=1}^\tau
\h_{t-1}\cd(\bs\delta_t^h)^T\\\nabla_{\w_i}&amp;=\sum_{t=1}^\tau\nabla_{\w_i}^t=\sum_{t=1}^\tau\x_t\cd(\bs
\delta_t^h)^T\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\W_i=\W_i-\eta\cd\nabla_{\w_i}\quad\W_h=\W_h-\eta\cd\nabla_{\w_h}\quad\b_h=\b_h-\eta\cd\nabla_{\b_h}\)</span></p>
<p><span class="math notranslate nohighlight">\(\W_o=\W_o-\eta\cd\nabla_{\w_o}\quad\b_o=\b_o-\eta\cd\nabla_{\b_o}\)</span></p>
</div>
</div>
<div class="section" id="training-rnns">
<h3>26.1.3 Training RNNs<a class="headerlink" href="#training-rnns" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/Algo26.1.png" src="../_images/Algo26.1.png" />
</div>
<div class="section" id="bidirectional-rnns">
<h3>26.1.4 Bidirectional RNNs<a class="headerlink" href="#bidirectional-rnns" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap27.html" class="btn btn-neutral float-right" title="Chapter 27 Regression Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap25.html" class="btn btn-neutral float-left" title="Chapter 25 Neural Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>