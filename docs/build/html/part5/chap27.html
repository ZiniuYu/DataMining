

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 27 Regression Evaluation &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Chapter 26 Deep Learning" href="chap26.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index5.html">Part 5 Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap23.html">Chapter 23 Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap24.html">Chapter 24 Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap25.html">Chapter 25 Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap26.html">Chapter 26 Deep Learning</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 27 Regression Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#univariate-regression">27.1 Univariate Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#estimating-variance-sg-2">27.1.1 Estimating Variance (<span class="math notranslate nohighlight">\(\sg^2\)</span>)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#goodness-of-fit">27.1.2 Goodness of Fit</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index5.html">Part 5 Regression</a> &raquo;</li>
        
      <li>Chapter 27 Regression Evaluation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part5/chap27.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}
\newcommand{\Z}{\boldsymbol{\mathrm{Z}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cE}{\mathcal{E}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-27-regression-evaluation">
<h1>Chapter 27 Regression Evaluation<a class="headerlink" href="#chapter-27-regression-evaluation" title="Permalink to this headline">Â¶</a></h1>
<p>Given a set of predictor attributes or independent variables
<span class="math notranslate nohighlight">\(X_1,X_2,\cds,X_d\)</span>, and given the response attribute <span class="math notranslate nohighlight">\(Y\)</span>, the goal
of regression is to learn a <span class="math notranslate nohighlight">\(f\)</span>, such that</p>
<div class="math notranslate nohighlight">
\[Y=f(X_1,X_2,\cds,X_d)+\ve=f(\X)+\ve\]</div>
<p>where <span class="math notranslate nohighlight">\(\X=(X_1,X_2,\cds,X_d)^T\)</span> is the <span class="math notranslate nohighlight">\(d\)</span>-dimensional multivariate
random variable comprised of the predictor variables.
Here, the random variable <span class="math notranslate nohighlight">\(\ve\)</span> denotes the inferent <em>error</em> in the
response that is not explained by the linear model.</p>
<p>When estimating the regression function <span class="math notranslate nohighlight">\(f\)</span>, we make assumptions about the form of <span class="math notranslate nohighlight">\(f\)</span>.
Once we have estimated the bias and coefficients, we need to formulate a
probabilistic model of regression to evaluate the learned model in terms of
goodness of fit, confidence intervals for the parameters, and to test for the
regression effects, namely whether <span class="math notranslate nohighlight">\(\X\)</span> really helps in predicting
<span class="math notranslate nohighlight">\(Y\)</span>.
In particular, we assume that even if the value of <span class="math notranslate nohighlight">\(\X\)</span> has been fixed,
there can still be uncertainty in the response <span class="math notranslate nohighlight">\(Y\)</span>.
Further, we will assume that the error <span class="math notranslate nohighlight">\(\ve\)</span> is independent of <span class="math notranslate nohighlight">\(\X\)</span>
and follows a normal (or Guassian) distribution with mean <span class="math notranslate nohighlight">\(\mu=0\)</span> and
variance <span class="math notranslate nohighlight">\(\sg^2\)</span>, that is, we assume that the errors are independent and
identically distributed with zero mean and fixed variance.</p>
<p>The probabilistic regression model comprises two components-the
<em>deterministic component</em> comprising the observed predictor attributes, and the
<em>random error component</em> comprising the error term, which is assumed to be
independent of the predictor attributes.</p>
<div class="section" id="univariate-regression">
<h2>27.1 Univariate Regression<a class="headerlink" href="#univariate-regression" title="Permalink to this headline">Â¶</a></h2>
<p>We assume that the true relationship can be modeled as a linear function</p>
<div class="math notranslate nohighlight">
\[Y=f(X)+\ve=\beta+\omega\cd X+\ve\]</div>
<p>where <span class="math notranslate nohighlight">\(\omega\)</span> is the slope of the best fitting line and <span class="math notranslate nohighlight">\(\beta\)</span> is
its intercept, and <span class="math notranslate nohighlight">\(\ve\)</span> is the random error variable that follows a
normal distribution with mean <span class="math notranslate nohighlight">\(\mu=0\)</span> and variance <span class="math notranslate nohighlight">\(\sg^2\)</span>.</p>
<p><strong>Mean and Variance of Response Variable</strong></p>
<p>Consider a fixed value <span class="math notranslate nohighlight">\(x\)</span> for the independent variable <span class="math notranslate nohighlight">\(X\)</span>.
The expected value of the response variable <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="math notranslate nohighlight">
\[E[Y|X=x]=E[\beta+\omega\cd x+\ve]=\beta+\omega\cd x+E[\ve]=\beta+\omega\cd x\]</div>
<p>The last step follows from our assumption that <span class="math notranslate nohighlight">\(E[\ve]=\mu=0\)</span>.
Also, since <span class="math notranslate nohighlight">\(x\)</span> is assumed to be fixed, and <span class="math notranslate nohighlight">\(\beta\)</span> and
<span class="math notranslate nohighlight">\(\omega\)</span> are constants, the expected value
<span class="math notranslate nohighlight">\(E[\beta+\omega\cd x]=\beta+\omega\cd x\)</span>.
Next, consider the variance of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X=x\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\rm{var}(Y|X=x)=\rm{var}(\beta+\omega\cd x+\ve)=\rm{var}(\beta+\omega\cd x)+\rm{var}(\ve)=0+\sg^2=\sg^2\]</div>
<p>Here <span class="math notranslate nohighlight">\(\rm{var}(\beta+\omega\cd x)=0\)</span>, since <span class="math notranslate nohighlight">\(\beta,\omega,x\)</span> are all constants.
Thus, given <span class="math notranslate nohighlight">\(X=x\)</span>, the response variable <span class="math notranslate nohighlight">\(Y\)</span> follows a normal
distribution with mean <span class="math notranslate nohighlight">\(E[Y|X=x]=\beta+\omega\cd x\)</span>, and variance
<span class="math notranslate nohighlight">\(\rm{var}(Y|X=x)=\sg^2\)</span></p>
<p><strong>Estimated Parameters</strong></p>
<p>The true parameters <span class="math notranslate nohighlight">\(\beta,\omega,\sg^2\)</span> are all unknown, and have to be
estimated from the training data <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(x_i\)</span> and corresponding response values <span class="math notranslate nohighlight">\(y_i\)</span>, for
<span class="math notranslate nohighlight">\(i=1,2,\cds,n\)</span>.
Let <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w\)</span> denote the estimated bias and weight terms; we can
then make predictions for any given value <span class="math notranslate nohighlight">\(x_i\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\hat{y_i}=b+w\cd x_i\]</div>
<p>The estimated bias <span class="math notranslate nohighlight">\(b\)</span> and weight <span class="math notranslate nohighlight">\(w\)</span> are obtained by minimizing the sum of squared errors, given as</p>
<div class="math notranslate nohighlight">
\[SSE=\sum_{i=1}^n(y_i-\hat{y_i})^2=\sum_{i=1}^n(y_i-b-w\cd x_i)^2\]</div>
<p>with the least squares estimates given as</p>
<div class="math notranslate nohighlight">
\[w=\frac{\sg_{XY}}{\sg_X^2}\quad\quad b=\mu_Y-w\cd\mu_X\]</div>
<div class="section" id="estimating-variance-sg-2">
<h3>27.1.1 Estimating Variance (<span class="math notranslate nohighlight">\(\sg^2\)</span>)<a class="headerlink" href="#estimating-variance-sg-2" title="Permalink to this headline">Â¶</a></h3>
<p>According to our model, the variance in prediction is entirely due to the random error term <span class="math notranslate nohighlight">\(\ve\)</span>.
We can estimate this variance by considering the predicted value
<span class="math notranslate nohighlight">\(\hat{y_i}\)</span> and its deviation from the true response <span class="math notranslate nohighlight">\(y_i\)</span>, that is,
by looking at the residual error</p>
<div class="math notranslate nohighlight">
\[\epsilon_i=y_i-\hat{y_i}\]</div>
<p>One of the properties of the estimated values <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w\)</span> is that the sum of residual errors is zero, since</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\sum_{i=1}^n\epsilon_i&amp;=\sum_{i=1}^n(y_i-b-w\cd x_i)\\&amp;=\sum_{i=1}^n(y_i-\mu_Y+w\cd\mu_X-w\cd x_i)\\&amp;=\bigg(\sum_{i=1}^ny_i\bigg)-n\cd\mu_Y+w\cd\bigg(n\mu_X-\sum_{i=1}^n x_i\bigg)\\&amp;=n\cd\mu_Y-n\cd\mu_Y+w\cd(n\cd\mu_X-n\cd\mu_X)=0\end{aligned}\end{align} \]</div>
<p>Thus, the expected value of <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is zero, since <span class="math notranslate nohighlight">\(E[\epsilon_i]=\frac{1}{n}\sum_{i=1}^n\epsilon_i=0\)</span>.</p>
<p>The estimated variance <span class="math notranslate nohighlight">\(\hat\sg^2\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\hat\sg^2=\rm{var}(\epsilon_i)=\frac{1}{n-2}\cd\sum_{i=1}^n(\epsilon_i-
E[\epsilon_i])^2=\frac{1}{n-2}\cd\sum_{i=1}^n\epsilon_i^2=\frac{1}{n-2}\cd
\sum_{i=1}^n(y_i-\hat{y_i})^2\]</div>
<p>Thus, the estimated variance is</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat\sg^2=\frac{SSE}{n-2}\)</span></p>
</div>
<p>We divide by <span class="math notranslate nohighlight">\(n-2\)</span> to get an unbiased estimate, since <span class="math notranslate nohighlight">\(n-2\)</span> is the
number of degrees of freedom for estimating SSE.</p>
<p>The squared root of the variance is called the <em>standard error of regression</em></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat\sg=\sqrt{\frac{SSE}{n-2}}\)</span></p>
</div>
</div>
<div class="section" id="goodness-of-fit">
<h3>27.1.2 Goodness of Fit<a class="headerlink" href="#goodness-of-fit" title="Permalink to this headline">Â¶</a></h3>
<p>The <em>total scatter</em>, also called <em>total sum of squares</em>, for the dependent variable <span class="math notranslate nohighlight">\(Y\)</span>, is defined as</p>
<div class="math notranslate nohighlight">
\[TSS=\sum_{i=1}^n(y_i-\mu_Y)^2\]</div>
<p>The total scatter can be decomposed into two components by adding and subtracting <span class="math notranslate nohighlight">\(\hat{y_i}\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}TSS&amp;=\sum_{i=1}^n(y_i-\mu_Y)^2=\sum_{i=1}^n(y_i-\hat{y_i}+\hat{y_i}-\mu_Y)^2\\&amp;=\sum_{i=1}^n(y_i-\hat{y_i})^2+\sum_{i=1}^n(\hat{y_i}-\mu_Y)^2+2\sum_{i=1}^n(y_i-\hat{y_i})\cd(\hat{y_i}-\mu_Y)\\&amp;=\sum_{i=1}^n(y_i-\hat{y_i})^2+\sum_{i=1}^n(\hat{y_1}-\mu_Y)^2=SSE+RSS\end{aligned}\end{align} \]</div>
<p>where we use the fact that <span class="math notranslate nohighlight">\(\sum_{i=1}^n(y_i-\hat{y_i})\cd(\hat{y_i}-\mu_Y)=0\)</span>, and</p>
<div class="math notranslate nohighlight">
\[RSS=\sum_{i=1}^n(\hat{y_i}-\mu_Y)^2\]</div>
<p>is a new term called <em>regression sum of squares</em> that measures the squared
deviation of the predictions from the true mean.
TSS can thus be decomposed into two parts: SSE, which is the amount of variation
not explained by the model, and RSS, which is the amount of variance explained
by the model.
Therefore, the fraction of the variation left unexplained by the model is given by the ration <span class="math notranslate nohighlight">\(\frac{SSE}{TSS}\)</span>.
Conversely, the fraction of the variation that is explained by the model called
the <em>coefficient of determination</em> or simply the <span class="math notranslate nohighlight">\(R^2\)</span> <em>statistic</em>, is
given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp R^2=\frac{TSS-SSE}{TSS}=1-\frac{SSE}{TSS}=\frac{RSS}{TSS}\)</span></p>
</div>
<p>The higher the <span class="math notranslate nohighlight">\(R^2\)</span> statistic the better the estimated model, with <span class="math notranslate nohighlight">\(R^2\in[0,1]\)</span>.</p>
<p><strong>Geometry of Goodness of Fit</strong></p>
<p>Recall that <span class="math notranslate nohighlight">\(Y\)</span> can be decomposed into two orthogonal parts</p>
<div class="math notranslate nohighlight">
\[Y=\hat{Y}+\bs\epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is the projection of <span class="math notranslate nohighlight">\(Y\)</span> onto the subspace spanned by <span class="math notranslate nohighlight">\(\{\1,X\}\)</span>.
Using the fact that this subspace is the same as that spanned by the orthogonal
vectors <span class="math notranslate nohighlight">\(\{\1,\bar{X}\}\)</span>, with <span class="math notranslate nohighlight">\(\bar{X}=X-\mu_X\cd\1\)</span>, we can
further decompose <span class="math notranslate nohighlight">\(\hat{Y}\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[\hat{Y}=\rm{proj}_\1(Y)\cd\1+\rm{proj}_{\bar{X}}(Y)\cd\bar{X}=\mu_Y\cd\1+
\frac{Y^T\bar{X}}{\bar{X}^T\bar{X}}\cd\bar{X}=\mu_Y\cd\1+w\cd\bar{X}\]</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap26.html" class="btn btn-neutral float-left" title="Chapter 26 Deep Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>