

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 27 Regression Evaluation &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Chapter 26 Deep Learning" href="chap26.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index5.html">Part 5 Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap23.html">Chapter 23 Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap24.html">Chapter 24 Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap25.html">Chapter 25 Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap26.html">Chapter 26 Deep Learning</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 27 Regression Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#univariate-regression">27.1 Univariate Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#estimating-variance-sg-2">27.1.1 Estimating Variance (<span class="math notranslate nohighlight">\(\sg^2\)</span>)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#goodness-of-fit">27.1.2 Goodness of Fit</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference-about-regression-coefficient-and-bias-term">27.1.3 Inference about Regression Coefficient and Bias Term</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index5.html">Part 5 Regression</a> &raquo;</li>
        
      <li>Chapter 27 Regression Evaluation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part5/chap27.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}
\newcommand{\Z}{\boldsymbol{\mathrm{Z}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cE}{\mathcal{E}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-27-regression-evaluation">
<h1>Chapter 27 Regression Evaluation<a class="headerlink" href="#chapter-27-regression-evaluation" title="Permalink to this headline">¶</a></h1>
<p>Given a set of predictor attributes or independent variables
<span class="math notranslate nohighlight">\(X_1,X_2,\cds,X_d\)</span>, and given the response attribute <span class="math notranslate nohighlight">\(Y\)</span>, the goal
of regression is to learn a <span class="math notranslate nohighlight">\(f\)</span>, such that</p>
<div class="math notranslate nohighlight">
\[Y=f(X_1,X_2,\cds,X_d)+\ve=f(\X)+\ve\]</div>
<p>where <span class="math notranslate nohighlight">\(\X=(X_1,X_2,\cds,X_d)^T\)</span> is the <span class="math notranslate nohighlight">\(d\)</span>-dimensional multivariate
random variable comprised of the predictor variables.
Here, the random variable <span class="math notranslate nohighlight">\(\ve\)</span> denotes the inferent <em>error</em> in the
response that is not explained by the linear model.</p>
<p>When estimating the regression function <span class="math notranslate nohighlight">\(f\)</span>, we make assumptions about the form of <span class="math notranslate nohighlight">\(f\)</span>.
Once we have estimated the bias and coefficients, we need to formulate a
probabilistic model of regression to evaluate the learned model in terms of
goodness of fit, confidence intervals for the parameters, and to test for the
regression effects, namely whether <span class="math notranslate nohighlight">\(\X\)</span> really helps in predicting
<span class="math notranslate nohighlight">\(Y\)</span>.
In particular, we assume that even if the value of <span class="math notranslate nohighlight">\(\X\)</span> has been fixed,
there can still be uncertainty in the response <span class="math notranslate nohighlight">\(Y\)</span>.
Further, we will assume that the error <span class="math notranslate nohighlight">\(\ve\)</span> is independent of <span class="math notranslate nohighlight">\(\X\)</span>
and follows a normal (or Guassian) distribution with mean <span class="math notranslate nohighlight">\(\mu=0\)</span> and
variance <span class="math notranslate nohighlight">\(\sg^2\)</span>, that is, we assume that the errors are independent and
identically distributed with zero mean and fixed variance.</p>
<p>The probabilistic regression model comprises two components-the
<em>deterministic component</em> comprising the observed predictor attributes, and the
<em>random error component</em> comprising the error term, which is assumed to be
independent of the predictor attributes.</p>
<div class="section" id="univariate-regression">
<h2>27.1 Univariate Regression<a class="headerlink" href="#univariate-regression" title="Permalink to this headline">¶</a></h2>
<p>We assume that the true relationship can be modeled as a linear function</p>
<div class="math notranslate nohighlight">
\[Y=f(X)+\ve=\beta+\omega\cd X+\ve\]</div>
<p>where <span class="math notranslate nohighlight">\(\omega\)</span> is the slope of the best fitting line and <span class="math notranslate nohighlight">\(\beta\)</span> is
its intercept, and <span class="math notranslate nohighlight">\(\ve\)</span> is the random error variable that follows a
normal distribution with mean <span class="math notranslate nohighlight">\(\mu=0\)</span> and variance <span class="math notranslate nohighlight">\(\sg^2\)</span>.</p>
<p><strong>Mean and Variance of Response Variable</strong></p>
<p>Consider a fixed value <span class="math notranslate nohighlight">\(x\)</span> for the independent variable <span class="math notranslate nohighlight">\(X\)</span>.
The expected value of the response variable <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="math notranslate nohighlight">
\[E[Y|X=x]=E[\beta+\omega\cd x+\ve]=\beta+\omega\cd x+E[\ve]=\beta+\omega\cd x\]</div>
<p>The last step follows from our assumption that <span class="math notranslate nohighlight">\(E[\ve]=\mu=0\)</span>.
Also, since <span class="math notranslate nohighlight">\(x\)</span> is assumed to be fixed, and <span class="math notranslate nohighlight">\(\beta\)</span> and
<span class="math notranslate nohighlight">\(\omega\)</span> are constants, the expected value
<span class="math notranslate nohighlight">\(E[\beta+\omega\cd x]=\beta+\omega\cd x\)</span>.
Next, consider the variance of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X=x\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\rm{var}(Y|X=x)=\rm{var}(\beta+\omega\cd x+\ve)=\rm{var}(\beta+\omega\cd x)+\rm{var}(\ve)=0+\sg^2=\sg^2\]</div>
<p>Here <span class="math notranslate nohighlight">\(\rm{var}(\beta+\omega\cd x)=0\)</span>, since <span class="math notranslate nohighlight">\(\beta,\omega,x\)</span> are all constants.
Thus, given <span class="math notranslate nohighlight">\(X=x\)</span>, the response variable <span class="math notranslate nohighlight">\(Y\)</span> follows a normal
distribution with mean <span class="math notranslate nohighlight">\(E[Y|X=x]=\beta+\omega\cd x\)</span>, and variance
<span class="math notranslate nohighlight">\(\rm{var}(Y|X=x)=\sg^2\)</span></p>
<p><strong>Estimated Parameters</strong></p>
<p>The true parameters <span class="math notranslate nohighlight">\(\beta,\omega,\sg^2\)</span> are all unknown, and have to be
estimated from the training data <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(x_i\)</span> and corresponding response values <span class="math notranslate nohighlight">\(y_i\)</span>, for
<span class="math notranslate nohighlight">\(i=1,2,\cds,n\)</span>.
Let <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w\)</span> denote the estimated bias and weight terms; we can
then make predictions for any given value <span class="math notranslate nohighlight">\(x_i\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\hat{y_i}=b+w\cd x_i\]</div>
<p>The estimated bias <span class="math notranslate nohighlight">\(b\)</span> and weight <span class="math notranslate nohighlight">\(w\)</span> are obtained by minimizing the sum of squared errors, given as</p>
<div class="math notranslate nohighlight">
\[SSE=\sum_{i=1}^n(y_i-\hat{y_i})^2=\sum_{i=1}^n(y_i-b-w\cd x_i)^2\]</div>
<p>with the least squares estimates given as</p>
<div class="math notranslate nohighlight">
\[w=\frac{\sg_{XY}}{\sg_X^2}\quad\quad b=\mu_Y-w\cd\mu_X\]</div>
<div class="section" id="estimating-variance-sg-2">
<h3>27.1.1 Estimating Variance (<span class="math notranslate nohighlight">\(\sg^2\)</span>)<a class="headerlink" href="#estimating-variance-sg-2" title="Permalink to this headline">¶</a></h3>
<p>According to our model, the variance in prediction is entirely due to the random error term <span class="math notranslate nohighlight">\(\ve\)</span>.
We can estimate this variance by considering the predicted value
<span class="math notranslate nohighlight">\(\hat{y_i}\)</span> and its deviation from the true response <span class="math notranslate nohighlight">\(y_i\)</span>, that is,
by looking at the residual error</p>
<div class="math notranslate nohighlight">
\[\epsilon_i=y_i-\hat{y_i}\]</div>
<p>One of the properties of the estimated values <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w\)</span> is that the sum of residual errors is zero, since</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\sum_{i=1}^n\epsilon_i&amp;=\sum_{i=1}^n(y_i-b-w\cd x_i)\\&amp;=\sum_{i=1}^n(y_i-\mu_Y+w\cd\mu_X-w\cd x_i)\\&amp;=\bigg(\sum_{i=1}^ny_i\bigg)-n\cd\mu_Y+w\cd\bigg(n\mu_X-\sum_{i=1}^n x_i\bigg)\\&amp;=n\cd\mu_Y-n\cd\mu_Y+w\cd(n\cd\mu_X-n\cd\mu_X)=0\end{aligned}\end{align} \]</div>
<p>Thus, the expected value of <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is zero, since <span class="math notranslate nohighlight">\(E[\epsilon_i]=\frac{1}{n}\sum_{i=1}^n\epsilon_i=0\)</span>.</p>
<p>The estimated variance <span class="math notranslate nohighlight">\(\hat\sg^2\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\hat\sg^2=\rm{var}(\epsilon_i)=\frac{1}{n-2}\cd\sum_{i=1}^n(\epsilon_i-
E[\epsilon_i])^2=\frac{1}{n-2}\cd\sum_{i=1}^n\epsilon_i^2=\frac{1}{n-2}\cd
\sum_{i=1}^n(y_i-\hat{y_i})^2\]</div>
<p>Thus, the estimated variance is</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat\sg^2=\frac{SSE}{n-2}\)</span></p>
</div>
<p>We divide by <span class="math notranslate nohighlight">\(n-2\)</span> to get an unbiased estimate, since <span class="math notranslate nohighlight">\(n-2\)</span> is the
number of degrees of freedom for estimating SSE.</p>
<p>The squared root of the variance is called the <em>standard error of regression</em></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\hat\sg=\sqrt{\frac{SSE}{n-2}}\)</span></p>
</div>
</div>
<div class="section" id="goodness-of-fit">
<h3>27.1.2 Goodness of Fit<a class="headerlink" href="#goodness-of-fit" title="Permalink to this headline">¶</a></h3>
<p>The <em>total scatter</em>, also called <em>total sum of squares</em>, for the dependent variable <span class="math notranslate nohighlight">\(Y\)</span>, is defined as</p>
<div class="math notranslate nohighlight">
\[TSS=\sum_{i=1}^n(y_i-\mu_Y)^2\]</div>
<p>The total scatter can be decomposed into two components by adding and subtracting <span class="math notranslate nohighlight">\(\hat{y_i}\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}TSS&amp;=\sum_{i=1}^n(y_i-\mu_Y)^2=\sum_{i=1}^n(y_i-\hat{y_i}+\hat{y_i}-\mu_Y)^2\\&amp;=\sum_{i=1}^n(y_i-\hat{y_i})^2+\sum_{i=1}^n(\hat{y_i}-\mu_Y)^2+2\sum_{i=1}^n(y_i-\hat{y_i})\cd(\hat{y_i}-\mu_Y)\\&amp;=\sum_{i=1}^n(y_i-\hat{y_i})^2+\sum_{i=1}^n(\hat{y_1}-\mu_Y)^2=SSE+RSS\end{aligned}\end{align} \]</div>
<p>where we use the fact that <span class="math notranslate nohighlight">\(\sum_{i=1}^n(y_i-\hat{y_i})\cd(\hat{y_i}-\mu_Y)=0\)</span>, and</p>
<div class="math notranslate nohighlight">
\[RSS=\sum_{i=1}^n(\hat{y_i}-\mu_Y)^2\]</div>
<p>is a new term called <em>regression sum of squares</em> that measures the squared
deviation of the predictions from the true mean.
TSS can thus be decomposed into two parts: SSE, which is the amount of variation
not explained by the model, and RSS, which is the amount of variance explained
by the model.
Therefore, the fraction of the variation left unexplained by the model is given by the ration <span class="math notranslate nohighlight">\(\frac{SSE}{TSS}\)</span>.
Conversely, the fraction of the variation that is explained by the model called
the <em>coefficient of determination</em> or simply the <span class="math notranslate nohighlight">\(R^2\)</span> <em>statistic</em>, is
given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp R^2=\frac{TSS-SSE}{TSS}=1-\frac{SSE}{TSS}=\frac{RSS}{TSS}\)</span></p>
</div>
<p>The higher the <span class="math notranslate nohighlight">\(R^2\)</span> statistic the better the estimated model, with <span class="math notranslate nohighlight">\(R^2\in[0,1]\)</span>.</p>
<p><strong>Geometry of Goodness of Fit</strong></p>
<p>Recall that <span class="math notranslate nohighlight">\(Y\)</span> can be decomposed into two orthogonal parts</p>
<div class="math notranslate nohighlight">
\[Y=\hat{Y}+\bs\epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is the projection of <span class="math notranslate nohighlight">\(Y\)</span> onto the subspace spanned by <span class="math notranslate nohighlight">\(\{\1,X\}\)</span>.
Using the fact that this subspace is the same as that spanned by the orthogonal
vectors <span class="math notranslate nohighlight">\(\{\1,\bar{X}\}\)</span>, with <span class="math notranslate nohighlight">\(\bar{X}=X-\mu_X\cd\1\)</span>, we can
further decompose <span class="math notranslate nohighlight">\(\hat{Y}\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[\hat{Y}=\rm{proj}_\1(Y)\cd\1+\rm{proj}_{\bar{X}}(Y)\cd\bar{X}=\mu_Y\cd\1+
\frac{Y^T\bar{X}}{\bar{X}^T\bar{X}}\cd\bar{X}=\mu_Y\cd\1+w\cd\bar{X}\]</div>
<p>Likewise, the vector <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat{Y}\)</span> can be centered by
subtracting their projections along the vector <span class="math notranslate nohighlight">\(\1\)</span></p>
<div class="math notranslate nohighlight">
\[\bar{Y}=Y-\mu_Y\cd\1\quad\quad\hat{\bar{Y}}=\hat{Y}-\mu_Y\cd\1=w\cd\bar{X}\]</div>
<p>The centered vectors <span class="math notranslate nohighlight">\(\bar{Y},\hat{\bar{Y}},\bar{X}\)</span> all lie in the
<span class="math notranslate nohighlight">\(n-1\)</span> dimensional subspace orthogonal to the vector <span class="math notranslate nohighlight">\(\1\)</span>.</p>
<p>In this subspace, the centered vectors <span class="math notranslate nohighlight">\(\bar{Y}\)</span> and
<span class="math notranslate nohighlight">\(\hat{\bar{Y}}\)</span>, and the error vector <span class="math notranslate nohighlight">\(\bs\epsilon\)</span> form a right
triangle, since <span class="math notranslate nohighlight">\(\hat{\bar{Y}}\)</span> is the orthogonal projection of
<span class="math notranslate nohighlight">\(\bar{Y}\)</span> onto the vector <span class="math notranslate nohighlight">\(\bar{X}\)</span>.
Noting that <span class="math notranslate nohighlight">\(\bs\epsilon=Y-\hat{Y}=\bar{Y}-\hat{\bar{Y}}\)</span>, by the Pythagoras theorem, we have</p>
<div class="math notranslate nohighlight">
\[\lv\bar{Y}\rv^2=\lv\hat{\bar{Y}}\rv^2+\lv\bs\epsilon\rv^2=\lv\hat{\bar{Y}}\rv^2+\lv Y-\hat{Y}\rv^2\]</div>
<p>This equation is equivalent to the decomposition of the total scatter, TSS, into
sum of squared erros, SSE, and residual sum of squares, RSS.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}TSS&amp;=\sum_{i=1}^n(y_i-\mu_Y)^2=\lv T-\mu_Y\cd\1\rv^2=\lv\bar{Y}\rv^2\\RSS&amp;=\sum_{i=1}^n(\hat{y_i}-\mu_Y)^2=\lv\hat{Y}-\mu_Y\cd\1\rv^2=\lv\hat{\bar{Y}}\rv^2\\SSE&amp;=\lv\bs\epsilon\rv^2=\lv Y-\hat{Y}\rv^2\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\lv\bar{Y}\rv^2&amp;=\lv\hat{\bar{Y}}\rv^2+\lv Y-\hat{Y}\rv^2\\\lv Y-\mu_Y\cd\1\rv^2&amp;=\lv\hat{Y}-\mu_Y\cd\1\rv^2+\lv Y-\hat{Y}\rv^2\\TSS&amp;=RSS+SSE\end{aligned}\end{align} \]</div>
<p>Notice further that since <span class="math notranslate nohighlight">\(\bar{Y},\hat{\bar{Y}},\bs\epsilon\)</span> form a right
triangle, the cosine of the angle between <span class="math notranslate nohighlight">\(\bar{Y}\)</span> and
<span class="math notranslate nohighlight">\(\hat{\bar{Y}}\)</span> is given as the ratio of the base to the hypotenuse.
On the other hand, the cosine of the angle is also the correlation between
<span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat{Y}\)</span> denoted <span class="math notranslate nohighlight">\(\rho_{Y\hat{Y}}\)</span>.
Thus, we have:</p>
<div class="math notranslate nohighlight">
\[\rho_{Y\hat{Y}}=\cos\th=\frac{\lv\hat{\bar{Y}}\rv}{\lv\bar{Y}\rv}\]</div>
<p>We can observe that</p>
<div class="math notranslate nohighlight">
\[\lv\hat{\bar{Y}}\rv=\rho_{Y\hat{Y}}\cd\lv\bar{Y}\rv\]</div>
<p>Note that, whereas <span class="math notranslate nohighlight">\(|\rho_{Y\hat{Y}}|\leq 1\)</span>, due to the projection
operation, the angle between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is always less than
or equal to <span class="math notranslate nohighlight">\(90^\circ\)</span>, which means that <span class="math notranslate nohighlight">\(\rho_{Y\hat{Y}}\in[0,1]\)</span>
for univariate regression.
Thus, the predicted response vector <span class="math notranslate nohighlight">\(\hat{\bar{Y}}\)</span> is smaller than the
true response vector <span class="math notranslate nohighlight">\(\bar{Y}\)</span> by an amount equal to the correlation
between them.
Furthermore, the coefficient of determination is the same as the squared
correlation between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat{Y}\)</span></p>
<div class="math notranslate nohighlight">
\[R^2=\frac{RSS}{TSS}=\frac{\lv\hat{\bar{Y}}\rv^2}{\lv\bar{Y}\rv^2}=\rho^2_{Y\hat{Y}}\]</div>
</div>
<div class="section" id="inference-about-regression-coefficient-and-bias-term">
<h3>27.1.3 Inference about Regression Coefficient and Bias Term<a class="headerlink" href="#inference-about-regression-coefficient-and-bias-term" title="Permalink to this headline">¶</a></h3>
<p>The estimated values of the bias and regression coefficient, <span class="math notranslate nohighlight">\(b\)</span> and
<span class="math notranslate nohighlight">\(w\)</span>, are only point estimates for the true parameters <span class="math notranslate nohighlight">\(\beta\)</span> and
<span class="math notranslate nohighlight">\(\omega\)</span>.
To obtain confidence intervals for these parameters, we treat each <span class="math notranslate nohighlight">\(y_i\)</span>
as a random variable for the response given the corresponding fixed value
<span class="math notranslate nohighlight">\(x_i\)</span>.
These random variables are all independent and identically distributed as
<span class="math notranslate nohighlight">\(Y\)</span>, with expected value <span class="math notranslate nohighlight">\(\beta+\omega\cd x_i\)</span> and variance
<span class="math notranslate nohighlight">\(\sg^2\)</span>.
On the other hand, the <span class="math notranslate nohighlight">\(x_i\)</span> values are fixed <em>a priori</em> and therefore
<span class="math notranslate nohighlight">\(\mu_X\)</span> and <span class="math notranslate nohighlight">\(\sg_X^2\)</span> are also fixed values.</p>
<p>We can now treat <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w\)</span> as random variables, with</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}b&amp;=\mu_Y-w\cd\mu_X\\w&amp;=\frac{\sum_{i=1}^n(x_i-\mu_X)(y_i-\mu_Y)}{\sum_{i=1}^n(x_i-\mu_X)^2}=
\frac{1}{s_X}\sum_{i=1}^n(x_i-\mu_X)\cd y_i=\sum_{i=1}^nc_i\cd y_i\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(c_i\)</span> is a constant, given as</p>
<div class="math notranslate nohighlight">
\[c_i=\frac{x_i-\mu_X}{s_X}\]</div>
<p>and <span class="math notranslate nohighlight">\(s_X=\sum_{i=1}^n(x_i-\mu_X)^2\)</span> is the total scatter for <span class="math notranslate nohighlight">\(X\)</span>,
defined as the sum of squared deviations of <span class="math notranslate nohighlight">\(x_i\)</span> from its mean
<span class="math notranslate nohighlight">\(\mu_X\)</span>.
We also use the fact that</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n(x_i-\mu_X)\cd\mu_Y=\mu_Y\cd\sum_{i=1}^n(x_i-\mu_X)=0\]</div>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^nc_i=\frac{1}{s_X}\sum_{i=1}^n(x_i-\mu_X)=0\]</div>
<p><strong>Mean and Variance of Regression Coefficient</strong></p>
<p>The expected value of <span class="math notranslate nohighlight">\(w\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}E[w]&amp;=E\bigg[\sum_{i=1}^nc_iy_i\bigg]=\sum_{i=1}^nc_i\cd E[y_i]=\sum_{i=1}^nc_i(\beta+\omega\cd x_i)\\&amp;=\beta\sum_{i=1}^nc_i+\omega\cd\sum_{i=1}^nc_i\cd x_i=\frac{\omega}{s_X}\cd
\sum_{i=1}^n(x_i-\mu_X)\cd x_i=\frac{\omega}{s_X}\cd s_X=\omega\end{aligned}\end{align} \]</div>
<p>which follows from the observation that <span class="math notranslate nohighlight">\(\sum_{i=1}^nc_i=0\)</span>, and further</p>
<div class="math notranslate nohighlight">
\[s_X=\sum_{i=1}^n(x_i-\mu_X)^2=\bigg(\sum_{i=1}^nx_i^2\bigg)-n\cd\mu_X^2=\sum_{i=1}^n(x_i-\mu_X)\cd x_i\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(w\)</span> is an unbiased estimator for the true parameter <span class="math notranslate nohighlight">\(\omega\)</span>.
Using the fact that the variables <span class="math notranslate nohighlight">\(y_i\)</span> are independent and identically
distributed as <span class="math notranslate nohighlight">\(Y\)</span>, we can compute the variance of <span class="math notranslate nohighlight">\(w\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[\rm{var}(w)=\rm{var}\bigg(\sum_{i=1}^nc_i\cd y_i\bigg)=\sum_{i=1}^nc_i^2\cd
\rm{var}(y_i)=\sg^2\cd\sum_{i=1}^nc_i^2=\frac{\sg^2}{s_X}\]</div>
<p>since <span class="math notranslate nohighlight">\(c_i\)</span> is a constant, <span class="math notranslate nohighlight">\(\rm{var}(y_i)=\sg^2\)</span>, and further</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^nc_i^2=\frac{1}{s^2_X}\cd\sum_{i=1}^n(x_i-\mu_X)^2=\frac{s_X}{s^2_X}=\frac{1}{s_X}\]</div>
<p>The standard deviation of <span class="math notranslate nohighlight">\(w\)</span>, also called the standard error of <span class="math notranslate nohighlight">\(w\)</span>, is given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\rm{se}(w)=\sqrt{\rm{var}(w)}=\frac{\sg}{\sqrt{s_X}}\)</span></p>
</div>
<p><strong>Mean and Variance of Bias Term</strong></p>
<p>The expected value of <span class="math notranslate nohighlight">\(b\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}E[b]&amp;=E[\mu_Y-w\cd\mu_X]=E\bigg[\frac{1}{n}\sum_{i=1}^ny_i-w\cd\mu_x\bigg]\\&amp;=\bigg(\frac{1}{n}\cd\sum_{i=1}^nE[y_i]\bigg)-\mu_X\cd E[w]=\bigg(
\frac{1}{n}\sum_{i=1}^n(\beta+\omega\cd x_i)\bigg)-\omega\cd\mu_X\\&amp;=\beta+\omega\cd\mu_X-\omega\cd\mu_X=\beta\end{aligned}\end{align} \]</div>
<p>Thus, <span class="math notranslate nohighlight">\(b\)</span> is an unbiased estimator for the true parameter <span class="math notranslate nohighlight">\(beta\)</span>.</p>
<p>Using the observation that all <span class="math notranslate nohighlight">\(y_i\)</span> are independent, the variance of the bias term can be computed as follows</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\rm{var}(b)&amp;=\rm{var}(\mu_Y-w\cd\mu_X)\\&amp;=\rm{var}\bigg(\frac{1}{n}\sum_{i=1}^ny_i\bigg)+\rm{var}(\mu_X\cd w)\\&amp;=\frac{1}{n^2}\cd n\sg^2+\mu_X^2\cd\rm{var}(w)=\frac{1}{n}\cd\sg^2+\mu_X^2\cd\frac{\sg^2}{s_X}\\&amp;=\bigg(\frac{1}{n}+\frac{\mu_X^2}{s_X}\bigg)\cd\sg^2\end{aligned}\end{align} \]</div>
<p>The standard deviation of <span class="math notranslate nohighlight">\(b\)</span>, also called the standard error of <span class="math notranslate nohighlight">\(b\)</span>, is given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\rm{se}(b)=\sqrt{\rm{var}(b)}=\sg\cd\sqrt{\frac{1}{n}+\frac{\mu_X^2}{s_X}}\)</span></p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap26.html" class="btn btn-neutral float-left" title="Chapter 26 Deep Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>