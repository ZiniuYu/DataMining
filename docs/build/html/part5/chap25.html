

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 25 Neural Networks &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 26 Deep Learning" href="chap26.html" />
    <link rel="prev" title="Chapter 24 Logistic Regression" href="chap24.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part3/index3.html">Part 3 Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index5.html">Part 5 Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap23.html">Chapter 23 Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap24.html">Chapter 24 Logistic Regression</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 25 Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#artificial-neuron-activation-functions">25.1 Artificial Neuron: Activation Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#derivatives-for-activation-functions">25.1.1 Derivatives for Activation Functions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neural-networks-regression-and-classification">25.2 Neural Networks: Regression and Classification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#regression">25.2.1 Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#classification">25.2.2 Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#error-functions">25.2.3 Error Functions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multilayer-perceptron-one-hidden-layer">25.3 Multilayer Perceptron: One Hidden Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#feed-forward-phase">25.3.1 Feed-forward Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="#backpropagation-phase">25.3.2 Backpropagation Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlp-training">25.3.3 MLP Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#deep-multilayer-perceptrons">25.4 Deep Multilayer Perceptrons</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">25.4.1 Feed-forward Phase</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap26.html">Chapter 26 Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap27.html">Chapter 27 Regression Evaluation</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index5.html">Part 5 Regression</a> &raquo;</li>
        
      <li>Chapter 25 Neural Networks</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part5/chap25.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-25-neural-networks">
<h1>Chapter 25 Neural Networks<a class="headerlink" href="#chapter-25-neural-networks" title="Permalink to this headline">¶</a></h1>
<p><em>Artificial neural networks</em> or simply <em>neural networks</em> are inspired by biological neuronal networks.
Artifical neural networks are comprised of abstract neurons that try to mimic real neurons at a very high level.
They can be described via a weighte directed graph <span class="math notranslate nohighlight">\(G=(V,E)\)</span>, with each
node <span class="math notranslate nohighlight">\(v_i\in V\)</span> representing a neuron, and each directed edge
<span class="math notranslate nohighlight">\((v_i,v_j)\in E\)</span> representing a synaptic to dendritic connection from
<span class="math notranslate nohighlight">\(v_i\)</span> to <span class="math notranslate nohighlight">\(v_j\)</span>.
The weight of the edge <span class="math notranslate nohighlight">\(w_{ij}\)</span> denotes the synaptic strength.
Nerual networks are characterized by the type of activation function used to
generate an output, and the architecture of the network in terms of how the
nodes are interconnected.</p>
<div class="section" id="artificial-neuron-activation-functions">
<h2>25.1 Artificial Neuron: Activation Functions<a class="headerlink" href="#artificial-neuron-activation-functions" title="Permalink to this headline">¶</a></h2>
<p>A neuro <span class="math notranslate nohighlight">\(z_k\)</span> has incoming edges from neurons <span class="math notranslate nohighlight">\(x_1,\cds,x_d\)</span>.
Let <span class="math notranslate nohighlight">\(x_i\)</span> denotes neuron <span class="math notranslate nohighlight">\(i\)</span>, and also the value of that neuron.
The net input at <span class="math notranslate nohighlight">\(z_k\)</span>, denoted <span class="math notranslate nohighlight">\(net_k\)</span>, is given as the weighted sum</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(net_k=b_k+\sum_{i=1}^dw_{ik}\cd x_i=b_k+\w^T\x\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\w_k=(w_{1k},w_{2k},\cds,w_{dk})^T\in\R^d\)</span> and <span class="math notranslate nohighlight">\(\x=(x_1,x_2,\cds,x_d)^T\in\R^d\)</span> is an input point.
Notice that <span class="math notranslate nohighlight">\(x_0\)</span> is a special <em>bias neuron</em> whose value is always fixed
at 1, and the weight from <span class="math notranslate nohighlight">\(x_0\)</span> to <span class="math notranslate nohighlight">\(z_k\)</span> is <span class="math notranslate nohighlight">\(b_k\)</span>, which
specifies the bias term for the neuron.
Finally, the output value of <span class="math notranslate nohighlight">\(z_k\)</span> is given as some <em>activation function</em>,
<span class="math notranslate nohighlight">\(f(\cd)\)</span>, applied to the net input at <span class="math notranslate nohighlight">\(z_k\)</span></p>
<div class="math notranslate nohighlight">
\[z_k=f(net_k)\]</div>
<p>The value <span class="math notranslate nohighlight">\(z_k\)</span> is then passed along the outgoing edges from <span class="math notranslate nohighlight">\(z_k\)</span> to other neurons.</p>
<p><strong>Linear/Identity Function:</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[f(net_k)=net_k\]</div>
</div></blockquote>
<p><strong>Step Function:</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}f(net_k)=\left\{\begin{array}{lr}1\quad\rm{if\ }net_k\leq 0\\0\quad\rm{if\ }net_k&gt;0\end{array}\right.\end{split}\]</div>
</div></blockquote>
<p><strong>Rectified Linear Unit (ReLU):</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}f(net_k)=\left\{\begin{array}{lr}1\quad\rm{if\ }net_k\leq 0\\net_k\quad\rm{if\ }net_k&gt;0\end{array}\right.\end{split}\]</div>
<p>An alternative expression for the ReLU activation is given as <span class="math notranslate nohighlight">\(f(net_k)=\max\{0,net_k\}\)</span>.</p>
</div></blockquote>
<p><strong>Sigmoid:</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[f(net_k)=\frac{1}{1+\exp\{-net_k\}}\]</div>
</div></blockquote>
<p><strong>Hyperbolic Tangent (tanh):</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[f(net_k)=\frac{\exp\{net_k\}-\exp\{-net_k\}}{\exp\{net_k\}+\exp\{-net_k\}}=
\frac{\exp\{2\cd net_k\}-1}{\exp\{2\cd net_k\}+1}\]</div>
</div></blockquote>
<p><strong>Softmax:</strong></p>
<blockquote>
<div><p>Softmax is mainly used at the output layer in a neural network, and unlike the
other functions it depends not only on the net input at neuron <span class="math notranslate nohighlight">\(k\)</span>, but it
depends on the net signal at all other neurons in the output layer.
Thus, given the net input vector,
<span class="math notranslate nohighlight">\(\bs{\rm{net}}=(net_1,net_2,\cds,net_p)^T\)</span>, for all the <span class="math notranslate nohighlight">\(p\)</span> output
neurons, the output of the softmax function for the <span class="math notranslate nohighlight">\(k\)</span>th neuron is
given as</p>
<div class="math notranslate nohighlight">
\[f(net_k|\bs{\rm{net}})=\frac{\exp\{net_k\}}{\sum_{i=1}^p\exp\{net_i\}}\]</div>
</div></blockquote>
<div class="section" id="derivatives-for-activation-functions">
<h3>25.1.1 Derivatives for Activation Functions<a class="headerlink" href="#derivatives-for-activation-functions" title="Permalink to this headline">¶</a></h3>
<p><strong>Linear/Identity Function:</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\pd f(net_j)}{\pd net_j}=1\]</div>
</div></blockquote>
<p><strong>Step Function:</strong></p>
<blockquote>
<div><p>The step function has a derivative of 0 everywhere except for the
discontinuity at 0, where the derivative is <span class="math notranslate nohighlight">\(\infty\)</span>.</p>
</div></blockquote>
<p><strong>Rectified Linear Unit (ReLU):</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\frac{\pd f(net_k)}{\pd net_j}=\left\{\begin{array}{lr}0\quad\rm{if\ }net_j\leq 0\\1\quad\rm{if\ }net_k&gt;0\end{array}\right.\end{split}\]</div>
<p>At 0, we can set the derivative to be any value in the range <span class="math notranslate nohighlight">\([0,1]\)</span>.</p>
</div></blockquote>
<p><strong>Sigmoid:</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\pd f(net_j)}{\pd net_j}=f(net_j)\cd (1-f(net_j))\]</div>
</div></blockquote>
<p><strong>Hyperbolic Tangent (tanh):</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\pd f(net_j)}{\pd net_j}=1-f(net_j)^2\]</div>
</div></blockquote>
<p><strong>Softmax:</strong></p>
<blockquote>
<div><p>Since softmax is used at the output layer, if we denote the <span class="math notranslate nohighlight">\(i\)</span>th
output neuron as <span class="math notranslate nohighlight">\(o_i\)</span>, then <span class="math notranslate nohighlight">\(f(net_i)=o_i\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\pd f(net_j|\bs{\rm{net}})}{\pd net_k}=\frac{\pd o_j}{\pd net_k}=
\left\{\begin{array}{lr}o+j\cd(1-o_j)\quad\rm{if\ }k=j\\
-o_k\cd o_j\quad\quad\quad\quad\rm{if\ }k\ne j\end{array}\right.\end{split}\]</div>
</div></blockquote>
</div>
</div>
<div class="section" id="neural-networks-regression-and-classification">
<h2>25.2 Neural Networks: Regression and Classification<a class="headerlink" href="#neural-networks-regression-and-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="regression">
<h3>25.2.1 Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h3>
<p>Consider the multiple (linear) regression problem, where given an input
<span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span>, the goal is to predict the response as follows:</p>
<div class="math notranslate nohighlight">
\[\hat{y_i}=b+w_1x_{i1}+w_2x_{i2}+\cds+w_dx_{id}\]</div>
<p>Given a training data <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\x_i\)</span> in a
<span class="math notranslate nohighlight">\(d\)</span>-dimensional space, along with their corresponding true response value
<span class="math notranslate nohighlight">\(y_i\)</span>, the bias and weights for linear regression are chosen so as to
minimize the sum of squared errors between the true and predicted response over
all data points</p>
<div class="math notranslate nohighlight">
\[SSE=\sum_{i=1}^n(y_i-\hat{y_i})^2\]</div>
<p>A neural network with <span class="math notranslate nohighlight">\(d+1\)</span> input neurons <span class="math notranslate nohighlight">\(x_0,x_1,\cds,x_d\)</span>,
including the bias neuron <span class="math notranslate nohighlight">\(x_0=1\)</span>, and a single output neuron <span class="math notranslate nohighlight">\(o\)</span>,
all with identity activation functions and with <span class="math notranslate nohighlight">\(\hat{y}=o\)</span>, represents
the exact same model as multiple linear regression.
Wereas the multiple regression problem has a closed form solution, neural
networks learn the bias and weights via a gradient descent approach that
minimizes the squared error.</p>
<p>Neural networks can just as easily model the multivariate (linear) regression
task, where we have a <span class="math notranslate nohighlight">\(p\)</span>-dimensional response vector <span class="math notranslate nohighlight">\(\y_i\in\R^p\)</span>
instead of a single value <span class="math notranslate nohighlight">\(y_i\)</span>.
That is, the training data <span class="math notranslate nohighlight">\(\D\)</span> comprises <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span> and their true response vectors <span class="math notranslate nohighlight">\(\y_i\in\R^p\)</span>.
Multivariate regression can be modeled by a neural network with <span class="math notranslate nohighlight">\(d+1\)</span>
input neurons, and <span class="math notranslate nohighlight">\(p\)</span> output neurons <span class="math notranslate nohighlight">\(o_1,o_2,\cds,o_p\)</span>, with all
input and output neurons using the identity activation function.
A neural network learns the weights by comparing its predicted output
<span class="math notranslate nohighlight">\(\hat\y=\o=(o_1,o_2,\cds,o_p)^T\)</span> with the true response vector
<span class="math notranslate nohighlight">\(\y=(y_1,y_2,\cds,y_p)^T\)</span>.
That is, training happens by first computing the <em>error function</em> or <em>loss function</em> between <span class="math notranslate nohighlight">\(\o\)</span> and <span class="math notranslate nohighlight">\(\y\)</span>.
When the prediction matches the true output the loss should be zero.
The most common loss function for regression is the squared error function</p>
<div class="math notranslate nohighlight">
\[\cl{E}_\x=\frac{1}{2}\lv\y-\o\rv^2=\frac{1}{2}\sum_{j=1}^p(y_j-o_j)^2\]</div>
<p>where <span class="math notranslate nohighlight">\(\cl{E}_\x\)</span> denotes the error on input <span class="math notranslate nohighlight">\(\x\)</span>.
Across all the points in a dataset, the total sum of squared error is</p>
<div class="math notranslate nohighlight">
\[\cl{E}=\sum_{i=1}^n\cl{E}_{\x_i}=\frac{1}{2}\cd\sum_{i=1}^n\lv\y_i-\o_i\rv^2\]</div>
</div>
<div class="section" id="classification">
<h3>25.2.2 Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
<p>Consider the binary classification problem, where <span class="math notranslate nohighlight">\(y=1\)</span> dentoes that the
point belongs to the positive class, and <span class="math notranslate nohighlight">\(y=0\)</span> means that it belongs to
the negative class.
In logistic regression, we model the probability of the positive class viar the logistic (or sigmoid) function:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\pi(\x)=P(y=1|\x)=\frac{1}{1+\exp\{-(b+\w^T\x)\}}\\P(y=0|\x)=1-P(y=1|\x)=1-\pi(\x)\end{aligned}\end{align} \]</div>
<p>Given input <span class="math notranslate nohighlight">\(\x\)</span>, true response <span class="math notranslate nohighlight">\(y\)</span>, and predicted response
<span class="math notranslate nohighlight">\(o\)</span>, recall that the <em>cross-entropy error</em> is defined as</p>
<div class="math notranslate nohighlight">
\[\cl{E}_\x=-(y\cd\ln(o)+(1-y)\cd\ln(1-o))\]</div>
<p>With sigmoid activation, the output of the neural network is given as</p>
<div class="math notranslate nohighlight">
\[o=f(net_o)=\rm{sigmoid}(b+\w^T\x)=\frac{1}{1+\exp\{-(b+\w^T\x)\}=\pi(\x)\]</div>
<p>which is the same as the logstic regression model.</p>
<p><strong>Multiclass Logistic Regression</strong></p>
<p>For the general classification problem with <span class="math notranslate nohighlight">\(K\)</span> classes
<span class="math notranslate nohighlight">\(\{c_1,c_2,\cds,c_K\}\)</span>, the true response <span class="math notranslate nohighlight">\(y\)</span> is encoded as a
one-hot vector.
Thus, class <span class="math notranslate nohighlight">\(c_i\)</span> is encoded as <span class="math notranslate nohighlight">\(\e_1=(1,0,\cds,0)^T\)</span>, and so on,
with <span class="math notranslate nohighlight">\(\e_1\in\{0,1\}^K\)</span> for <span class="math notranslate nohighlight">\(i=1,2,\cds,K\)</span>.
Thus, we encode <span class="math notranslate nohighlight">\(y\)</span> as a multivariate vector <span class="math notranslate nohighlight">\(\y\in\{\e_1,\e_2,\cds,\e_K\}\)</span>.
Recall that in multiclass logistic regression the task is to estimate the per
class bias <span class="math notranslate nohighlight">\(b_i\)</span> and weight vector <span class="math notranslate nohighlight">\(\w_i\in\R^d\)</span>, with the last
class <span class="math notranslate nohighlight">\(c_K\)</span> used as the base class with fixed bias <span class="math notranslate nohighlight">\(b_K=0\)</span> and fixed
weight vector <span class="math notranslate nohighlight">\(\w_K=(0,0,\cds,0)^T\in\R^d\)</span>.
The probability vector across all <span class="math notranslate nohighlight">\(K\)</span> classes is modeled via the softmax function:</p>
<div class="math notranslate nohighlight">
\[\pi_i(\x)=\frac{\exp\{b_i+\w_i^T\x\}}{\sum_{j=1}^K\exp\{b_J+\w_J^T\x\}},\ \rm{for\ all\ }i=1,2,\cds,K\]</div>
<p>Therefore, the neural network can solve the multiclass logistic regression task,
provided we use a softmax activation at the outputs, and use the <span class="math notranslate nohighlight">\(K\)</span>-way
cross-entropy error, defined as</p>
<div class="math notranslate nohighlight">
\[\cl{E}_\x=-(y_1\cd\ln(o_1)+\cds+y_K\cd\ln(o_K))\]</div>
<p>where <span class="math notranslate nohighlight">\(\x\)</span> is an input vector, <span class="math notranslate nohighlight">\(\o=(o_1,o_2,\cds,o_K)^T\)</span> is the
predicted response vector, and <span class="math notranslate nohighlight">\(\y=(y_1,y_2,\cds,y_k)^T\)</span> is the true
response vector.
Note that only one element of <span class="math notranslate nohighlight">\(\y\)</span> is 1, and the rest are 0, due to the one-hot encoding.</p>
<p>With softmax activation, the output of the neural network is given as</p>
<div class="math notranslate nohighlight">
\[o_i=P(\y=\e_i|\x)=f(net_i|\bs{\rm{net}})=\frac{\exp\{net_i\}}{\sum_{j=1}^p\exp\{net_j\}}=\pi_i(\x)\]</div>
<p>The only restriction we have to impose on the neural network is that weights on
edges into the last output neuron should be zero to model the base class weights
<span class="math notranslate nohighlight">\(\w_K\)</span>.
However, in practice, we relax this restriction, and just learn a regular weight vector for class <span class="math notranslate nohighlight">\(c_K\)</span>.</p>
</div>
<div class="section" id="error-functions">
<h3>25.2.3 Error Functions<a class="headerlink" href="#error-functions" title="Permalink to this headline">¶</a></h3>
<p><strong>Squared Error:</strong></p>
<blockquote>
<div><p>Given an input vector <span class="math notranslate nohighlight">\(\x\in\R^d\)</span>, the squared error loss function
measures the squared deviation between the predicted output vector
<span class="math notranslate nohighlight">\(\o\in\R^p\)</span> and the true response <span class="math notranslate nohighlight">\(\y\in\R^p\)</span>, defined as
follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\cl{E}_\x=\frac{1}{2}\lv\y-\o\rv^2=\frac{1}{2}\sum_{j=1}^p(y_j-o_j)^2\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\cl{E}_\x\)</span> denotes the error on input <span class="math notranslate nohighlight">\(\x\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd\cl{E}_\x}{\pd o_j}=\frac{1}{2}\cd 2\cd(y_j-o_j)\cd -1=o_j-y_j\\\frac{\pd\cl{E}_\x}{\pd\o}=\o-\y\end{aligned}\end{align} \]</div>
</div></blockquote>
<p><strong>Cross-Entropy Error:</strong></p>
<blockquote>
<div><p>For classification tasks, with <span class="math notranslate nohighlight">\(K\)</span> classes
<span class="math notranslate nohighlight">\(\{c_1,c_2,\cds,c_K\}\)</span>, we usually set the number of output neurons
<span class="math notranslate nohighlight">\(p=K\)</span>, with one output neuron per class.
Furthermore, each of the classes is coded as a one-hot vector, with class
<span class="math notranslate nohighlight">\(c_i\)</span> encoded as the <span class="math notranslate nohighlight">\(i\)</span>th standard basis vector
<span class="math notranslate nohighlight">\(\e_i=(e_{i1},e_{i2},\cds,e_{iK})^T\in\{0,1\}^K\)</span>, with
<span class="math notranslate nohighlight">\(e_{ii}=1\)</span> and <span class="math notranslate nohighlight">\(e_{ij}=0\)</span> for all <span class="math notranslate nohighlight">\(j\ne i\)</span>.
Thus, given input <span class="math notranslate nohighlight">\(\x\in\R^d\)</span>, with the true response
<span class="math notranslate nohighlight">\(\y=(y_1,y_2,\cds,y_K)^T\)</span>, where <span class="math notranslate nohighlight">\(\y\in\{\e_1,\e_2,\cds,\e_K\}\)</span>,
the cross-entropy loss is defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\cl{E}_\x=-\sum_{i=1}^Ky_i\cd\ln(o_i)=-(y_1\cd\ln(o_1)+\cds+y_K\cd\ln(o_K))\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd\cl{E}_\x}{\dp o_j}=-\frac{y_j}{o_j}\\\frac{\pd\cl{E}_\x}{\pd\o}=\bigg(\frac{\pd\cl{E}_\x}{\dp o_1},
\frac{\pd\cl{E}_\x}{\dp o_2},\cds,\frac{\pd\cl{E}_\x}{\dp o_K}\bigg)^T=
\bigg(-\frac{y_1}{o_1},-\frac{y_2}{o_2},\cds,-\frac{y_K}{o_K}\bigg)^T\end{aligned}\end{align} \]</div>
</div></blockquote>
<p><strong>Binary Cross-Entropy Error:</strong></p>
<blockquote>
<div><p>For classification tasks with binary classes, it is typical to encode the
positive class as 1 and the negative class as 0, as opposed to using a one-hot encoding as in the general <span class="math notranslate nohighlight">\(K\)</span>-class case.
Given an input <span class="math notranslate nohighlight">\(\x\in\R^d\)</span>, with true response <span class="math notranslate nohighlight">\(y\in\{0,1\}\)</span>, there is only one output neuron <span class="math notranslate nohighlight">\(o\)</span>.
Therefore, the binary cross-entropy error is defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\cl{E}_\x=-(y\cd\ln(o)+(1-y)\cd\ln(1-o))\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\cl{E}_\x}&amp;=\frac{\pd}{\pd o}\{y\cd\ln(o)-(1-y)\cd\ln(1-o)\}\\&amp;=-\bigg(\frac{y}{o}+\frac{1-y}{1-o}\cd-1\bigg)=\frac{-y\cd(1-o)+(1-y)\cd o}{o\cd(1-o)}\\&amp;=\frac{o-y}{o\cd(1-o)}\end{aligned}\end{align} \]</div>
</div></blockquote>
</div>
</div>
<div class="section" id="multilayer-perceptron-one-hidden-layer">
<h2>25.3 Multilayer Perceptron: One Hidden Layer<a class="headerlink" href="#multilayer-perceptron-one-hidden-layer" title="Permalink to this headline">¶</a></h2>
<p>A multilayer perceptron (MLP) is a neural network that has distinct layers of neurons.
The inputs to the neural network comprise the <em>input layer</em>, and the ﬁnal
outputs from the MLP comprise the <em>output layer</em>.
Any intermediate layer is called a <em>hidden layer</em>, and an MLP can have one or many hidden layers.
Networks with many hidden layers are called <em>deep neural networks</em>.
An MLP is also a feed-forward network.
Typically, MLPs are fully connected between layers.</p>
<div class="section" id="feed-forward-phase">
<h3>25.3.1 Feed-forward Phase<a class="headerlink" href="#feed-forward-phase" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\D\)</span> denote the training dataset, comprising <span class="math notranslate nohighlight">\(n\)</span> input points
<span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span> and corresponding true response vectors <span class="math notranslate nohighlight">\(\y_i\in\R^p\)</span>.
For each pair <span class="math notranslate nohighlight">\((\x,\y)\)</span> in the data, in the feed-forward phase, the point
<span class="math notranslate nohighlight">\(\x=(x_1,x_2,\cds,x_d)^T\in\R^d\)</span> is supplied as an input to the MLP.</p>
<p>Given the input neuron values, we compute the output value for each hidden neuron <span class="math notranslate nohighlight">\(z_k\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[z_k=f(net_k)=f\bigg(b_K+\sum_{i=1}^dw_{ik}\cd x_i\bigg)\]</div>
<p>where <span class="math notranslate nohighlight">\(w_{ik}\)</span> denotes the weight between input neuron <span class="math notranslate nohighlight">\(x_i\)</span> and hidden neuron <span class="math notranslate nohighlight">\(z_k\)</span>.</p>
<div class="math notranslate nohighlight">
\[o_j=f(net_j)=f\bigg(b_j+\sum_{i=1}^mw_{ij}\cd z_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(w_{ij}\)</span> denotes the weight between hidden neuron <span class="math notranslate nohighlight">\(z_i\)</span> and output neuron <span class="math notranslate nohighlight">\(o_j\)</span>.</p>
<p>We define the <span class="math notranslate nohighlight">\(d\times m\)</span> matrix <span class="math notranslate nohighlight">\(\W_h\)</span> comprising the weights
between input and hidden layer neurons, and vector <span class="math notranslate nohighlight">\(\b_j\in\R^m\)</span>
comprising the bias terms for hidden layer neurons, given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\W_h=\bp w_{11}&amp;w_{12}&amp;\cds&amp;w_{1m}\\w_{21}&amp;w_{22}&amp;\cds&amp;w_{2m}\\\vds&amp;\vds&amp;
\dds&amp;\vds\\w_{d1}&amp;w_{d2}&amp;\cds&amp;w_{dm}\ep\quad\b_h=\bp b_1\\b_2\\\vds\\b_m\ep\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(w_{ij}\)</span> denotes the weight on the edge between input neuron
<span class="math notranslate nohighlight">\(x_i\)</span> and hidden neuron <span class="math notranslate nohighlight">\(z_j\)</span>, and <span class="math notranslate nohighlight">\(b_i\)</span> denotes the bias
weight from <span class="math notranslate nohighlight">\(x_0\)</span> to <span class="math notranslate nohighlight">\(z_i\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{net}}_h=\b_h+\W_h^T\x\)</span></p>
<p><span class="math notranslate nohighlight">\(\z=f(\bs{\rm{net}}_h=f(\b_h+\w_h^T\x)\)</span></p>
</div>
<p>Here, <span class="math notranslate nohighlight">\(\bs{\rm{net}}_h=(net_1,\cds,net_m)^T\)</span> denotes the net input at each
hidden neuron, and <span class="math notranslate nohighlight">\(\z=(z_1,z_2,\cds,z_m)^T\)</span> denotes the vector of hidden
neuron values.</p>
<p>Likewise, let <span class="math notranslate nohighlight">\(\W_o\in\R^{m\times p}\)</span> denote the weight matrix between the
hidden and output layers, and let <span class="math notranslate nohighlight">\(\b_o\in\R^p\)</span> be the bias vector for
output neurons, given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\W_o=\bp w_{11}&amp;w_{12}&amp;\cds&amp;w_{1p}\\w_{21}&amp;w_{22}&amp;\cds&amp;w_{2p}\\\vds&amp;\vds&amp;
\dds&amp;\vds\\w_{m1}&amp;w_{m2}&amp;\cds&amp;w_{mp}\ep\quad\b_h=\bp b_1\\b_2\\\vds\\b_p\ep\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{net}}_o=\b_o+\W_o^T\z\)</span></p>
<p><span class="math notranslate nohighlight">\(\o=f(\bs{\rm{net}}_o=f(\b_o+\w_o^T\z)\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[\o=f(\b_o+\w_o^T\z)=f(\b_o+\W_o^T\cd f(\b_h+\W_h^T\x))\]</div>
</div>
<div class="section" id="backpropagation-phase">
<h3>25.3.2 Backpropagation Phase<a class="headerlink" href="#backpropagation-phase" title="Permalink to this headline">¶</a></h3>
<p>For a given input pair <span class="math notranslate nohighlight">\((\x,\y)\)</span> in the training data, the MLP first
computes the output vector <span class="math notranslate nohighlight">\(\o\)</span> via the feed-forward step.
Next, it computes the error in the predicted output <em>vis-a-vis</em> the true
response <span class="math notranslate nohighlight">\(\y\)</span> using the squared error function</p>
<div class="math notranslate nohighlight">
\[\cl{E}_\x=\frac{1}{2}\lv\y-\o\rv^2=\frac{1}{2}\sum_{j=1}^p(y_j-o_j)^2\]</div>
<p>The weight update is done via a gradient descent approach to minimize the error.
Let <span class="math notranslate nohighlight">\(\nabla_{w_{ij}}\)</span> be the gradient of the error function with respect
to <span class="math notranslate nohighlight">\(w_{ij}\)</span>, or simply the <em>weight gradient</em> at <span class="math notranslate nohighlight">\(w_{ij}\)</span>.
Given the previous weight estimate <span class="math notranslate nohighlight">\(w_{ij}\)</span>, a new weight is computed by
taking a small step <span class="math notranslate nohighlight">\(\eta\)</span> in a direction that is opposite to the weight
gradient at <span class="math notranslate nohighlight">\(w_{ij}\)</span></p>
<div class="math notranslate nohighlight">
\[w_{ij}=w_{ij}-\eta\cd\nabla_{w_{ij}}\]</div>
<p>In a similar manner, the bias term <span class="math notranslate nohighlight">\(b_j\)</span> is also updated via gradient descent</p>
<div class="math notranslate nohighlight">
\[b_j=b_j-\eta\cd\nabla_{b_j}\]</div>
<p>where <span class="math notranslate nohighlight">\(\nabla_{b_j}\)</span> is the gradient of the error function with respect to
<span class="math notranslate nohighlight">\(b_j\)</span>, which we call the <em>bias gradient</em> at <span class="math notranslate nohighlight">\(b_j\)</span>.</p>
<p><strong>Updating Parameters Between Hidden and Output Layer</strong></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla_{w_{ij}}&amp;=\frac{\pd\cl{E}_\x}{\pd w_{ij}}=\frac{\pd\cl{E}_\x}
{\pd net_j}\cd\frac{\pd net_j}{\pd w_{ij}}=\delta_j\cd z_i\\\nabla_{b_j}&amp;=\frac{\pd\cl{E}_\x}{\pd b_j}=\frac{\pd\cl{E}_\x}{\pd net_j}\cd
\frac{\pd net_j}{\pd b_j}=\delta_j\end{aligned}\end{align} \]</div>
<p>where we use the symbol <span class="math notranslate nohighlight">\(\delta_j\)</span> to denote the partial derivative of the
error with respect to net signal at <span class="math notranslate nohighlight">\(o_j\)</span>, which we also call the
<em>net gradient</em> at <span class="math notranslate nohighlight">\(o_j\)</span></p>
<div class="math notranslate nohighlight">
\[\delta_j=\frac{\pd\cl{E}_\x}{\pd net_j}\]</div>
<p>Futhermore, the partial derivative of <span class="math notranslate nohighlight">\(net_j\)</span> with respect to <span class="math notranslate nohighlight">\(w_{ij}\)</span> and <span class="math notranslate nohighlight">\(b_j\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\frac{\pd net_j}{\pd w_{ij}}=\frac{\pd}{\pd w_{ij}}\bigg\{b_j+\sum_{k=1}^m
w_{kj}\cd z_k\bigg\}=z_i\quad\quad\frac{\pd net_j}{\pd b_j}=\frac{\pd}
{\pd b_j}\bigg\{b_j+\sum_{k=1}^mw_{kj}\cd z_k\bigg\}=1\]</div>
<div class="math notranslate nohighlight">
\[\delta_j=\frac{\pd\cl{E}_\x}{\pd net_j}=\frac{\pd\cl{E}_\x}{\pd f(net_j)}\cd\frac{\pd f(net_j)}{\pd net_j}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(f(net_j)=o_j\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\frac{\pd\cl{E}_\x}{\pd f(net_j)}=\frac{\pd\cl{E}_\x}{\pd o_j}=\frac{\pd}
{\pd o_j}\bigg\{\frac{1}{2}\sum_{k=1}^p(y_k-o_k)^2\bigg\}=(o_j-y_j)\]</div>
<div class="math notranslate nohighlight">
\[\frac{\pd f(net_j)}{\pd net_j}=o_j\cd(1-o_j)\]</div>
<p>Putting it all together, we get</p>
<div class="math notranslate nohighlight">
\[\delta_j=(o_j-y_j)\cd o_j\cd(1-o_j)\]</div>
<p>Let <span class="math notranslate nohighlight">\(\bs\delta_o=(\delta_1,\delta_2,\cds,\delta_p)^T\)</span> denote the vector of
net gradients at each output neuron, which we call the <em>net gradient vector</em> for
the output layer.
We can write <span class="math notranslate nohighlight">\(\bs\delta_o\)</span> as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\delta_o=\o\od(\1-\o)\od(\o-\y)\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\od\)</span> denotes the element-wise product (also called the <em>Hadamard product</em>) between the vectors.</p>
<p>Let <span class="math notranslate nohighlight">\(\z=(z_1,z_2,\cds,z_m)^T\)</span> denote the vector comprising the values of
all hidden layer neurons (after applying the activation function).
We can compute the gradients <span class="math notranslate nohighlight">\(\delta_{w_{ij}}\)</span> for all hidden to output
neuron connections via the outer product of <span class="math notranslate nohighlight">\(\z\)</span> and <span class="math notranslate nohighlight">\(\bs\delta_o\)</span>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\bs\nabla_{\W_o}=\bp\delta_{w_{11}}&amp;\delta_{w_{12}}&amp;\cds&amp;\delta_{w_{1p}}\\\delta_{w_{21}}&amp;\delta_{w_{22}}&amp;\cds&amp;\delta_{w_{2p}}\\\vds&amp;\vds&amp;\dds&amp;\vds\\\delta_{w_{m1}}&amp;\delta_{w_{m2}}&amp;\cds&amp;\delta_{w_{mp}}\ep=\z\cd\bs\delta_o^T\)</span></p>
</div>
<p>The vector of bias gradients is given as:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\nabla_{\b_o}=(\nabla_{b_1},\nabla_{b_2},\cds,\nabla_{b_p})^T=\bs\delta_o\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\W_o=\W_o-\eta\cd\bs\nabla_{\w_o}\)</span></p>
<p><span class="math notranslate nohighlight">\(\b_o=\b_o-\eta\cd\nabla_{\b_o}\)</span></p>
</div>
<p><strong>Updating Parameters Between Input and Hidden Layer</strong></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\nabla_{w_{ij}}&amp;=\frac{\pd\cl{E}_\x}{\pd w_{ij}}=\frac{\pd\cl{E}_\x}
{\pd net_j}\cd\frac{\pd net_j}{\pd w_{ij}}=\delta_j\cd x_i\\\nabla_{b_j}&amp;=\frac{\pd\cl{E}_\x}{\pd b_j}=\frac{\pd\cl{E}_\x}{\pd net_j}\cd
\frac{\pd net_j}{\pd b_j}=\delta_j\end{aligned}\end{align} \]</div>
<p>which follows from</p>
<div class="math notranslate nohighlight">
\[\frac{\pd net_j}{\pd w_{ij}}=\frac{\pd}{\pd w_{ij}}\bigg\{b_j+\sum_{k=1}^m
w_{kj}\cd x_k\bigg\}=x_i\quad\quad\frac{\pd net_j}{\pd b_j}=\frac{\pd}
{\pd b_j}\bigg\{b_j+\sum_{k=1}^mw_{kj}\cd x_k\bigg\}=1\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\delta_j&amp;=\frac{\pd\cl{E}_\x}{\pd net_j}=\sum_{k=1}^p\frac{\pd\cl{E}_\x}
{\pd net_k}\cd\frac{\pd net_k}{\pd z_j}\cd\frac{\pd z_j}{\pd net_j}=
\frac{\pd z_j}{\pd net_j}\cd\sum_{k=1}^p\frac{\pd\cl{E}_\x}{\pd net_k}\cd
\frac{\pd net_k}{\pd z_j}\\&amp;=z_j\cd(1-z_j)\cd\sum_{k=1}^p\delta_k\cd w_{jk}\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\frac{\pd z_j}{\pd net_j}=z_j\cd(1-z_j)\)</span>, since we assume a sigmoid
activation function for the hidden neurons.</p>
<p>Let <span class="math notranslate nohighlight">\(\bs\delta_o=(\delta_1,\delta_2,\cds,\delta_p)^T\)</span> denote the vector of
net gradients at the output nerurons, and
<span class="math notranslate nohighlight">\(\bs\delta_h=(\delta_1,\delta_2,\cds,\delta_m)^T\)</span> the net gradients at the
hidden layer neurons.
We can write <span class="math notranslate nohighlight">\(\bs\delta_h\)</span> compactly as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs\delta_h=\z\od(\1-\z)\od(\W_o\cd\bs\delta_o)\)</span></p>
</div>
<p>Furthermore, <span class="math notranslate nohighlight">\(\W_o\cd\bs\delta_o\in\R^m\)</span> is the vector of weighted gradients at each hidden neuron, since</p>
<div class="math notranslate nohighlight">
\[\W_o\cd\bs\delta_o=\bigg(\sum_{k=1}^p\delta_k\cd w_{1k},\sum_{k=1}^p\delta_k
\cd w_{2k},\cds,\sum_{k=1}^p\delta_k\cd w_{mk}\bigg)^T\]</div>
<p>Let <span class="math notranslate nohighlight">\(\x=(x_1,x_2,\cds,x_d)^T\)</span> denote the input vector, we can compute the
gradients <span class="math notranslate nohighlight">\(\nabla_{w_{ij}}\)</span> for all input to hidden layer connections via
the outer product:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\bs\nabla_{\W_h}=\bp\delta_{w_{11}}&amp;\delta_{w_{12}}&amp;\cds&amp;\delta_{w_{1m}}\\\delta_{w_{21}}&amp;\delta_{w_{22}}&amp;\cds&amp;\delta_{w_{2m}}\\\vds&amp;\vds&amp;\dds&amp;\vds\\\delta_{w_{d1}}&amp;\delta_{w_{d2}}&amp;\cds&amp;\delta_{w_{dm}}\ep=\x\cd\bs\delta_h^T\)</span></p>
</div>
<p>The vector of bias gradients is given as:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\nabla_{\b_j}=(\nabla_{b1},\nabla_{b2},\cds,\nabla_{bm})^T=\bs\delta_h\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\W_h=\W_h-\eta\cd\bs\nabla_{\W_h}\)</span></p>
<p><span class="math notranslate nohighlight">\(\b_h=\b_h-\eta\cd\nabla_{\b_h}\)</span></p>
</div>
</div>
<div class="section" id="mlp-training">
<h3>25.3.3 MLP Training<a class="headerlink" href="#mlp-training" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/Algo25.1.png" src="../_images/Algo25.1.png" />
<p>The total training time per iteration is <span class="math notranslate nohighlight">\(O(dm+mp)\)</span>.</p>
</div>
</div>
<div class="section" id="deep-multilayer-perceptrons">
<h2>25.4 Deep Multilayer Perceptrons<a class="headerlink" href="#deep-multilayer-perceptrons" title="Permalink to this headline">¶</a></h2>
<p>Consider an MLP with <span class="math notranslate nohighlight">\(h\)</span> hidden layers.
We assume that the input to the MLP comprises <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(\x_i\in\R^d\)</span> with the corresponding true response vector
<span class="math notranslate nohighlight">\(\y_i\in\R^p\)</span>.
We denote the input neurons as layer <span class="math notranslate nohighlight">\(l=0\)</span>, the first hidden layer as
<span class="math notranslate nohighlight">\(l=1\)</span>, the last hidden layer as <span class="math notranslate nohighlight">\(l=h\)</span>, and the output layer as layer
<span class="math notranslate nohighlight">\(l=h+1\)</span>.
We use <span class="math notranslate nohighlight">\(n_l\)</span> to denote the number of neurons in layer <span class="math notranslate nohighlight">\(l\)</span>.
We have <span class="math notranslate nohighlight">\(n_0=d\)</span>, and <span class="math notranslate nohighlight">\(n_{h+1}=p\)</span>.
The vector of neuron values for layer <span class="math notranslate nohighlight">\(l\)</span> (for <span class="math notranslate nohighlight">\(l=0,\cds,h+1\)</span>) is denoted as</p>
<div class="math notranslate nohighlight">
\[\z^l=(z_1^l,\cds,z_{n_l}^l)^T\]</div>
<p>Each layer except the output layer has one extra bias neuron, which is the neuron at index 0.
Thus, the bias neuron for layer <span class="math notranslate nohighlight">\(l\)</span> is denoted <span class="math notranslate nohighlight">\(z_0^l\)</span> and its value is fixed at <span class="math notranslate nohighlight">\(z_0^l=1\)</span>.</p>
<p>The vector of input neuron values is also written as</p>
<div class="math notranslate nohighlight">
\[\x=(x_1,x_2,\cds,x_d)^T=(z_1^0,z_2^0,\cds,z_d^0)^T=\z^0\]</div>
<p>and the vector of output neuron values is also denoted as</p>
<div class="math notranslate nohighlight">
\[\o=(o_1,o_2,\cds,o_p)^T=(z_1^{h+1},z_2^{h+1},\cds,z_p^{h+1})^T=\z^{h+1}\]</div>
<p>The weight matrix between layer <span class="math notranslate nohighlight">\(l\)</span> and layer <span class="math notranslate nohighlight">\(l+1\)</span> neurons is
denoted <span class="math notranslate nohighlight">\(\W_l\in\R^{n_l\times n_{l+1}}\)</span>, and the vector of bias terms from
the bias neuron <span class="math notranslate nohighlight">\(z_0^l\)</span> to neurons in layer <span class="math notranslate nohighlight">\(l+1\)</span> is denoted
<span class="math notranslate nohighlight">\(\b_l\in\R^{n_{l+1}}\)</span>, for <span class="math notranslate nohighlight">\(l=0,1,\cds,h\)</span>.</p>
<p>Define <span class="math notranslate nohighlight">\(\delta_i^l\)</span> as the net gradient, i.e., the partial derivative of
the error function with respect to the net value at <span class="math notranslate nohighlight">\(z_i^l\)</span></p>
<div class="math notranslate nohighlight">
\[\delta_i^l=\frac{\pd\cl{E}_\x}{\pd net_i}\]</div>
<p>and let <span class="math notranslate nohighlight">\(\bs\delta^l\)</span> denote the net gradient vector at layer <span class="math notranslate nohighlight">\(l\)</span>, for <span class="math notranslate nohighlight">\(l=1,2,\cds,h+1\)</span></p>
<div class="math notranslate nohighlight">
\[\bs\delta^l=(\delta_1^l,\cds,\delta_{n_l}^l)^t\]</div>
<p>Let <span class="math notranslate nohighlight">\(f^l\)</span> denote the activation function for layer <span class="math notranslate nohighlight">\(l\)</span>, for
<span class="math notranslate nohighlight">\(l=0,1,\cds,h+1\)</span>, and futher let <span class="math notranslate nohighlight">\(\pd\f^l\)</span> denote the vector of the
derivatives of the activation function with respect to <span class="math notranslate nohighlight">\(net_i\)</span> for all
neurons <span class="math notranslate nohighlight">\(z_i^l\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>:</p>
<div class="math notranslate nohighlight">
\[\pd\f^l=\bigg(\frac{\pd f^l(net_1)}{\pd net_1},\cds,\frac{\pd f^l(net_{n_l})}{\pd net_{n_l}}\bigg)^T\]</div>
<p>Finally, let <span class="math notranslate nohighlight">\(\pd\cl{E}_\x\)</span> denote the vector of partial derivatives of
the error function with respect to the values :math:<a href="#id1"><span class="problematic" id="id2">`</span></a>o_i`for all output neurons:</p>
<div class="math notranslate nohighlight">
\[\pd\bs{\cl{E}}_\x=\bigg(\frac{\pd\cl{E}_\x}{\pd o_1},
\frac{\pd\cl{E}_\x}{\pd o_2},\cds,\frac{\pd\cl{E}_\x}{\pd o_p}\bigg)^T\]</div>
<div class="section" id="id3">
<h3>25.4.1 Feed-forward Phase<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>We assume a fixed activation function <span class="math notranslate nohighlight">\(f^l\)</span> for all neurons in a given layer.
For a given input pair <span class="math notranslate nohighlight">\((\x,\y)\in\D\)</span>, the deep MLP computes the output vector via the feed-forward process:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\o&amp;=f^{h+1}(\b_h+\W_h^T\cd\z^h)\\&amp;=f^{h+1}(\b_h+\W_h^T\cd f^h(\b_{h-1}+\W_{h-1}^T\cd\z^{h-1}))\\&amp;=\vds\\&amp;=f^{h+1}(\b_h+\W_h^T\cd f^h(\b_{h-1}+\W_{h-1}^T\cd f^{h-1}(\cds f^2(\b_1+\W_1^T\cd f^1(\b_0+\W_0^T\cd\x)))))\end{aligned}\end{align} \]</div>
<p>Note that each <span class="math notranslate nohighlight">\(f^l\)</span> distributes over its argument.
That is</p>
<div class="math notranslate nohighlight">
\[f^l(\b_{l-1}+\W_{l-1}^T\cd\x)=(f^l(net_1),f^l(net_2),\cds,f^l(net_{n_l}))^T\]</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap26.html" class="btn btn-neutral float-right" title="Chapter 26 Deep Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap24.html" class="btn btn-neutral float-left" title="Chapter 24 Logistic Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>