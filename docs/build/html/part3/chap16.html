

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 16 Spectral and Graph Clustering &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 17 Clustering Validation" href="chap17.html" />
    <link rel="prev" title="Chapter 15 Density-based Clustering" href="chap15.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index3.html">Part 3 Clustering</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap13.html">Chapter 13 Representative-based Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap14.html">Chapter 14 Hierarchical Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap15.html">Chapter 15 Density-based Clustering</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 16 Spectral and Graph Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#graphs-and-matrices">16.1 Graphs and Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="#clustering-as-graph-cuts">16.2 Clustering as Graph Cuts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#clustering-objective-functions-ratio-and-normalized-cut">16.2.1 Clustering Objective Functions: Ratio and Normalized Cut</a></li>
<li class="toctree-l4"><a class="reference internal" href="#spectral-clustering-algorithm">16.2.2 Spectral Clustering Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#maximization-objectives-average-cut-and-modularity">16.2.3 Maximization Objectives: Average Cut and Modularity</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#markov-clustering">16.3 Markov Clustering</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#markov-clustering-algorithm">16.3.1 Markov Clustering Algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap17.html">Chapter 17 Clustering Validation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part5/index5.html">Part 5 Regression</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index3.html">Part 3 Clustering</a> &raquo;</li>
        
      <li>Chapter 16 Spectral and Graph Clustering</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part3/chap16.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\ra}{\rightarrow}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-16-spectral-and-graph-clustering">
<h1>Chapter 16 Spectral and Graph Clustering<a class="headerlink" href="#chapter-16-spectral-and-graph-clustering" title="Permalink to this headline">¶</a></h1>
<div class="section" id="graphs-and-matrices">
<h2>16.1 Graphs and Matrices<a class="headerlink" href="#graphs-and-matrices" title="Permalink to this headline">¶</a></h2>
<p>Given a dataset <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(\x_i\in\R^d\ (i=1,2,\cds,n)\)</span>, let <span class="math notranslate nohighlight">\(\A\)</span> denote the <span class="math notranslate nohighlight">\(n\times n\)</span>
symmetric <em>similarity matrix</em> between the points, given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\A = \left(\begin{array}{cccc}a_{11}&amp;a_{12}&amp;\cds&amp;a_{1n}\\
\a_{21}&amp;a_{22}&amp;\cds&amp;a_{1n}\\\vds&amp;\vds&amp;\dds&amp;\vds\\
\a_{n1}&amp;a_{n2}&amp;\cds&amp;a_{nn}\end{array}\right)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\A(i,j)=a_{ij}\)</span> denotes the similarity or affinity between points <span class="math notranslate nohighlight">\(\x_i\)</span> and <span class="math notranslate nohighlight">\(\x_j\)</span>.
We require the similarity to be symmetric and non-negative, that is, <span class="math notranslate nohighlight">\(a_{ij}=a_{ji}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\geq 0\)</span>.
The matrix <span class="math notranslate nohighlight">\(\A\)</span> may be considered to be a <em>weighted adjacency matrix</em> of
the weighted (undirected) graph <span class="math notranslate nohighlight">\(G=(V,E)\)</span>, where each vertex is a point
and each edge joins a pair of points, that is,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}V&amp;=\{\x_i|i=1,\cds,n\}\\E&amp;=\{(\x_i,\x_j)|1\leq i,j\leq n\}\end{aligned}\end{align} \]</div>
<p>Further, the similarity matrix <span class="math notranslate nohighlight">\(\A\)</span> gives the weight on each edge, that
is, <span class="math notranslate nohighlight">\(a_{ij}\)</span> denotes the weight of the edge <span class="math notranslate nohighlight">\((\x_i,\x_j)\)</span>.
If all affinities are 0 or 1, then <span class="math notranslate nohighlight">\(\A\)</span> represents the regular adjacency relationship between the vertices.</p>
<p>For a vertex <span class="math notranslate nohighlight">\(\x_i\)</span>, let <span class="math notranslate nohighlight">\(d_j\)</span> denote the <em>degree</em> of the vertex, defined as</p>
<div class="math notranslate nohighlight">
\[d_i=\sum_{j=1}^n a_{ij}\]</div>
<p>We define the <em>degree matrix</em> <span class="math notranslate nohighlight">\(\Delta\)</span> of graph <span class="math notranslate nohighlight">\(G\)</span> as the <span class="math notranslate nohighlight">\(n\times n\)</span> diagonal matrix:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\Delta=\left(\begin{array}{cccc}d_1&amp;0&amp;\cds&amp;0\\0&amp;d_2&amp;\cds&amp;0\\\vds&amp;\vds&amp;\dds&amp;\vds\\0&amp;0&amp;\cds&amp;d_n\end{array}\right)\)</span>
<span class="math notranslate nohighlight">\(=\left(\begin{array}{cccc}\sum_{j=1}^na_{1j}&amp;0&amp;\cds&amp;0\\0&amp;\sum_{j=1}^na_{2j}&amp;\cds&amp;0\\\vds&amp;\vds&amp;\dds&amp;\vds\\0&amp;0&amp;\cds&amp;\sum_{j=1}^na_{nj}\end{array}\right)\)</span></p>
</div>
<p><span class="math notranslate nohighlight">\(\Delta\)</span> can be compactly written as <span class="math notranslate nohighlight">\(\Delta(i,i)=d_i\)</span> for all <span class="math notranslate nohighlight">\(1\leq i\leq n\)</span>.</p>
<p><strong>Normalized Adjacency Matrix</strong></p>
<p>The normalized adjacency matrix is obtained by dividing each row of the
adjacency matrix by the degree of the corresponding node.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{M}}=\Delta\im\A=\)</span>
<span class="math notranslate nohighlight">\(\left(\begin{array}{cccc}\frac{a_{11}}{d_1}&amp;\frac{a_{12}}{d_1}&amp;\cds&amp;\frac{a_{1n}}{d_1}\\\frac{a_{21}}{d_2}&amp;\frac{a_{22}}{d_2}&amp;\cds&amp;\frac{a_{2n}}{d_2}\\\vds&amp;\vds&amp;\dds&amp;\vds\\\frac{a_{n1}}{d_n}&amp;\frac{a_{n2}}{d_n}&amp;\cds&amp;\frac{a_{nn}}{d_n}\end{array}\right)\)</span></p>
</div>
<p>Each element of <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>, namely <span class="math notranslate nohighlight">\(m_{ij}\)</span> is also non-negative,
as <span class="math notranslate nohighlight">\(m_{ij}=\frac{a_{ij}}{d_i}\geq 0\)</span>.
Consider the sum of the <span class="math notranslate nohighlight">\(i\)</span>th row in <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\sum_{j=1}^nm_{ij}=\sum_{j=1}^n\frac{a_{ij}}{d_i}=\frac{d_i}{d_i}=1\]</div>
<p>Thus, each row in <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span> sums to 1.
This implies that 1 is an eigenvalue of <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>.
In fact, <span class="math notranslate nohighlight">\(\ld=1\)</span> is the largest eigenvalue of <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>, and the
other eigenvalues satisfy the property that <span class="math notranslate nohighlight">\(|\ld_i|\leq 1\)</span>.
Also, if <span class="math notranslate nohighlight">\(G\)</span> is connected then the eigenvector corresponding to
<span class="math notranslate nohighlight">\(\ld_1\)</span> is
<span class="math notranslate nohighlight">\(\u_1=\frac{1}{\sqrt{n}}=(1,1,\cds,1)^T=\frac{1}{\sqrt{n}}\1\)</span>.
Because <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span> is not symmetric, its eigenvectors are not necessarily orthogonal.</p>
<p><strong>Graph Laplacian Matrices</strong></p>
<p>The <em>Laplacian matrix</em> of a graph is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bs{\rm{L}}=\Delta-\A=\left(\begin{array}{cccc}\sum_{j=1}^na_{1j}&amp;0&amp;\cds&amp;0\\
0&amp;\sum_{j=1}^na_{2j}&amp;\cds&amp;0\\\vds&amp;\vds&amp;\dds&amp;\vds\\
0&amp;0&amp;\cds&amp;\sum_{j=1}^na_{nj}\end{array}\right)
-\left(\begin{array}{cccc}a_{11}&amp;a_{12}&amp;\cds&amp;a_{1n}\\
\a_{21}&amp;a_{22}&amp;\cds&amp;a_{1n}\\\vds&amp;\vds&amp;\dds&amp;\vds\\
\a_{n1}&amp;a_{n2}&amp;\cds&amp;a_{nn}\end{array}\right)\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(=\left(\begin{array}{cccc}\sum_{j\ne 1}^na_{1j}&amp;-a_{12}&amp;\cds&amp;-a_{1n}\\-a{21}&amp;\sum_{j\ne 2}^na_{2j}&amp;\cds&amp;-a_{2n}\\\vds&amp;\vds&amp;\dds&amp;\vds\\-a_{n1}&amp;-a_{n2}&amp;\cds&amp;\sum_{j\ne n}^na_{nj}\end{array}\right)\)</span></p>
</div>
<p><span class="math notranslate nohighlight">\(\bs{\rm{L}}\)</span> is a symmetric, positive semidefinite matrix, as for any <span class="math notranslate nohighlight">\(\c\in\R^n\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\c^T\bs{\rm{L}}\c&amp;=\c^T(\Delta-\A)\c=\c^T\Delta\c-\c^T\A\c\\&amp;=\sum_{i=1}^nd_ic_i^2-\sum_{i=1}^n\sum_{j=1}^nc_ic_ja_{ij}\\&amp;=\frac{1}{2}\bigg(\sum_{i=1}^nd_ic_i^2-2\sum_{i=1}^n\sum_{j=1}^nc_ic_ja_{ij}+\sum_{j=1}^nd_jc_j^2\bigg)\\&amp;=\frac{1}{2}\bigg(\sum_{i=1}^n\sum_{j=1}^na_{ij}c_i^2-2\sum_{i=1}^n
\sum_{j=1}^nc_ic_ja_{ij}+\sum_{i=j}^n\sum_{i=1}^na_{ij}c_j^2\bigg)\\&amp;=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^na_{ij}(c_i-c_j)^2\\&amp;\geq 0\end{aligned}\end{align} \]</div>
<p>This means that <span class="math notranslate nohighlight">\(\bs{\rm{L}}\)</span> has <span class="math notranslate nohighlight">\(n\)</span> real, non-negative
eigenvalues, which can be arranged in decreasing order as follows:
<span class="math notranslate nohighlight">\(\ld_1\geq\ld_2\geq\cds\geq\ld_n\geq 0\)</span>.
Because <span class="math notranslate nohighlight">\(\bs{\rm{L}}\)</span> is symmetric, its eigenvectors are orthonormal.
We can observe that the first column (and the first row) is a linear combination of the remaining columns (rows).
This implies that the rank of <span class="math notranslate nohighlight">\(\bs{\rm{L}}\)</span> is at most <span class="math notranslate nohighlight">\(n-1\)</span>, and
the smallest eigenvalue is <span class="math notranslate nohighlight">\(\ld_n=0\)</span>, with the corresponding eigenvector
given as <span class="math notranslate nohighlight">\(\u_n=\frac{1}{\sqrt{n}}=(1,1,\cds,1)^T=\frac{1}{\sqrt{n}}\1\)</span>,
provided the graph is connected.
If the graph is disconnected, then the number of eigenvalues equal to zero
specifies the number of connected components in the graph.</p>
<p>The <em>normalized symmetric Laplacian matrix</em> of a graph is defined as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\bs{\rm{L}}^S&amp;=\Delta^{-1/2}\bs{\rm{L}}\Delta^{-1/2}\\&amp;=\bs{\rm{L}}^{-1/2}(\Delta-\A)\Delta^{-1/2}=\Delta^{-1/2}\Delta\Delta^{-1/2}-\Delta^{-1/2}\A\Delta^{-1/2}\\&amp;=\I-\Delta^{-1/2}\A\Delta^{-1/2}\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{L}}^S=\Delta^{-1/2}\bs{\rm{L}}\Delta^{-1/2}\)</span>
<span class="math notranslate nohighlight">\(=\left(\begin{array}{cccc} \frac{\sum_{j\ne 1}a_{1j}}{\sqrt{d_1d_1}}&amp;-\frac{a_{12}}{\sqrt{d_1d_2}}&amp;\cds&amp;-\frac{a_{1n}}{\sqrt{d_1d_n}}\\-\frac{a_{21}}{\sqrt{d_2d_1}}&amp;\frac{\sum_{j\ne 2}a_{2j}}{\sqrt{d_2d_2}}&amp;\cds&amp;-\frac{a_{2n}}{\sqrt{d_2d_n}}\\\vds&amp;\vds&amp;\dds&amp;\vds\\-\frac{a_{n1}}{\sqrt{d_nd_1}}&amp;-\frac{a_{n2}}{\sqrt{d_nd_2}}&amp;\cds&amp;\frac{\sum_{j\ne n}a_{nj}}{\sqrt{d_nd_n}}\end{array}\right)\)</span></p>
</div>
<p>We can hsow that <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span> is also positive semidefinite because for any <span class="math notranslate nohighlight">\(\c\in\R^d\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[\c^T\bs{\rm{L}}^s\c=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^na_{ij}
\bigg(\frac{c_i}{\sqrt{d_i}}-\frac{c_j}{\sqrt{d_j}}\bigg)^2\geq 0\]</div>
<p>The first column is also a linear combination of the other columns, which means
that <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span> has rank at most <span class="math notranslate nohighlight">\(n-1\)</span>, with the smallest
eigenvalue <span class="math notranslate nohighlight">\(\ld_n=0\)</span>, and the corresponding eigenvector
<span class="math notranslate nohighlight">\(\frac{1}{\sqrt{\sum_id_i}}(\sqrt{d_1},\sqrt{d_2},\cds,\sqrt{d_n})^T=\frac{1}{\sqrt{\sum_id_i}}\Delta^{1/2}\1\)</span>.
Combined with the fact that <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span> is positive semidefinite, we
conclude that <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span> has <span class="math notranslate nohighlight">\(n\)</span> (not necessarily distinct)
real, positive eigenvalues <span class="math notranslate nohighlight">\(\ld_1\geq\ld_2\geq\cds\geq\ld_n=0\)</span>.</p>
<p>The <em>normalized asymmetric Laplacian</em> matrix is defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{L}}^a=\Delta\im\bs{\rm{L}}=\Delta\im(\Delta-\A)=\I-\Delta\im\A\)</span>
<span class="math notranslate nohighlight">\(=\left(\begin{array}{cccc}\frac{\sum_{j\ne 1}a_{1j}}{d_1}&amp;-\frac{a_{12}}{d_1}&amp;\cds&amp;-\frac{a_{1n}}{d_1}\\-\frac{a_{21}}{d_2}&amp;\frac{\sum_{j\ne 2}a_{2j}}{d_2}&amp;\cds&amp;-\frac{a_{2n}}{d_2}\\\vds&amp;\vds&amp;\dds&amp;\vds\\-\frac{a_{n1}}{d_n}&amp;-\frac{a_{n2}}{d_n}&amp;\cds&amp;\frac{\sum_{j\ne n}a_{nj}}{d_n}\end{array}\right)\)</span></p>
</div>
<p>Consider the eigenvalue equation for the symmetric Laplacian <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\bs{\rm{L}}^S\u&amp;=\ld\u\\\Delta^{-1/2}\bs{\rm{L}}^S\u&amp;=\ld\Delta^{-1/2}\u\\\Delta^{-1/2}(\Delta^{-1/2}\bs{\rm{L}}\Delta^{-1/2})\u&amp;=\ld\Delta^{-1/2}\u\\\Delta\im\bs{\rm{L}}(\Delta^{-1/2}\u)&amp;=\ld(\Delta^{-1/2}\u)\\\bs{\rm{L}}^a\v=\ld\v\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\v=\Delta^{-1/2}\u\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\bs{\rm{L}}^a\)</span>, and
<span class="math notranslate nohighlight">\(\u\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span>.</p>
</div>
<div class="section" id="clustering-as-graph-cuts">
<h2>16.2 Clustering as Graph Cuts<a class="headerlink" href="#clustering-as-graph-cuts" title="Permalink to this headline">¶</a></h2>
<p>A <em>k-way cut</em> in a graph is a partitioning or clustering of the vertex set,
given as <span class="math notranslate nohighlight">\(\cl{C}=\{C_1,\cds,C_k\}\)</span>, such that <span class="math notranslate nohighlight">\(C_i\ne\emptyset\)</span> for
all <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(C_i\cap C_j=\emptyset\)</span> for all <span class="math notranslate nohighlight">\(i, j\)</span>, and
<span class="math notranslate nohighlight">\(V=\bigcup_ic_i\)</span>.
We require <span class="math notranslate nohighlight">\(\cl{C}\)</span> to optimize some objective function that cptures the
intuition that nodes within a cluster should have high similarity, and nodes
from different clusters should have low similarity.</p>
<p>Given a weighted graph <span class="math notranslate nohighlight">\(G\)</span> defined by its similarity matrix, let
<span class="math notranslate nohighlight">\(S, T\subseteq V\)</span> be any two subsets of the vertices.
We denote by <span class="math notranslate nohighlight">\(W(S,T)\)</span> the sum of the weights on all edges with one vertex
in <span class="math notranslate nohighlight">\(S\)</span> and the other in <span class="math notranslate nohighlight">\(T\)</span>, given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp W(S,T)=\sum_{v_i\in S}\sum_{v_j\in T}a_{ij}\)</span></p>
</div>
<p>Given <span class="math notranslate nohighlight">\(S\subseteq V\)</span>, we denote by <span class="math notranslate nohighlight">\(\bar{S}\)</span> the complementary set
of vertices, that is, <span class="math notranslate nohighlight">\(\bar{S}=V-S\)</span>.
A <em>（vertex) cut</em> in a graph is defined as a partitioning of <span class="math notranslate nohighlight">\(V\)</span> into <span class="math notranslate nohighlight">\(S\subset V\)</span> and <span class="math notranslate nohighlight">\(\bar{S}\)</span>.
The <em>weight of the cut</em> or <em>cut weight</em> is defined as the sum of all the weights
on edges between vertices in <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(\bar{S}\)</span>, given as
<span class="math notranslate nohighlight">\(W(S,\bar{S})\)</span>.</p>
<p>Given a clustering <span class="math notranslate nohighlight">\(\cl{C}=\{C_1,\cds,C_k\}\)</span> comprising <span class="math notranslate nohighlight">\(k\)</span>
clusters, the <em>size</em> of a cluster <span class="math notranslate nohighlight">\(C_i\)</span> is the number of nodes in the
cluster, given as <span class="math notranslate nohighlight">\(|C_i|\)</span>.
The <em>volume</em> of a cluster <span class="math notranslate nohighlight">\(C_i\)</span> is defined as the sum of all the weights
on edges with one end in cluster <span class="math notranslate nohighlight">\(C_i\)</span>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp vol(C_i)=\sum_{v_j\in C_i}d_j=\sum_{v_j\in C_i}\sum_{v_r\in V}a_{jr}=W(C_i,V)\)</span></p>
</div>
<p>Let <span class="math notranslate nohighlight">\(\c_i=\{0,1\}^n\)</span> be the <em>cluster indicator vector</em> that records the
cluster membership for cluster <span class="math notranslate nohighlight">\(C_i\)</span>, defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}c_{ij}=\left\{\begin{array}{lr}1\quad\rm{if\ }v_j\in C_i\\0\quad\rm{if\ }v_j\notin C_i\end{array}\right.\end{split}\]</div>
<p>Because a clustering creates pairwise disjoint clusters, we immediately have</p>
<div class="math notranslate nohighlight">
\[\c_i^T\c_j=0\]</div>
<p>The cluster size can be written as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(|C_i|=\c_i^T\c_i=\lv\c_i\rv^2\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[vol(C_i)=W(C_i,V)=\sum_{v_r\in C_i}d_r=\sum_{v_r\in C_i}c_{ir}d_rc_{ir}=
\sum_{r=1}^n\sum_{s=1}^nc_{ir}\Delta_{rs}c_{is}\]</div>
<p>The volume of the cluster can be written as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(vol(C_i)=\c_i^T\Delta\c_i\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[W(C_i,C_i)=\sum_{v_r\in C_i}\sum_{v_s\in C_i}a_{rs}=\sum_{r=1}^n\sum_{s=1}^nc_{ir}a_{rs}c_{is}\]</div>
<p>The sum of internal weights can be written as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(W(C_i,C_i)=\c_i^T\A\c_i\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp W(C_i,\bar{C_i})=\sum_{v_r\in C_i}\sum_{v_s\in V-C_i}a_{rs}=W(C_i,V)-W(C_i,C_i)\)</span>
<span class="math notranslate nohighlight">\(=\c_i(\Delta-\A)\c_i=\c_i^T\bs{\rm{L}}\c_i\)</span></p>
</div>
<div class="section" id="clustering-objective-functions-ratio-and-normalized-cut">
<h3>16.2.1 Clustering Objective Functions: Ratio and Normalized Cut<a class="headerlink" href="#clustering-objective-functions-ratio-and-normalized-cut" title="Permalink to this headline">¶</a></h3>
<p><strong>Ratio Cut</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_{\cl{C}}J_{rc}(\cl{C})=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{|C_i|}\)</span>
<span class="math notranslate nohighlight">\(=\dp\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\c_i}\)</span>
<span class="math notranslate nohighlight">\(=\dp\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\lv\c_i\rv^2}\)</span></p>
</div>
<p>ratio cut tries to minimize the sum of the similarities from a cluster
<span class="math notranslate nohighlight">\(C_i\)</span> to other points not in the cluster <span class="math notranslate nohighlight">\(\bar{C_i}\)</span>, taking into
account the size of each cluster.</p>
<p>For binary cluster indicator vectors <span class="math notranslate nohighlight">\(\c_i\)</span>, the ratio cut objective is NP-hard.
An obvious relaxation is to allow <span class="math notranslate nohighlight">\(\c_i\)</span> to take on any real value.</p>
<div class="math notranslate nohighlight">
\[\min_{\cl{C}}J_{rc}(\cl{C})=
\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\lv\c_i\rv^2}=
\sum_{i=1}^k\bigg(\frac{\c_i}{\lv\c_i\rv}\bigg)^T\bs{\rm{L}}
\bigg(\frac{\c_i}{\lv\c_i\rv}\bigg)=\sum_{i=1}^k\u_i^T\bs{\rm{L}}\u_i\]</div>
<p>where <span class="math notranslate nohighlight">\(\u_i=\frac{\c_i}{\lv\c_i\rv}\)</span> is the unit vector in the direction of <span class="math notranslate nohighlight">\(\c_i\in\R^n\)</span>.</p>
<p>To incorporate the constraint the <span class="math notranslate nohighlight">\(\u_i^T\u_i=1\)</span>, we introduce the
Lagrange multiplier <span class="math notranslate nohighlight">\(\ld_i\)</span> for each cluster <span class="math notranslate nohighlight">\(C_i\)</span>.
We have</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\u_i}\bigg(\sum_{i=1}^k\u_i^T\bs{\rm{L}}\u_i+\sum_{i=1}^n\ld_i(1-\u_i^T\u_i)\bigg)&amp;=\0\\2\bs{\rm{L}}\u_i-2\ld_i\u_i&amp;=\0\\\bs{\rm{L}}\u_i&amp;=\ld_i\u_i\\\u_i^T\bs{\rm{L}}\u_i&amp;=\u_i^T\ld_i\u_i=\ld_i\end{aligned}\end{align} \]</div>
<p>which in turn implies that to minimize the ratio cut objective, we should choose
the <span class="math notranslate nohighlight">\(k\)</span> smallest eigenvalues, and the corresponding eigenvectors, so that</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_{\cl{C}}J_{rc}(\cl{C})&amp;=\u_n^T\bs{\rm{L}}\u_n+\cds+\u_{n-k+1}^T\bs{\rm{L}}\u_{n-k+1}\\&amp;=\ld_n+\cds+\ld_{n-k+1}\end{aligned}\end{align} \]</div>
<p><strong>Normalized Cut</strong></p>
<p><em>Normalized cut</em> is similar to ratio cut, except that it divides the cut weight
of each cluster by the volume of a cluster instead of its size.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_{\cl{C}}J_{nc}(\cl{C})=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{vol(C_i)}\)</span>
<span class="math notranslate nohighlight">\(=\dp\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\Delta\c_i}\)</span></p>
</div>
<p>We assume <span class="math notranslate nohighlight">\(\c_i\)</span> to be an arbitrary real vector, and rewrite the
normalized cut objective in terms of the normalized symmetrc Laplacian, as
follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_{\cl{C}}J_{nc}(\cl{C})
&amp;=\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\Delta\c_i}
=\sum_{i=1}^k\frac{\c_i^T(\Delta^{1/2}\Delta^{-1/2})\bs{\rm{L}}
(\Delta^{-1/2}\Delta^{1/2})\c_i}{\c_i^T(\Delta^{1/2}\Delta^{1/2})\c_i}\\&amp;=\sum_{i=1}^k\frac{(\Delta^{1/2}\c_i)^T(\Delta^{-1/2}\bs{\rm{L}}
\Delta^{-1/2})(\Delta^{1/2}\c_i)}{(\Delta^{1/2}\c_i)^T(\Delta^{1/2}\c_i)}\\&amp;=\sum_{i=1}^k\bigg(\frac{\Delta^{1/2}\c_i}{\lv\Delta^{1/2}\c_i\rv}\bigg)^T
\bs{\rm{L}}^S\bigg(\frac{\Delta^{1/2}\c_i}{\lv\Delta^{1/2}\c_i\rv}\bigg)
=\sum_{i=1}^k\u_i^T\bs{\rm{L}}^S\u_i\end{aligned}\end{align} \]</div>
<p>The normalized cut objective can also be expressed in terms of the normalized asymmetric Laplacian:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\c_i}\bigg(\sum_{j=1}^k\frac{\c_j^T\bs{\rm{L}}\c_j}
{\c_j^T\Delta\c_j}\bigg)=\frac{\pd}{\pd\c_i}
\bigg(\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\Delta\c_i}\bigg)&amp;=\0\\\frac{\bs{\rm{L}}\c_i(\c_i^T\Delta\c_i)-\Delta\c_i(\c_i^T\bs{\rm{L}}\c_i)}
{(\c_i^T\Delta\c_i)^2}&amp;=0\\\bs{\rm{L}}\c_i&amp;=\bigg(\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\Delta\c_i}\bigg)\Delta\c_i\\\Delta\im\bs{\rm{L}}\c_i&amp;=\ld_i\c_i\\\bs{\rm{L}}^a\c_i&amp;=\ld_i\c_i\end{aligned}\end{align} \]</div>
</div>
<div class="section" id="spectral-clustering-algorithm">
<h3>16.2.2 Spectral Clustering Algorithm<a class="headerlink" href="#spectral-clustering-algorithm" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/Algo16.1.png" src="../_images/Algo16.1.png" />
<p>Eq.(16.23): <span class="math notranslate nohighlight">\(\dp\y_i=\frac{1}{\sqrt{\sum_{j=1}^ku_{n-j+1,i}^2}}(u_{n,i},u_{n-1,i},\cds,u_{n-k+1,i})^T\)</span></p>
<p><strong>Computational Complexity</strong></p>
<p>The computational complexity of the spectral clustering algorithm is <span class="math notranslate nohighlight">\(O(n^3)\)</span>.</p>
</div>
<div class="section" id="maximization-objectives-average-cut-and-modularity">
<h3>16.2.3 Maximization Objectives: Average Cut and Modularity<a class="headerlink" href="#maximization-objectives-average-cut-and-modularity" title="Permalink to this headline">¶</a></h3>
<p><strong>Average Weight</strong></p>
<p>The <em>average weight</em> objective is defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\max_{\cl{C}}J_{aw}(\cl{C})=\sum_{i=1}^k\frac{W(C_i,C_i)}{|C_i|}=\sum_{i=1}^k\frac{\c_i^T\A\c_i}{\c_i^T\c_i}\)</span></p>
</div>
<p>Instead of trying to minimize the weights on edges between clusters as in ratio
cut, average weight tries to maximize the within cluster weights.
The problem of maximizing <span class="math notranslate nohighlight">\(J_{aw}\)</span> for binary cluster indicator vectors is
also NP-hard; we can obtain a solution by relaxing the constraint on
<span class="math notranslate nohighlight">\(\c_i\)</span>, by assuming that it can take on any real values for its elements.</p>
<div class="math notranslate nohighlight">
\[\max_{\cl{C}}J_{aw}(\cl{C})=\sum_{i=1}^k\u_i^T\A\u_i\]</div>
<p>We can maximize the objective by selecting the <span class="math notranslate nohighlight">\(k\)</span> largest eigenvalues of
<span class="math notranslate nohighlight">\(\A\)</span>, and the corresponding eigenvectors</p>
<div class="math notranslate nohighlight">
\[\max_{\cl{C}}J_{aw}(\cl{C})=\u_1^T\A\u_1+\cds+\u_k^T\A\u_k=\ld_1+\cds+\ld_k\]</div>
<p>where <span class="math notranslate nohighlight">\(\ld_1\geq\ld_2\geq\cds\ld_n\)</span>.</p>
<p><strong>Average Weight and Kernel K-means</strong></p>
<p>If the weighted adjacency matrix <span class="math notranslate nohighlight">\(\A\)</span> represents the kernel value between
a pair of points, so that <span class="math notranslate nohighlight">\(a_{ij}=K(\x_i,\x_j)\)</span>, then we may use the sum
of squared errors objective of kernel K-means for graph clustering.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_{\cl{C}}J_{sse}(\cl{C})&amp;=\sum_{j=1}^nK(\x_j,\x_j)-\sum_{i=1}^k
\frac{1}{|C_i|}\sum_{\x_r\in C_i}\sum_{\x_s\in C_i}K(\x_r,\x_s)\\&amp;=\sum_{j=1}^na_{jj}-\sum_{i=1}^k\frac{1}{|C_i|}\sum_{v_r\in C_i}\sum_{v_s\in C_i}a_{rs}\\&amp;=\sum_{j=1}^na_{jj}-\sum_{i=1}^k\frac{\c_i^T\A\c_i}{\c_i^T\c_i}\\&amp;=\sum_{j=1}^na_{jj}-J_{aw}(\cl{C})\end{aligned}\end{align} \]</div>
<p>We can observe that because <span class="math notranslate nohighlight">\(\sum_{j=1}^na_{jj}\)</span> is independent of the
clustering, minimizing the SSE objective is the same as maximizing the average
weight objective.
In particular, if <span class="math notranslate nohighlight">\(a_{jj}\)</span> represents the linear kernel <span class="math notranslate nohighlight">\(\x_i^T\x_j\)</span>
between the nodes, then maximizing the average weight objective is equivalent to
minimizing the regular K-means SSE objective.</p>
<p><strong>Modularity</strong></p>
<p>Informally, modularity is defined as the difference between the observed and
expected fraction of edges within a cluster.
It measures the extent to which nodes of the same type are linked to each other.</p>
<p><strong>Unweighted Graphs</strong></p>
<p>Let us assume for the moment that the graph <span class="math notranslate nohighlight">\(G\)</span> is unweighted, and that <span class="math notranslate nohighlight">\(\A\)</span> is its binary adjacency matrix.
The number of edges within a cluster <span class="math notranslate nohighlight">\(C_i\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2}\sum_{v_r\in C_i}\sum_{v_s\in C_i}a_{rs}\]</div>
<p>where we divide by <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> because each edge is counted twice in the summation.
Over all the clusters, the observed number of edges within the same cluster is given as</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2}\sum_{i=1}^k\sum_{v_r\in C_i}\sum_{v_s\in C_i}a_{rs}\]</div>
<p>The probability that one end of an edge is <span class="math notranslate nohighlight">\(v_r\)</span> and the other <span class="math notranslate nohighlight">\(v_s\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[p_{rs}=\frac{d_r}{2m}\cd\frac{d_s}{2m}=\frac{d_rd_s}{4m^2}\]</div>
<p>The number of edges between <span class="math notranslate nohighlight">\(v_r\)</span> and <span class="math notranslate nohighlight">\(v_s\)</span> follows a binomial
distribution with success probability <span class="math notranslate nohighlight">\(p_{rs}\)</span> over <span class="math notranslate nohighlight">\(2m\)</span> trails
(because we are selecting the two ends of <span class="math notranslate nohighlight">\(m\)</span> edges).
The expected number of edges between <span class="math notranslate nohighlight">\(v_r\)</span> and <span class="math notranslate nohighlight">\(v_s\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[2m\cd p_{rs}=\frac{d_rd_s}{2m}\]</div>
<p>The expected number of edges within a cluster <span class="math notranslate nohighlight">\(C_i\)</span> is then</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2}\sum_{v_r\in C_i}\sum_{v_s\in C_i}\frac{d_rd_s}{2m}\]</div>
<p>and the expected number of edges within the same cluster, summed over all <span class="math notranslate nohighlight">\(k\)</span> clusters, is given as</p>
<div class="math notranslate nohighlight">
\[\frac{1}{2}\sum_{i=1}^k\sum_{v_r\in C_i}\sum_{v_s\in C_i}\frac{d_rd_s}{2m}\]</div>
<p>where we divide by 2 because each edge is counted twice.
The <em>modularity</em> of the clustering <span class="math notranslate nohighlight">\(\cl{C}\)</span> is defined as the difference
between the observed and expected fraction of edges within the same cluster.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Q&amp;=\frac{1}{2m}\sum_{i=1}^k\sum_{v_r\in C_i}\sum_{v_s\in C_i}\bigg(a_{rs}-\frac{d_rd_s}{2m}\bigg)\\&amp;=\sum_{i=1}^k\sum_{v_r\in C_i}\sum_{v_s\in C_i}\bigg(
    \frac{a_{rs}}{\sum_{j=1}^nd_j}-\frac{d_rd_s}{(\sum_{j=1}^nd_j)^2}\bigg)\end{aligned}\end{align} \]</div>
<p><strong>Weighted Graphs</strong></p>
<p>Assume that <span class="math notranslate nohighlight">\(\A\)</span> is the weighted adjacency matrix; we interpret the
modularity of a clustering as the difference between the observed and expected
fraction of weights on edges within the clusters.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\sum_{v_r\in C_i}\sum_{v_s\in C_i}a_{rs}=W(C_i,C_i)\\\sum_{v_r\in C_i}\sum_{v_s\in C_i}d_rd_s=\bigg(\sum_{v_r\in C_i}d_r\bigg)\bigg(\sum_{v_s\in C_i}d_s\bigg)=W(C_i,V)^2\\\sum_{j=1}^nd_j=W(V,V)\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp \max_{\cl{C}}J_Q(\cl{C})=\sum_{i=1}^k\bigg(\frac{W(C_i,C_i)}{W(V,V)}\)</span>
<span class="math notranslate nohighlight">\(\dp-\bigg(\frac{W(C_i,V)}{W(V,V)}\bigg)^2\bigg)\)</span></p>
</div>
<p>We now express the modularity obejctive in matrix terms.
We have</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}W(C_i,C_i)=\c_i^T\A\c_i\\W(C_i,V)=\sum_{v_r\in C_i}d_r=\sum_{v_r\in C_i}d_rc_{ir}=\sum_{j=1}^nd_jc_{ij}=\bs{\rm{d}}^T\c_i\\W(V,V)=\sum_{j=1}^nd_j=tr(\Delta)\end{aligned}\end{align} \]</div>
<p>The clustering objective based on modularity can then be written as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\max_{\cl{C}}J_Q(\cl{C})&amp;=\sum_{i=1}^k\bigg(\frac{\c_i^T\A\c_i}{tr(\Delta)}-
\frac{(\bs{\rm{d}}^T\c_i)^2}{tr(\Delta)^2}\bigg)\\&amp;=\sum_{i=1}^k\bigg(\c_i^T\bigg(\frac{\A}{tr(\Delta)}\bigg)\c_i-
\c_i^T\bigg(\frac{\bs{\rm{d}}\cd\bs{\rm{d}}^T}{tr(\Delta)^2}\bigg)\c_i\bigg)\\&amp;=\sum_{i=1}^k\c_i^T\bs{\rm{Q}}\c_i\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\bs{\rm{Q}}\)</span> is the <em>modularity matrix</em>:</p>
<div class="math notranslate nohighlight">
\[\bs{\rm{Q}}=\frac{1}{tr(\Delta)}\bigg(\A-\frac{\bs{\rm{d}}\cd\bs{\rm{d}}^T}{tr(\Delta)}\bigg)\]</div>
<p>We select the <span class="math notranslate nohighlight">\(k\)</span> largest eigenvalues and the corresponding eigenvectors to obtain</p>
<div class="math notranslate nohighlight">
\[\max_{\cl{C}}=J_Q(\cl{C})=\u_1^T\bs{\rm{Q}}\u_1+\cds+\u_k^T\bs{\rm{Q}}\u_k=\ld_1+\cds+\ld_k\]</div>
<p><strong>Modularity as Average Weight</strong></p>
<p>We know that each row of <span class="math notranslate nohighlight">\(\bs{\rm{M}}=\Delta\im\A\)</span> sums to 1, that is</p>
<div class="math notranslate nohighlight">
\[\sum_{j=1}^nm_{ij}=d_i=1,\forall i=1,\cds,n\]</div>
<p>The modularity matrix can then be written as</p>
<div class="math notranslate nohighlight">
\[\bs{\rm{Q}}=\frac{1}{n}\bs{\rm{M}}-\frac{1}{n^2}\1_{n\times n}\]</div>
<p>For large graphs with many nodes, <span class="math notranslate nohighlight">\(n\)</span> is large and the second term
practically vanishes, as <span class="math notranslate nohighlight">\(\frac{1}{n^2}\)</span> will be very small.
Thus, the modularity matrix can be reasonably approximated as</p>
<div class="math notranslate nohighlight">
\[\bs{\rm{Q}}\simeq\frac{1}{n}\bs{\rm{M}}\]</div>
<div class="math notranslate nohighlight">
\[\max_{\cl{C}}J_Q(\cl{C})=\sum_{i=1}^k\c_i^T\bs{\rm{Q}}\c_i=\sum_{i=1}^k\c_i^T\bs{\rm{M}}\c_i\]</div>
<p>where we dropped the <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span> factor because it is a constant for a
given graph; it only scales the eigenvalues without effecting the eigenvectors.</p>
<p>In conclusion, if we use the normalized adjacency matrix, maximizing the
modularity is equivalent to selecting the <span class="math notranslate nohighlight">\(k\)</span> largest eigenvalues and the
corresponding eigenvectors of the normalized adjacency matrix
<span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>.</p>
<p><strong>Normalized Modularity as Normalized Cut</strong></p>
<p>Define the <em>normalized modularity</em> objective as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\max_{\cl{C}}J_{nQ}(\cl{C})=\sum_{i=1}^k\frac{1}{W(C_i,V)}\)</span>
<span class="math notranslate nohighlight">\(\dp\bigg(\frac{W(C_i,C_i)}{W(V,V)}-\bigg(\frac{W(C_i,V)}{W(V,V)}\bigg)^2\bigg)\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}J_{nQ}(\cl{C})&amp;=\frac{1}{W(V,V)}\sum_{i=1}^k\bigg(\frac{W(C_i,C_i)}{W(C_i,V)}-\frac{W(C_i,V)}{W(V,V)}\bigg)\\&amp;=\frac{1}{W(V,V)}\bigg(\sum_{i=1}^k\bigg(\frac{W(C_i,C_i)}{W(C_i,V)}\bigg)-
-\sum_{i=1}^k\bigg(\frac{W(C_i,V)}{W(V,V)}\bigg)\bigg)\\&amp;=\frac{1}{W(V,V)}\bigg(\sum_{i=1}^k\bigg(\frac{W(C_i,C_i)}{W(C_i,V)}\bigg)-1\bigg)\end{aligned}\end{align} \]</div>
<p>We have</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}(k-1)-W(V,V)J_{nQ}(\cl{C})&amp;=(k-1)-\bigg(\sum_{i=1}^k\bigg(\frac{W(C_i,C_i)}{W(C_i,V)}\bigg)-1\bigg)\\&amp;=k-\sum_{i=1}^k\frac{W(C_i,C_i)}{W(C_i,V)}\\&amp;=\sum_{i=1}^k1-\frac{W(C_i,C_i)}{W(C_i,V)}\\&amp;=\sum_{i=1}^k\frac{W(C_i,V)-W(C_i,C_i)}{W(C_i,V)}\\&amp;=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{W(C_i,V)}\\&amp;=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{vol(C_i)}\\&amp;=J_{nc}(\cl{C})\end{aligned}\end{align} \]</div>
<p>In other words the normalized cut objective is related to the normalized modularity objective by the following equation:</p>
<div class="math notranslate nohighlight">
\[J_{nc}(\cl{C})=(k-1)-W(V,V)\cd J_{nQ}(\cl{C})\]</div>
<p>Since <span class="math notranslate nohighlight">\(W(V,V)\)</span> is a constant for a given graph, we observe that minimizing
normalized cut is equivalent to maximizing normalized modularity.</p>
<p><strong>Spectral Clustering Algorithm</strong></p>
<p>The matrix <span class="math notranslate nohighlight">\(\bs{\rm{B}}\)</span> is chosen to be <span class="math notranslate nohighlight">\(\A\)</span> if we are maximizing
average weight or <span class="math notranslate nohighlight">\(\bs{\rm{Q}}\)</span> for the modularity objective.
Instead of computing the <span class="math notranslate nohighlight">\(k\)</span> smallest eigenvalues we have to select the
<span class="math notranslate nohighlight">\(k\)</span> largest eigenvalues and their corresponding eigenvectors.
Because both <span class="math notranslate nohighlight">\(\A\)</span> and <span class="math notranslate nohighlight">\(\bs{\rm{Q}}\)</span> can have negative eigenvalues,
we must select only the positive eigenvalues.</p>
</div>
</div>
<div class="section" id="markov-clustering">
<h2>16.3 Markov Clustering<a class="headerlink" href="#markov-clustering" title="Permalink to this headline">¶</a></h2>
<p>If node transitions reflect the weights on the edges, then transitions from one
node to another within a cluster are much more likely than transitions between
nodes from different clusters.</p>
<p>Given the weighted adhacency matrix <span class="math notranslate nohighlight">\(\A\)</span> for a graph <span class="math notranslate nohighlight">\(G\)</span>, the
normalized adjacency matrix is given as <span class="math notranslate nohighlight">\(\M=\Delta\im\A\)</span>.
The matrix <span class="math notranslate nohighlight">\(\M\)</span> can be interpreted as the <span class="math notranslate nohighlight">\(n\times n\)</span> <em>transition</em>
<em>matrix</em> where the entry <span class="math notranslate nohighlight">\(m_{ij}=\frac{a_{ij}}{d_i}\)</span> can be interpreted as
the probability of transitioning or jumpping from node <span class="math notranslate nohighlight">\(i\)</span> to node
<span class="math notranslate nohighlight">\(j\)</span> in the graph <span class="math notranslate nohighlight">\(G\)</span>.
This is because <span class="math notranslate nohighlight">\(\M\)</span> is a <em>row stochastic</em> or <em>Markov</em> matrix. which
satisfies the following conditions: (1) elements of the matrix are non-negative,
that is, <span class="math notranslate nohighlight">\(m_{ij}\geq 0\)</span>, which follows from the fact that <span class="math notranslate nohighlight">\(\A\)</span> is
non-negative, and (2) rows of <span class="math notranslate nohighlight">\(\M\)</span> are probability vectors, that is, row
elements add to 1, because</p>
<div class="math notranslate nohighlight">
\[\sum_{j=1}^nm_{ij}=\sum_{j=1}^n\frac{\a_{ij}}{d_i}=1\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(\M\)</span> is thus the transition matrix for a <em>Markov chain</em> or a Markov random walk on graph <span class="math notranslate nohighlight">\(G\)</span>.
The Markov chain makese a transition from one node to another at discrete
timesteps <span class="math notranslate nohighlight">\(t=1,2,\cds\)</span>, with the probability of making a transition from
node <span class="math notranslate nohighlight">\(i\)</span> to node <span class="math notranslate nohighlight">\(j\)</span> given as <span class="math notranslate nohighlight">\(m_{ij}\)</span>.
Let the random variable <span class="math notranslate nohighlight">\(X_t\)</span> denote the state at time <span class="math notranslate nohighlight">\(t\)</span>.
The Markov property means that the probability distribution of <span class="math notranslate nohighlight">\(X_t\)</span> over
the states at time <span class="math notranslate nohighlight">\(t\)</span> depends only on the probability distribution of
<span class="math notranslate nohighlight">\(X_{t=1}\)</span>, that is,</p>
<div class="math notranslate nohighlight">
\[P(X_t=i|X_0,X_1,\cds,X_{t-1})=P(X_t=i|X_{t-1})\]</div>
<p>Further, we assume that the Markov chain is <em>homogeneous</em>, that is, the transition probability</p>
<div class="math notranslate nohighlight">
\[P(X_t=j|X_{t-1}=i)=m_{ij}\]</div>
<p>is independent of the time step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>The transition probability matrix for <span class="math notranslate nohighlight">\(t\)</span> time steps is given as</p>
<div class="math notranslate nohighlight">
\[\M^{t-1}\cd\M=\M^t\]</div>
<p>A random walk on <span class="math notranslate nohighlight">\(G\)</span> thus corresponds to taking successive powers of the transition matrix <span class="math notranslate nohighlight">\(\M\)</span>.
Let <span class="math notranslate nohighlight">\(\ppi_0\)</span> specify the initial state probability vector at time
<span class="math notranslate nohighlight">\(t=0\)</span>, that is, <span class="math notranslate nohighlight">\(\ppi_{0i}=P(X_0=i)\)</span> is the probability of starting
at node <span class="math notranslate nohighlight">\(i\)</span>, for all <span class="math notranslate nohighlight">\(i=1,\cds,n\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\ppi_t^T&amp;=\ppi_{t-1}^T\M\\&amp;=(\ppi_{t-2}^T\M)\cd\M=\ppi_{t-2}^T\M^2\\&amp;=(\ppi_{t-3}^T\M^2)\cd\M=\ppi_{t-3}^T\M^3\\&amp;=\vds\\&amp;=\ppi_0^T\M^t\end{aligned}\end{align} \]</div>
<p>Equivalently, taking transpose on both sides, we get</p>
<div class="math notranslate nohighlight">
\[\ppi_t=(\M^t)^T\ppi_0=(\M^T)^t\ppi_0\]</div>
<p>The state probability vector thus converges to the dominant eigenvector of
<span class="math notranslate nohighlight">\(\M^T\)</span>, reflecting the steady-state probability of reaching any node in
the graph, regardless of the staring node.</p>
<p><strong>Transition Probability Inflation</strong></p>
<p>We now consider a variation of the random walk, where the probability of
transitioning from node <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span> is inflated by taking each
element <span class="math notranslate nohighlight">\(m_{ij}\)</span> to the power <span class="math notranslate nohighlight">\(r\geq 1\)</span>.
Given a transition matrix <span class="math notranslate nohighlight">\(\M\)</span>, define the inflation operator <span class="math notranslate nohighlight">\(\Upsilon\)</span> as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\Upsilon(\M,r)=\bigg\{\frac{(m_{ij})^r}{\sum_{a=1}^n(m_{ia})^r}\bigg\}_{i,j=1}^n\)</span></p>
</div>
<p>The net effect of the inflation operator is to increase the higher probability
transitions and decrease the lower probability transitions.</p>
<div class="section" id="markov-clustering-algorithm">
<h3>16.3.1 Markov Clustering Algorithm<a class="headerlink" href="#markov-clustering-algorithm" title="Permalink to this headline">¶</a></h3>
<p>The Markov clustering algorithm (MCL) is an iterative method that interleaves matrix expansion and inflation steps.</p>
<p>The matrix difference is given in terms of the <em>Frobenius norm</em>:</p>
<div class="math notranslate nohighlight">
\[\lv\M_t-\M_{t-1}\rv_F=\sqrt{\sum_{i=1}^n\sum_{j=1}^n(\M_t(i,j)-\M_{t-1}(i,j))^2}\]</div>
<img alt="../_images/Algo16.2.png" src="../_images/Algo16.2.png" />
<p><strong>MCL Graph</strong></p>
<p>The final clusters are found by enumerating the weakly connected components in
the directed graph induced by the converged transition matrix <span class="math notranslate nohighlight">\(\M_t\)</span>.
The directed graph induced by <span class="math notranslate nohighlight">\(\M_t\)</span> is denoted as <span class="math notranslate nohighlight">\(G_t=(V_t,E_t)\)</span>.
The vertex set is the same as the set of nodes in the original graph, that is
<span class="math notranslate nohighlight">\(V_t=V\)</span>, and the edge set is given as</p>
<div class="math notranslate nohighlight">
\[E_t=\{(i,j)|\M_t(i,j)&gt;0\}\]</div>
<p>In other words, a directed edge <span class="math notranslate nohighlight">\((i,j)\)</span> exists only if node <span class="math notranslate nohighlight">\(i\)</span> can
transition to node <span class="math notranslate nohighlight">\(j\)</span> within <span class="math notranslate nohighlight">\(t\)</span> steps of the expansion and inflation process.
A node <span class="math notranslate nohighlight">\(j\)</span> is called an <em>attractor</em> if <span class="math notranslate nohighlight">\(\M_t(j,j)&gt;0\)</span>, and we say
that node <span class="math notranslate nohighlight">\(i\)</span> is attracted to attractor <span class="math notranslate nohighlight">\(j\)</span> if <span class="math notranslate nohighlight">\(\M_t(i,j)&gt;0\)</span>.
The MCL process yields a set of attractor nodes, <span class="math notranslate nohighlight">\(V_a\subseteq V\)</span>, such
that other nodes are attracted to at least one attractor in <span class="math notranslate nohighlight">\(V_a\)</span>.</p>
<p><strong>Computational Complexity</strong></p>
<p>The computational complexity of the MCL algorithm is <span class="math notranslate nohighlight">\(O(tn^3)\)</span>, where
<span class="math notranslate nohighlight">\(t\)</span> is the number of iterations until convergence.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap17.html" class="btn btn-neutral float-right" title="Chapter 17 Clustering Validation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap15.html" class="btn btn-neutral float-left" title="Chapter 15 Density-based Clustering" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>