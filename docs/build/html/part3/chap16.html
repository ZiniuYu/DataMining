

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 16 Spectral and Graph Clustering &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 17 Clustering Validation" href="chap17.html" />
    <link rel="prev" title="Chapter 15 Density-based Clustering" href="chap15.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index3.html">Part 3 Clustering</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap13.html">Chapter 13 Representative-based Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap14.html">Chapter 14 Hierarchical Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap15.html">Chapter 15 Density-based Clustering</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 16 Spectral and Graph Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#graphs-and-matrices">16.1 Graphs and Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="#clustering-as-graph-cuts">16.2 Clustering as Graph Cuts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#clustering-objective-functions-ratio-and-normalized-cut">16.2.1 Clustering Objective Functions: Ratio and Normalized Cut</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap17.html">Chapter 17 Clustering Validation</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index3.html">Part 3 Clustering</a> &raquo;</li>
        
      <li>Chapter 16 Spectral and Graph Clustering</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part3/chap16.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\ra}{\rightarrow}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{B}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}\\\newcommand{\mmu}{\boldsymbol{\mu}}\\\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-16-spectral-and-graph-clustering">
<h1>Chapter 16 Spectral and Graph Clustering<a class="headerlink" href="#chapter-16-spectral-and-graph-clustering" title="Permalink to this headline">¶</a></h1>
<div class="section" id="graphs-and-matrices">
<h2>16.1 Graphs and Matrices<a class="headerlink" href="#graphs-and-matrices" title="Permalink to this headline">¶</a></h2>
<p>Given a dataset <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points
<span class="math notranslate nohighlight">\(\x_i\in\R^d\ (i=1,2,\cds,n)\)</span>, let <span class="math notranslate nohighlight">\(\A\)</span> denote the <span class="math notranslate nohighlight">\(n\times n\)</span>
symmetric <em>similarity matrix</em> between the points, given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\A = \left(\begin{array}{cccc}a_{11}&amp;a_{12}&amp;\cds&amp;a_{1n}\\
\a_{21}&amp;a_{22}&amp;\cds&amp;a_{1n}\\\vds&amp;\vds&amp;\dds&amp;\vds\\
\a_{n1}&amp;a_{n2}&amp;\cds&amp;a_{nn}\end{array}\right)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\A(i,j)=a_{ij}\)</span> denotes the similarity or affinity between points <span class="math notranslate nohighlight">\(\x_i\)</span> and <span class="math notranslate nohighlight">\(\x_j\)</span>.
We require the similarity to be symmetric and non-negative, that is, <span class="math notranslate nohighlight">\(a_{ij}=a_{ji}\)</span> and <span class="math notranslate nohighlight">\(a_{ji}\geq 0\)</span>.
The matrix <span class="math notranslate nohighlight">\(\A\)</span> may be considered to be a <em>weighted adjacency matrix</em> of
the weighted (undirected) graph <span class="math notranslate nohighlight">\(G=(V,E)\)</span>, where each vertex is a point
and each edge joins a pair of points, that is,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}V&amp;=\{\x_i|i=1,\cds,n\}\\E&amp;=\{(\x_i,\x_j)|1\leq i,j\leq n\}\end{aligned}\end{align} \]</div>
<p>Further, the similarity matrix <span class="math notranslate nohighlight">\(\A\)</span> gives the weight on each edge, that
is, <span class="math notranslate nohighlight">\(a_{ij}\)</span> denotes the weight of the edge <span class="math notranslate nohighlight">\((\x_i,\x_j)\)</span>.
If all affinities are 0 or 1, then <span class="math notranslate nohighlight">\(\A\)</span> represents the regular adjacency relationship between the vertices.</p>
<p>For a vertex <span class="math notranslate nohighlight">\(\x_i\)</span>, let <span class="math notranslate nohighlight">\(d_j\)</span> denote the <em>degree</em> of the vertex, defined as</p>
<div class="math notranslate nohighlight">
\[d_i=\sum_{j=1}^n a_{ij}\]</div>
<p>We define the <em>degree matrix</em> <span class="math notranslate nohighlight">\(\Delta\)</span> of graph <span class="math notranslate nohighlight">\(G\)</span> as the <span class="math notranslate nohighlight">\(n\times n\)</span> diagonal matrix:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\Delta=\left(\begin{array}{cccc}d_1&amp;0&amp;\cds&amp;0\\0&amp;d_2&amp;\cds&amp;0\\\vds&amp;\vds&amp;\dds&amp;\vds\\0&amp;0&amp;\cds&amp;d_n\end{array}\right)\)</span>
<span class="math notranslate nohighlight">\(=\left(\begin{array}{cccc}\sum_{j=1}^na_{1j}&amp;0&amp;\cds&amp;0\\0&amp;\sum_{j=1}^na_{2j}&amp;\cds&amp;0\\\vds&amp;\vds&amp;\dds&amp;\vds\\0&amp;0&amp;\cds&amp;\sum_{j=1}^na_{nj}\end{array}\right)\)</span></p>
</div>
<p><span class="math notranslate nohighlight">\(\Delta\)</span> can be compactly written as <span class="math notranslate nohighlight">\(\Delta(i,i)=d_i\)</span> for all <span class="math notranslate nohighlight">\(1\leq i\leq n\)</span>.</p>
<p><strong>Normalized Adjacency Matrix</strong></p>
<p>The normalized adjacency matrix is obtained by dividing each row of the
adjacency matrix by the degree of the corresponding node.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{M}}=\Delta\im\A=\)</span>
<span class="math notranslate nohighlight">\(\left(\begin{array}{cccc}\frac{a_{11}}{d_1}&amp;\frac{a_{12}}{d_1}&amp;\cds&amp;\frac{a_{1n}}{d_1}\\\frac{a_{21}}{d_2}&amp;\frac{a_{22}}{d_2}&amp;\cds&amp;\frac{a_{2n}}{d_2}\\\vds&amp;\vds&amp;\dds&amp;\vds\\\frac{a_{n1}}{d_n}&amp;\frac{a_{n2}}{d_n}&amp;\cds&amp;\frac{a_{nn}}{d_n}\end{array}\right)\)</span></p>
</div>
<p>Each element of <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>, namely <span class="math notranslate nohighlight">\(m_{ij}\)</span> is also non-negative,
as <span class="math notranslate nohighlight">\(m_{ij}=\frac{a_{ij}}{d_i}\geq 0\)</span>.
Consider the sum of the <span class="math notranslate nohighlight">\(i\)</span>th row in <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\sum_{j=1}^nm_{ij}=\sum_{j=1}^n\frac{a_{ij}}{d_i}=\frac{d_i}{d_i}=1\]</div>
<p>Thus, each row in <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span> sums to 1.
This implies that 1 is an eigenvalue of <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>.
In fact, <span class="math notranslate nohighlight">\(\ld=1\)</span> is the largest eigenvalue of <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span>, and the
other eigenvalues satisfy the property that <span class="math notranslate nohighlight">\(|\ld_i|\leq 1\)</span>.
Also, if <span class="math notranslate nohighlight">\(G\)</span> is connected then the eigenvector corresponding to
<span class="math notranslate nohighlight">\(\ld_1\)</span> is
<span class="math notranslate nohighlight">\(\u_1=\frac{1}{\sqrt{n}}=(1,1,\cds,1)^T=\frac{1}{\sqrt{n}}\1\)</span>.
Because <span class="math notranslate nohighlight">\(\bs{\rm{M}}\)</span> is not symmetric, its eigenvectors are not necessarily orthogonal.</p>
<p><strong>Graph Laplacian Matrices</strong></p>
<p>The <em>Laplacian matrix</em> of a graph is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bs{\rm{L}}=\Delta-\A=\left(\begin{array}{cccc}\sum_{j=1}^na_{1j}&amp;0&amp;\cds&amp;0\\
0&amp;\sum_{j=1}^na_{2j}&amp;\cds&amp;0\\\vds&amp;\vds&amp;\dds&amp;\vds\\
0&amp;0&amp;\cds&amp;\sum_{j=1}^na_{nj}\end{array}\right)
-\left(\begin{array}{cccc}a_{11}&amp;a_{12}&amp;\cds&amp;a_{1n}\\
\a_{21}&amp;a_{22}&amp;\cds&amp;a_{1n}\\\vds&amp;\vds&amp;\dds&amp;\vds\\
\a_{n1}&amp;a_{n2}&amp;\cds&amp;a_{nn}\end{array}\right)\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(=\left(\begin{array}{cccc}\sum_{j\ne 1}^na_{1j}&amp;-a_{12}&amp;\cds&amp;-a_{1n}\\-a{21}&amp;\sum_{j\ne 2}^na_{2j}&amp;\cds&amp;-a_{2n}\\\vds&amp;\vds&amp;\dds&amp;\vds\\-a_{n1}&amp;-a_{n2}&amp;\cds&amp;\sum_{j\ne n}^na_{nj}\end{array}\right)\)</span></p>
</div>
<p><span class="math notranslate nohighlight">\(\bs{\rm{L}}\)</span> is a symmetric, positive semidefinite matrix, as for any <span class="math notranslate nohighlight">\(\c\in\R^n\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\c^T\bs{\rm{L}}\c&amp;=\c^T(\Delta-\A)\c=\c^T\Delta\c-\c^T\A\c\\&amp;=\sum_{i=1}^nd_ic_i^2-\sum_{i=1}^n\sum_{j=1}^nc_ic_ja_{ij}\\&amp;=\frac{1}{2}\bigg(\sum_{i=1}^nd_ic_i^2-2\sum_{i=1}^n\sum_{j=1}^nc_ic_ja_{ij}+\sum_{j=1}^nd_jc_j^2\bigg)\\&amp;=\frac{1}{2}\bigg(\sum_{i=1}^n\sum_{j=1}^na_{ij}c_i^2-2\sum_{i=1}^n
\sum_{j=1}^nc_ic_ja_{ij}+\sum_{i=j}^n\sum_{i=1}^na_{ij}c_j^2\bigg)\\&amp;=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^na_{ij}(c_i-c_j)^2\\&amp;\geq 0\end{aligned}\end{align} \]</div>
<p>This means that <span class="math notranslate nohighlight">\(\bs{\rm{L}}\)</span> has <span class="math notranslate nohighlight">\(n\)</span> real, non-negative
eigenvalues, which can be arranged in decreasing order as follows:
<span class="math notranslate nohighlight">\(\ld_1\geq\ld_2\geq\cds\geq\ld_n\geq 0\)</span>.
Because <span class="math notranslate nohighlight">\(\bs{\rm{L}}\)</span> is symmetric, its eigenvectors are orthonormal.
We can observe that the first column (and the first row) is a linear combination of the remaining columns (rows).
This implies that the rank of <span class="math notranslate nohighlight">\(\bs{\rm{L}}\)</span> is at most <span class="math notranslate nohighlight">\(n-1\)</span>, and
the smallest eigenvalue is <span class="math notranslate nohighlight">\(\ld_n=0\)</span>, with the corresponding eigenvector
given as <span class="math notranslate nohighlight">\(\u_n=\frac{1}{\sqrt{n}}=(1,1,\cds,1)^T=\frac{1}{\sqrt{n}}\1\)</span>,
provided the graph is connected.
If the graph is disconnected, then the number of eigenvalues equal to zero
specifies the number of connected components in the graph.</p>
<p>The <em>normalized symmetric Laplacian matrix</em> of a graph is defined as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\bs{\rm{L}}^S&amp;=\Delta^{-1/2}\bs{\rm{L}}\Delta^{-1/2}\\&amp;=\bs{\rm{L}}^{-1/2}(\Delta-\A)\Delta^{-1/2}=\Delta^{-1/2}\Delta\Delta^{-1/2}-\Delta^{-1/2}\A\Delta^{-1/2}\\&amp;=\I-\Delta^{-1/2}\A\Delta^{-1/2}\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{L}}^S=\Delta^{-1/2}\bs{\rm{L}}\Delta^{-1/2}\)</span>
<span class="math notranslate nohighlight">\(=\left(\begin{array}{cccc} \frac{\sum_{j\ne 1}a_{1j}}{\sqrt{d_1d_1}}&amp;-\frac{a_{12}}{\sqrt{d_1d_2}}&amp;\cds&amp;-\frac{a_{1n}}{\sqrt{d_1d_n}}\\-\frac{a_{21}}{\sqrt{d_2d_1}}&amp;\frac{\sum_{j\ne 2}a_{2j}}{\sqrt{d_2d_2}}&amp;\cds&amp;-\frac{a_{2n}}{\sqrt{d_2d_n}}\\\vds&amp;\vds&amp;\dds&amp;\vds\\-\frac{a_{n1}}{\sqrt{d_nd_1}}&amp;-\frac{a_{n2}}{\sqrt{d_nd_2}}&amp;\cds&amp;\frac{\sum_{j\ne n}a_{nj}}{\sqrt{d_nd_n}}\end{array}\right)\)</span></p>
</div>
<p>We can hsow that <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span> is also positive semidefinite because for any <span class="math notranslate nohighlight">\(\c\in\R^d\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[\c^T\bs{\rm{L}}^s\c=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^na_{ij}
\bigg(\frac{c_i}{\sqrt{d_i}}-\frac{c_j}{\sqrt{d_j}}\bigg)^2\geq 0\]</div>
<p>The first column is also a linear combination of the other columns, which means
that <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span> has rank at most <span class="math notranslate nohighlight">\(n-1\)</span>, with the smallest
eigenvalue <span class="math notranslate nohighlight">\(\ld_n=0\)</span>, and the corresponding eigenvector
<span class="math notranslate nohighlight">\(\frac{1}{\sqrt{\sum_id_i}}(\sqrt{d_1},\sqrt{d_2},\cds,\sqrt{d_n})^T=\frac{1}{\sqrt{\sum_id_i}}\Delta^{1/2}\1\)</span>.
Combined with the fact that <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span> is positive semidefinite, we
conclude that <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span> has <span class="math notranslate nohighlight">\(n\)</span> (not necessarily distinct)
real, positive eigenvalues <span class="math notranslate nohighlight">\(\ld_1\geq\ld_2\geq\cds\geq\ld_n=0\)</span>.</p>
<p>The <em>normalized asymmetric Laplacian</em> matrix is defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{L}}^a=\Delta\im\bs{\rm{L}}=\Delta\im(\Delta-\A)=\I-\Delta\im\A\)</span>
<span class="math notranslate nohighlight">\(=\left(\begin{array}{cccc}\frac{\sum_{j\ne 1}a_{1j}}{d_1}&amp;-\frac{a_{12}}{d_1}&amp;\cds&amp;-\frac{a_{1n}}{d_1}\\-\frac{a_{21}}{d_2}&amp;\frac{\sum_{j\ne 2}a_{2j}}{d_2}&amp;\cds&amp;-\frac{a_{2n}}{d_2}\\\vds&amp;\vds&amp;\dds&amp;\vds\\-\frac{a_{n1}}{d_n}&amp;-\frac{a_{n2}}{d_n}&amp;\cds&amp;\frac{\sum_{j\ne n}a_{nj}}{d_n}\end{array}\right)\)</span></p>
</div>
<p>Consider the eigenvalue equation for the symmetric Laplacian <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\bs{\rm{L}}^S\u&amp;=\ld\u\\\Delta^{-1/2}\bs{\rm{L}}^S\u&amp;=\ld\Delta^{-1/2}\u\\\Delta^{-1/2}(\Delta^{-1/2}\bs{\rm{L}}\Delta^{-1/2})\u&amp;=\ld\Delta^{-1/2}\u\\\Delta\im\bs{\rm{L}}(\Delta^{-1/2}\u)&amp;=\ld(\Delta^{-1/2}\u)\\\bs{\rm{L}}^a\v=\ld\v\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\v=\Delta^{-1/2}\u\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\bs{\rm{L}}^a\)</span>, and
<span class="math notranslate nohighlight">\(\u\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\bs{\rm{L}}^S\)</span>.</p>
</div>
<div class="section" id="clustering-as-graph-cuts">
<h2>16.2 Clustering as Graph Cuts<a class="headerlink" href="#clustering-as-graph-cuts" title="Permalink to this headline">¶</a></h2>
<p>A <em>k-way cut</em> in a graph is a partitioning or clustering of the vertex set,
given as <span class="math notranslate nohighlight">\(\cl{C}=\{C_1,\cds,C_k\}\)</span>, such that <span class="math notranslate nohighlight">\(C_i\ne\emptyset\)</span> for
all <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(C_i\cap C_j=\emptyset\)</span> for all <span class="math notranslate nohighlight">\(i, j\)</span>, and
<span class="math notranslate nohighlight">\(V=\bigcup_ic_i\)</span>.
We require <span class="math notranslate nohighlight">\(\cl{C}\)</span> to optimize some objective function that cptures the
intuition that nodes within a cluster should have high similarity, and nodes
from different clusters should have low similarity.</p>
<p>Given a weighted graph <span class="math notranslate nohighlight">\(G\)</span> defined by its similarity matrix, let
<span class="math notranslate nohighlight">\(S, T\subseteq V\)</span> be any two subsets of the vertices.
We denote by <span class="math notranslate nohighlight">\(W(S,T)\)</span> the sum of the weights on all edges with one vertex
in <span class="math notranslate nohighlight">\(S\)</span> and the other in <span class="math notranslate nohighlight">\(T\)</span>, given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp W(S,T)=\sum_{v_i\in S}\sum_{v_j\in T}a_{ij}\)</span></p>
</div>
<p>Given <span class="math notranslate nohighlight">\(S\subseteq V\)</span>, we denote by <span class="math notranslate nohighlight">\(\bar{S}\)</span> the complementary set
of vertices, that is, <span class="math notranslate nohighlight">\(\bar{S}=V-S\)</span>.
A <em>（vertex) cut</em> in a graph is defined as a partitioning of <span class="math notranslate nohighlight">\(V\)</span> into <span class="math notranslate nohighlight">\(S\subset V\)</span> and <span class="math notranslate nohighlight">\(\bar{S}\)</span>.
The <em>weight of the cut</em> or <em>cut weight</em> is defined as the sum of all the weights
on edges between vertices in <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(\bar{S}\)</span>, given as
<span class="math notranslate nohighlight">\(W(S,\bar{S})\)</span>.</p>
<p>Given a clustering <span class="math notranslate nohighlight">\(\cl{C}=\{C_1,\cds,C_k\}\)</span> comprising <span class="math notranslate nohighlight">\(k\)</span>
clusters, the <em>size</em> of a cluster <span class="math notranslate nohighlight">\(C_i\)</span> is the number of nodes in the
cluster, given as <span class="math notranslate nohighlight">\(|C_i|\)</span>.
The <em>volume</em> of a cluster <span class="math notranslate nohighlight">\(C_i\)</span> is defined as the sum of all the weights
on edges with one end in cluster <span class="math notranslate nohighlight">\(C_i\)</span>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp vol(C_i)=\sum_{v_j\in C_i}d_j=\sum_{v_j\in C_i}\sum_{v_r\in V}a_{jr}=W(C_i,V)\)</span></p>
</div>
<p>Let <span class="math notranslate nohighlight">\(\c_i=\{0,1\}^n\)</span> be the <em>cluster indicator vector</em> that records the
cluster membership for cluster <span class="math notranslate nohighlight">\(C_i\)</span>, defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}c_{ij}=\left\{\begin{array}{lr}1\quad\rm{if\ }v_j\in C_i\\0\quad\rm{if\ }v_j\notin C_i\end{array}\right.\end{split}\]</div>
<p>Because a clustering creates pairwise disjoint clusters, we immediately have</p>
<div class="math notranslate nohighlight">
\[\c_i^T\c_j=0\]</div>
<p>The cluster size can be written as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(|C_i|=\c_i^T\c_i=\lv\c_i\rv^2\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[vol(C_i)=W(C_i,V)=\sum_{v_r\in C_i}d_r=\sum_{v_r\in C_i}c_{ir}d_rc_{ir}=
\sum_{r=1}^n\sum_{s=1}^nc_{ir}\Delta_{rs}c_{is}\]</div>
<p>The volume of the cluster can be written as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(vol(C_i)=\c_i^T\Delta\c_i\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[W(C_i,C_i)=\sum_{v_r\in C_i}\sum_{v_s\in C_i}a_{rs}=\sum_{r=1}^n\sum_{s=1}^nc_{ir}a_{rs}c_{is}\]</div>
<p>The sum of internal weights can be written as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(W(C_i,C_i)=\c_i^T\A\c_i\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp W(C_i,\bar{C_i})=\sum_{v_r\in C_i}\sum_{v_s\in V-C_i}a_{rs}=W(C_i,V)-W(C_i,C_i)\)</span>
<span class="math notranslate nohighlight">\(=\c_i(\Delta-\A)\c_i=\c_i^T\bs{\rm{L}}\c_i\)</span></p>
</div>
<div class="section" id="clustering-objective-functions-ratio-and-normalized-cut">
<h3>16.2.1 Clustering Objective Functions: Ratio and Normalized Cut<a class="headerlink" href="#clustering-objective-functions-ratio-and-normalized-cut" title="Permalink to this headline">¶</a></h3>
<p><strong>Ratio Cut</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_{\cl{C}}J_{rc}(\cl{C})=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{|C_i|}\)</span>
<span class="math notranslate nohighlight">\(=\dp\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\c_i}\)</span>
<span class="math notranslate nohighlight">\(=\dp\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\lv\c_i\rv^2}\)</span></p>
</div>
<p>ratio cut tries to minimize the sum of the similarities from a cluster
<span class="math notranslate nohighlight">\(C_i\)</span> to other points not in the cluster <span class="math notranslate nohighlight">\(\bar{C_i}\)</span>, taking into
account the size of each cluster.</p>
<p>For binary cluster indicator vectors <span class="math notranslate nohighlight">\(\c_i\)</span>, the ratio cut objective is NP-hard.
An obvious relaxation is to allow <span class="math notranslate nohighlight">\(\c_i\)</span> to take on any real value.</p>
<div class="math notranslate nohighlight">
\[\min_{\cl{C}}J_{rc}(\cl{C})=
\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\lv\c_i\rv^2}=
\sum_{i=1}^k\bigg(\frac{\c_i}{\lv\c_i\rv}\bigg)^T\bs{\rm{L}}
\bigg(\frac{\c_i}{\lv\c_i\rv}\bigg)=\sum_{i=1}^k\u_i^T\bs{\rm{L}}\u_i\]</div>
<p>where <span class="math notranslate nohighlight">\(\u_i=\frac{\c_i}{\lv\c_i\rv}\)</span> is the unit vector in the direction of <span class="math notranslate nohighlight">\(\c_i\in\R^n\)</span>.</p>
<p>To incorporate the constraint the <span class="math notranslate nohighlight">\(\u_i^T\u_i=1\)</span>, we introduce the
Lagrange multiplier <span class="math notranslate nohighlight">\(\ld_i\)</span> for each cluster <span class="math notranslate nohighlight">\(C_i\)</span>.
We have</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\u_i}\bigg(\sum_{i=1}^k\u_i^T\bs{\rm{L}}\u_i+\sum_{i=1}^n\ld_i(1-\u_i^T\u_i)\bigg)&amp;=\0\\2\bs{\rm{L}}\u_i-2\ld_i\u_i&amp;=\0\\\bs{\rm{L}}\u_i&amp;=\ld_i\u_i\\\u_i^T\bs{\rm{L}}\u_i&amp;=\u_i^T\ld_i\u_i=\ld_i\end{aligned}\end{align} \]</div>
<p>which in turn implies that to minimize the ratio cut objective, we should choose
the <span class="math notranslate nohighlight">\(k\)</span> smallest eigenvalues, and the corresponding eigenvectors, so that</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_{\cl{C}}J_{rc}(\cl{C})&amp;=\u_n^T\bs{\rm{L}}\u_n+\cds+\u_{n-k+1}^T\bs{\rm{L}}\u_{n-k+1}\\&amp;=\ld_n+\cds+\ld_{n-k+1}\end{aligned}\end{align} \]</div>
<p><strong>Normalized Cut</strong></p>
<p><em>Normalized cut</em> is similar to ratio cut, except that it divides the cut weight
of each cluster by the volume of a cluster instead of its size.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\min_{\cl{C}}J_{nc}(\cl{C})=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{vol(C_i)}\)</span>
<span class="math notranslate nohighlight">\(=\dp\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\Delta\c_i}\)</span></p>
</div>
<p>We assume <span class="math notranslate nohighlight">\(\c_i\)</span> to be an arbitrary real vector, and rewrite the
normalized cut objective in terms of the normalized symmetrc Laplacian, as
follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_{\cl{C}}J_{nc}(\cl{C})
&amp;=\sum_{i=1}^k\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\Delta\c_i}
=\sum_{i=1}^k\frac{\c_i^T(\Delta^{1/2}\Delta^{-1/2})\bs{\rm{L}}
(\Delta^{-1/2}\Delta^{1/2})\c_i}{\c_i^T(\Delta^{1/2}\Delta^{1/2})\c_i}\\&amp;=\sum_{i=1}^k\frac{(\Delta^{1/2}\c_i)^T(\Delta^{-1/2}\bs{\rm{L}}
\Delta^{-1/2})(\Delta^{1/2}\c_i)}{(\Delta^{1/2}\c_i)^T(\Delta^{1/2}\c_i)}\\&amp;=\sum_{i=1}^k\bigg(\frac{\Delta^{1/2}\c_i}{\lv\Delta^{1/2}\c_i\rv}\bigg)^T
\bs{\rm{L}}^S\bigg(\frac{\Delta^{1/2}\c_i}{\lv\Delta^{1/2}\c_i\rv}\bigg)
=\sum_{i=1}^k\u_i^T\bs{\rm{L}}^S\u_i\end{aligned}\end{align} \]</div>
<p>The normalized cut objective can also be expressed in terms of the normalized asymmetric Laplacian:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\pd}{\pd\c_i}\bigg(\sum_{j=1}^k\frac{\c_j^T\bs{\rm{L}}\c_j}
{\c_j^T\Delta\c_j}\bigg)=\frac{\pd}{\pd\c_i}
\bigg(\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\Delta\c_i}\bigg)&amp;=\0\\\frac{\bs{\rm{L}}\c_i(\c_i^T\Delta\c_i)-\Delta\c_i(\c_i^T\bs{\rm{L}}\c_i)}
{(\c_i^T\Delta\c_i)^2}&amp;=0\\\bs{\rm{L}}\c_i&amp;=\bigg(\frac{\c_i^T\bs{\rm{L}}\c_i}{\c_i^T\Delta\c_i}\bigg)\Delta\c_i\\\Delta\im\bs{\rm{L}}\c_i&amp;=\ld_i\c_i\\\bs{\rm{L}}^a\c_i&amp;=\ld_i\c_i\end{aligned}\end{align} \]</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap17.html" class="btn btn-neutral float-right" title="Chapter 17 Clustering Validation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap15.html" class="btn btn-neutral float-left" title="Chapter 15 Density-based Clustering" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>