

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 13 Representative-based Clustering &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 14 Hierarchical Clustering" href="chap14.html" />
    <link rel="prev" title="Part 3 Clustering" href="index3.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index3.html">Part 3 Clustering</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 13 Representative-based Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-means-algorithm">13.1 K-Means Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-k-means">13.2 Kernel K-Means</a></li>
<li class="toctree-l3"><a class="reference internal" href="#expectation-maximization-clustering">13.3 Expectation-Maximization Clustering</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#em-in-one-dimension">13.3.1 EM in One Dimension</a></li>
<li class="toctree-l4"><a class="reference internal" href="#em-in-d-dimensions">13.3.2 EM in <span class="math notranslate nohighlight">\(d\)</span> Dimensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#maximum-likelihood-estimation">13.3.3 Maximum Likelihood Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#em-approach">13.3.4 EM Approach</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap14.html">Chapter 14 Hierarchical Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap15.html">Chapter 15 Density-based Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap16.html">Chapter 16 Spectral and Graph Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap17.html">Chapter 17 Clustering Validation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part5/index5.html">Part 5 Regression</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index3.html">Part 3 Clustering</a> &raquo;</li>
        
      <li>Chapter 13 Representative-based Clustering</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part3/chap13.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-13-representative-based-clustering">
<h1>Chapter 13 Representative-based Clustering<a class="headerlink" href="#chapter-13-representative-based-clustering" title="Permalink to this headline">Â¶</a></h1>
<p>Given a dataset <span class="math notranslate nohighlight">\(\D\)</span> with <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\x_i\)</span> in a
<span class="math notranslate nohighlight">\(d\)</span>-dimensional space, and given the number of desired clusters <span class="math notranslate nohighlight">\(k\)</span>,
the goal of representative-based clustering is to partition the dataset into
<span class="math notranslate nohighlight">\(k\)</span> groups or clusters, which is called a <em>clustering</em> and is denoted as
<span class="math notranslate nohighlight">\(\cl{C}=\{C_1,C_2,\cds,C_k\}\)</span>.
Further, for each cluster <span class="math notranslate nohighlight">\(C_i\)</span> there exists a representative point that
summarizes the cluster, a common choice being the mean (also called the
<em>centroid</em>) <span class="math notranslate nohighlight">\(\mmu_i\)</span> of all points in the cluster, that is,</p>
<div class="math notranslate nohighlight">
\[\mmu_i=\frac{1}{n_i}\sum_{x_j\in C_i}\x_j\]</div>
<p>where <span class="math notranslate nohighlight">\(n_i=|C_i|\)</span> is the number of points in cluster <span class="math notranslate nohighlight">\(C_i\)</span>.</p>
<p>The <em>exact</em> number of ways of partitioning <span class="math notranslate nohighlight">\(n\)</span> points into <span class="math notranslate nohighlight">\(k\)</span>
nonempty and disjoint parts is given by the <em>Stirling numbers of second kind</em>,
given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}S(n,k)=\frac{1}{k!}\sum_{t=0}^k(-1)^t\bp k\\t \ep(k-t)^n\end{split}\]</div>
<div class="section" id="k-means-algorithm">
<h2>13.1 K-Means Algorithm<a class="headerlink" href="#k-means-algorithm" title="Permalink to this headline">Â¶</a></h2>
<p>Given a clustering <span class="math notranslate nohighlight">\(\cl{C}=\{C_1,C_2,\cds,C_k\}\)</span> we need some scoring
function that evaluates its quality or goodness.
This <em>sum of squared errors</em> scoring function is defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(SSE(\cl{c})=\sum_{i=1}^k\sum_{\x_j\in C_i}\lv\x_j-\mmu_i\rv^2\)</span></p>
</div>
<p>The goal is to find the clustering that minimizes the SSE scores:</p>
<div class="math notranslate nohighlight">
\[\cl{C}^*=\arg\min_{\cl{C}}\{SSE(\cl{C})\}\]</div>
<p>K-means employs a greedy iterative approach to find a clustering that minimizes the SSE objective.</p>
<img alt="../_images/Algo13.1.png" src="../_images/Algo13.1.png" />
<p>The cluster assignment step take <span class="math notranslate nohighlight">\(O(nkd)\)</span> time because for each of the
<span class="math notranslate nohighlight">\(n\)</span> points we have to compute its distance to each of the <span class="math notranslate nohighlight">\(k\)</span>
clusters, which takes <span class="math notranslate nohighlight">\(d\)</span> operations in <span class="math notranslate nohighlight">\(d\)</span> dimensions.
The centroid re-computation step takes <span class="math notranslate nohighlight">\(O(nd)\)</span> time because we have to
add at total of <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(d\)</span>-dimensional points.
Assuming that there are <span class="math notranslate nohighlight">\(t\)</span> iterations, the total time for K-means is given as <span class="math notranslate nohighlight">\(O(tnkd)\)</span>.
In terms of the I/O cost it requires <span class="math notranslate nohighlight">\(O(t)\)</span> full database scans, because
we have to read the entire database in each iteration.</p>
</div>
<div class="section" id="kernel-k-means">
<h2>13.2 Kernel K-Means<a class="headerlink" href="#kernel-k-means" title="Permalink to this headline">Â¶</a></h2>
<p>Assume for the moment that all points <span class="math notranslate nohighlight">\(\x_i\in\D\)</span> have been mapped to
their corresponding images <span class="math notranslate nohighlight">\(\phi(\x_i)\)</span> in feature space.
Let <span class="math notranslate nohighlight">\(\K=\{K(\x_i,\x_j)\}_{i,j=1,\cds,n}\)</span> denote the <span class="math notranslate nohighlight">\(n\times n\)</span>
matrix, where <span class="math notranslate nohighlight">\(K(\x_i,\x_j)=\phi(\x_i)^T\phi(\x_j)\)</span>.
Let <span class="math notranslate nohighlight">\(\{C_1,\cds,C_k\}\)</span> specify the partitioning of the <span class="math notranslate nohighlight">\(n\)</span> points
into <span class="math notranslate nohighlight">\(k\)</span> clusters, and let the corresponding cluster means in feature
space be given as <span class="math notranslate nohighlight">\(\{\mmu_1^\phi,\cds,\mmu_k^\phi\}\)</span>, where</p>
<div class="math notranslate nohighlight">
\[\mmu_i^\phi=\frac{1}{n_i}\sum_{\x_j\in C_i}\phi(\x_j)\]</div>
<p>is the mean of cluster <span class="math notranslate nohighlight">\(C_i\)</span> in feature space, with <span class="math notranslate nohighlight">\(n_i=|C_i|\)</span>.</p>
<p>In feature space, the kernel K-means sum of squared errors objective can be written as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\min_{\cl{C}}SSE(\cl{C})&amp;=\sum_{i=1}^k\sum_{\x_j\in C_i}\lv\phi(\x_j)-\mmu_i^\phi\rv^2\\&amp;=\sum_{i=1}^k\sum_{\x_j\in C_i}\lv\phi(\x_j)\rv^2-2\phi(\x_j)^T\mmu_i^\phi+\lv\mmu_i\rv^2\\&amp;=\sum_{i=1}^k\bigg(\bigg(\sum_{\x_j\in C_i}\lv\phi(\x_j)\rv^2\bigg)-2n_i
\bigg(\frac{1}{n_i}\sum_{\x_j\in C_i}\phi(\x_j)\bigg)^T\mmu_i^\phi+
n_i\lv\mmu_i^\phi\rv^2\bigg)\\&amp;=\bigg(\sum_{i=1}^k\sum_{\x_j\in C_i}\phi(\x_j)^T\phi(\x_j)\bigg)-\bigg(\sum_{i=1}^k n_i\lv\mmu_i^\phi\rv^2\bigg)\\&amp;=\sum_{i=1}^k\sum_{\x_j\in C_i}K(\x_j,\x_j)-\sum_{i=1}^k\frac{1}{n_i}
\sum_{\x_a\in C_i}\sum_{\x_b\in C_i}K(\x_a,\x_b)\\&amp;=\sum_{j=1}^nK(\x_j,\x_j)-\sum_{i=1}^k\frac{1}{n_i}\sum_{\x_a\in C_i}\sum_{\x_b\in C_i}K(\x_a,\x_b)\end{aligned}\end{align} \]</div>
<p>Consider the distance of a point <span class="math notranslate nohighlight">\(\phi(\x_j)\)</span> to the mean
<span class="math notranslate nohighlight">\(\mmu_i^\phi\)</span> in feature space, which can be computed as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\lv\phi(\x_j)-\mmu_i^\phi\rv^2&amp;=\lv\phi(\x_j)\rv^2-2\phi(\x_j)^T\mmu_i^\phi+\lv\mmu_i^\phi\rv^2\\&amp;=\phi(\x_j)^T\phi(\x_j)-\frac{2}{n_i}\sum_{\x_a\in C_i}\phi(\x_j)^T
\phi(\x_a)+\frac{1}{n^2}\sum_{\x_a\in C_i}\sum_{\x_b\in C_i}
\phi(\x_a)^T\phi(\x_b)\\&amp;=K(\x_j,\x_j)-\frac{2}{n_i}\sum_{\x_a\in C_i}K(\x_a,\x_j)+\frac{1}{n_i^2}
\sum_{\x_a\in C_i}\sum_{\x_b\in C_i}K(\x_a,\x_b)\end{aligned}\end{align} \]</div>
<p>In the cluster assignment step of kernel K-means, we assign a point to the closest cluster mean as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}C^*(\x_j)&amp;=\arg\min_i\{\lv\phi(\x_j)-\mmu_i^\phi\rv^2\}\\&amp;=\arg\min_i\bigg\{K(\x_j,\x_j)-\frac{2}{n_i}\sum_{\x_a\in C_i}K(\x_a,\x_j)+
\frac{1}{n^2}\sum_{\x_a\in C_i}\sum_{\x_b\in C_i}K(\x_a,\x_b)\bigg\}\\&amp;=\arg\min_i\bigg\{\frac{1}{n_i^2}\sum_{\x_a\in C_i}\sum_{\x_b\in C_i}
K(\x_a,\x_b)-\frac{2}{n_i}\sum_{\x_a\in C_i}K(\x_a,\x_j)\bigg\}\end{aligned}\end{align} \]</div>
<img alt="../_images/Algo13.2.png" src="../_images/Algo13.2.png" />
<p>The fraction of points reassigned to a different cluster in the current iteration is given as</p>
<div class="math notranslate nohighlight">
\[\frac{n-\sum_{i=1}^k|C_i^T\cap C_i^{t-1}|}{n}=1-\frac{1}{n}\sum_{i=1}^k|C_i^T\cap C_i^{t-1}|\]</div>
<p><strong>Computational Complexity</strong></p>
<p>The total computational complexity of kernel K-means is <span class="math notranslate nohighlight">\(O(tn^2)\)</span>, where
<span class="math notranslate nohighlight">\(t\)</span> is the number of iterations until convergence.
The I/O complexity is <span class="math notranslate nohighlight">\(O(t)\)</span> scans of the kernel matrix <span class="math notranslate nohighlight">\(\K\)</span>.</p>
</div>
<div class="section" id="expectation-maximization-clustering">
<h2>13.3 Expectation-Maximization Clustering<a class="headerlink" href="#expectation-maximization-clustering" title="Permalink to this headline">Â¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\D\)</span> consist of <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\x_j\)</span> in <span class="math notranslate nohighlight">\(d\)</span>-dimensional space <span class="math notranslate nohighlight">\(\R^d\)</span>.
Let <span class="math notranslate nohighlight">\(X_a\)</span> denote the random variable corresponding to the <span class="math notranslate nohighlight">\(a\)</span>th attribute.
Let <span class="math notranslate nohighlight">\(\X=(X_1,X_2,\cds,X_d)\)</span> denote the vector random variable across the <span class="math notranslate nohighlight">\(d\)</span>-attributes, with <span class="math notranslate nohighlight">\(\x_j\)</span>
being a data sample from <span class="math notranslate nohighlight">\(\X\)</span>.</p>
<p><strong>Gaussian Mixture Model</strong></p>
<p>Assume that each cluster <span class="math notranslate nohighlight">\(C_i\)</span> is characterized by a multivariate normal distribution, that is,</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp f_i(\x)=f(\x|\mmu_i,\Sg_i)=\frac{1}{(2\pi)^{\frac{d}{2}}|\Sg_i|^{\frac{1}{2}}}\)</span>
<span class="math notranslate nohighlight">\(\dp\exp\bigg\{-\frac{(\x-\mmu_i)^T\Sg_i\im(\x-\mmu_i)}{2}\bigg\}\)</span></p>
</div>
<p>where the cluster mean <span class="math notranslate nohighlight">\(\mmu_i\in\R^d\)</span> and covariance matrix
<span class="math notranslate nohighlight">\(\Sg_i\in\R^{d\times d}\)</span> are both unknown parameters.
<span class="math notranslate nohighlight">\(f_i(\x)\)</span> is the probability density at <span class="math notranslate nohighlight">\(\x\)</span> attributable to cluster <span class="math notranslate nohighlight">\(C_i\)</span>.
We assume that the probability density function of <span class="math notranslate nohighlight">\(\X\)</span> is given as a
<em>Gaussian mixture model</em> over all the <span class="math notranslate nohighlight">\(k\)</span> cluster normals, defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp f(\x)=\sum_{i=1}^kf_i(\x)P(C_i)=\sum_{i=1}^kf(\x|\mmu_i,\Sg_i)P(C_i)\)</span></p>
</div>
<p>where the prior probabilities <span class="math notranslate nohighlight">\(P(C_i)\)</span> are called the <em>mixture parameters</em>, which must satisfy the condition</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^kP(C_i)=1\]</div>
<p>We write the set of all the model parameters compactly as</p>
<div class="math notranslate nohighlight">
\[\bs\theta=\{\mmu_1,\Sg_1,P(C_1),\cds,\mmu_k,\Sg_k,P(C_k)\}\]</div>
<p><strong>Maximum Likelihood Estimation</strong></p>
<p>Given the dataset <span class="math notranslate nohighlight">\(\D\)</span>, we define the <em>likelihood</em> of <span class="math notranslate nohighlight">\(\bs\th\)</span> as
the conditional probability of the data <span class="math notranslate nohighlight">\(\D\)</span> given the model parameters
<span class="math notranslate nohighlight">\(\bs\th\)</span>, denoted as <span class="math notranslate nohighlight">\(P(\D|\bs\th)\)</span>.</p>
<div class="math notranslate nohighlight">
\[p(\D|\bs\th)=\prod_{j=1}^nf(\x_j)\]</div>
<p>The goal of maximum likelihood estimation (MLE) is to choose the parameters <span class="math notranslate nohighlight">\(\bs\th\)</span> that maximize the likelihood</p>
<div class="math notranslate nohighlight">
\[\bs\th^*=\arg\max_{\bs\th}\{P(\D|\bs\th)\}\]</div>
<p>It is typical the maximize the log of the likelihood function</p>
<div class="math notranslate nohighlight">
\[\bs\th^*=\arg\max_{\bs\th}\{\ln P(\D|\bs\th)\}\]</div>
<p>where the <em>log-likelihood</em> function is given as</p>
<div class="math notranslate nohighlight">
\[\ln P(\D|\bs\th)=\sum_{j=1}^n\ln f(\x_j)=\sum_{j=1}^n\ln\bigg(\sum_{i=1}^kf(\x_j|\mmu_i,\Sg_i)P(C_i)\bigg)\]</div>
<p>We can use the expectation-maximization (EM) approach for finding the maximum
likelihood estimates for the parameters <span class="math notranslate nohighlight">\(\bs\th\)</span>.
EM is a two-step iterative approach that starts from an initial guess for the parameters <span class="math notranslate nohighlight">\(\bs\th\)</span>.
Given the current estimates for <span class="math notranslate nohighlight">\(\bs\th\)</span>, in the <em>expectation step</em> EM
computes the cluster posterior probabilities <span class="math notranslate nohighlight">\(P(C_i|\x_j)\)</span> via the Bayes
theorem:</p>
<div class="math notranslate nohighlight">
\[P(C_i|\x_j)=\frac{P(C_i\rm{\ and\ }\x_j)}{P(\x_j)}=\frac{P(\x_j|C_i)P(C_i)}{\sum_{a=1}^kP(\x_j|C_a)P(C_a)}\]</div>
<p>Because each cluster is modeled as a multivariate normal distribution, the
probability of <span class="math notranslate nohighlight">\(\x_j\)</span> given cluster <span class="math notranslate nohighlight">\(C_i\)</span> can be obtained by
considering a small interval <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> centered at <span class="math notranslate nohighlight">\(\x_j\)</span>, as
follows:</p>
<div class="math notranslate nohighlight">
\[P(\x_j|C_i)\simeq 2\epsilon\cd f(\x_j|\mmu_i,\Sg_i)=2\epsilon\cd f_i(\x_j)\]</div>
<p>The posterior probability of <span class="math notranslate nohighlight">\(C_i\)</span> given <span class="math notranslate nohighlight">\(\x_j\)</span> is thus given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp P(C_i|\x_j)=\frac{f_i(\x_j)\cd P(C_i)}{\sum_{a=1}^kf_a(\x_j)\cd P(C_a)}\)</span></p>
</div>
<p>and <span class="math notranslate nohighlight">\(P(C_i|\x_j)\)</span> can be considered as the weight or contribution of the point <span class="math notranslate nohighlight">\(\x_j\)</span> to cluster <span class="math notranslate nohighlight">\(C_i\)</span>.
Next, in the <em>maximization step</em>, using the weights <span class="math notranslate nohighlight">\(P(C_i|\x_j)\)</span> EM
re-estimates <span class="math notranslate nohighlight">\(\bs\th\)</span>, for each cluster <span class="math notranslate nohighlight">\(C_i\)</span>.
The re-estimated mean is given as the weighted average of all the points, the
re-estimated covariance matrix is given as the weighted covariance over all
pairs of dimensions, and the re0estimated prior probability for each cluster is
given as the fraction of weights that contribute to that cluster.</p>
<div class="section" id="em-in-one-dimension">
<h3>13.3.1 EM in One Dimension<a class="headerlink" href="#em-in-one-dimension" title="Permalink to this headline">Â¶</a></h3>
<p>Consider a dataset <span class="math notranslate nohighlight">\(\D\)</span> consisting of a single attribute <span class="math notranslate nohighlight">\(X\)</span>, where
each point <span class="math notranslate nohighlight">\(x_j\in\R\)</span> (<span class="math notranslate nohighlight">\(j=1,\cds,n\)</span>) is a random sample from
<span class="math notranslate nohighlight">\(X\)</span>.
For the mixture model, we use univariate normals for each cluster:</p>
<div class="math notranslate nohighlight">
\[f_i(x)=f(x|\mu_i,\sg_i^2)=\frac{1}{\sqrt{2\pi}\sg_i}\exp\bigg\{-\frac{(x-\mu_i)^2}{2\sg_i^2}\bigg\}\]</div>
<p>with the cluster parameters <span class="math notranslate nohighlight">\(\mu_i,\sg_i^2\)</span>, and <span class="math notranslate nohighlight">\(P(C_i)\)</span>.</p>
<p><strong>Initialization</strong></p>
<p>For each cluster <span class="math notranslate nohighlight">\(C_i\)</span>, with <span class="math notranslate nohighlight">\(i=1,2,\cds,k\)</span>, we can randomly
initialize the cluster parameters <span class="math notranslate nohighlight">\(\mu,\sg_i^2\)</span>, and <span class="math notranslate nohighlight">\(P(C_i)\)</span>.</p>
<p><strong>Expectation Step</strong></p>
<p>The posterior probabilities are computed as</p>
<div class="math notranslate nohighlight">
\[P(C_i|x_j)=\frac{f(x_j|\mu_i,\sg_i^2)\cd P(C_i)}{\sum_{a=1}^kf(x_j|\mu_a,\sg_a^2)\cd P(C_a)}\]</div>
<p>For convenience, we use the notation <span class="math notranslate nohighlight">\(w_{ij}=P(C_i|x_j)\)</span>, and let</p>
<div class="math notranslate nohighlight">
\[\w_i=(w_{i1},\cds,w_{in})^T\]</div>
<p>denote the weight vector for cluster <span class="math notranslate nohighlight">\(C_i\)</span> across all the <span class="math notranslate nohighlight">\(n\)</span> points.</p>
<p><strong>Maximization Step</strong></p>
<p>The re-estimated value for the cluster mean, <span class="math notranslate nohighlight">\(\mu_i\)</span>, is computed as the weighted mean of all the points:</p>
<div class="math notranslate nohighlight">
\[\mu_i=\frac{\sum_{j=1}^nw_{ij}\cd x_j}{\sum_{j=1}^nw_{ij}}\]</div>
<p>In terms of the weight vector <span class="math notranslate nohighlight">\(\w_i\)</span> and the attribute vector <span class="math notranslate nohighlight">\(X=(x_1,x_2,\cds,x_n)^T\)</span>, we can write as</p>
<div class="math notranslate nohighlight">
\[\mu_i=\frac{\w_i^TX}{\w_i^T\1}\]</div>
<p>The re-estimated value of the cluster variance is computed as the weighted variance across all the points:</p>
<div class="math notranslate nohighlight">
\[\sg_i^2=\frac{\sum_{j=1}^nw_{ij}(x_j-\mu_i)^2}{\sum_{j=1}^nw_{ij}}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\bar{X}_i=X-\mu_i\1=(x_1-\mu_i,x_2-\mu_i,\cds,x_n-\mu_i)^T=\)</span>
<span class="math notranslate nohighlight">\((\bar{x}_{i1},\bar{x}_{i2},\cds,\bar{x}_{in})^T\)</span> be the centered
attribute vector for cluster <span class="math notranslate nohighlight">\(C_i\)</span>, and let <span class="math notranslate nohighlight">\(\bar{X}_i^s\)</span> be the
squared vector given as
<span class="math notranslate nohighlight">\(\bar{X}_i^s=(\bar{x}_{i1}^2,\cds,\bar{x}_{in}^2)^T\)</span>.
The variance can be expressed compactly as</p>
<div class="math notranslate nohighlight">
\[\sg_i^2=\frac{\w_i^T\bar{X}_i^s}{\w_i^T\1}\]</div>
<p>The prior probability of cluster <span class="math notranslate nohighlight">\(C_i\)</span> is re-estimated as the fraction of
the total weight belonging to <span class="math notranslate nohighlight">\(C_i\)</span>, computed as</p>
<div class="math notranslate nohighlight">
\[P(C_i)=\frac{\sum_{j=1}^nw_{ij}}{\sum_{a=1}^k\sum_{j=1}^nw_{aj}}=
\frac{\sum_{j=1}^nw_{ij}}{\sum_{j=1}^n1}=\frac{\sum_{j=1}^nw_{ij}}{n}\]</div>
<p>where we made use of the fact that</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^kw_{ij}=\sum_{i=1}^kP(C_i|x_j)=1\]</div>
<p>In vector notation the prior probability can be written as</p>
<div class="math notranslate nohighlight">
\[P(C_i)=\frac{\w_i^T\1}{n}\]</div>
<p><strong>Iteration</strong></p>
<p>Starting from an initial set of values for the cluster parameters
<span class="math notranslate nohighlight">\(\mu_i,\sg_i^2\)</span>, and <span class="math notranslate nohighlight">\(P(C_i)\)</span> for all <span class="math notranslate nohighlight">\(i=1,\cds,k\)</span>, the EM
algorithm applies the expectation step to compute the weights
<span class="math notranslate nohighlight">\(w_{ij}=P(C_i|x_j)\)</span>.</p>
</div>
<div class="section" id="em-in-d-dimensions">
<h3>13.3.2 EM in <span class="math notranslate nohighlight">\(d\)</span> Dimensions<a class="headerlink" href="#em-in-d-dimensions" title="Permalink to this headline">Â¶</a></h3>
<p>For each cluster <span class="math notranslate nohighlight">\(C_i\)</span>, we now need to estimate the <span class="math notranslate nohighlight">\(d\)</span>-dimensional mean vector:</p>
<div class="math notranslate nohighlight">
\[\mmu_i=(\mu_{i1},\mu_{i2},\cds,\mu_{id})^T\]</div>
<p>and the <span class="math notranslate nohighlight">\(d\times d\)</span> covariance matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sg_i=\bp (\sg_1^i)^2&amp;\sg_{12}^i&amp;\cds&amp;\sg_{id}^i\\
\sg_{21}^i&amp;(\sg_2^i)^2&amp;\cds&amp;\sg_{2d}^i\\\vds&amp;\vds&amp;\dds&amp;\vds\\
\sg_{d1}^i&amp;\sg_{d2}^i&amp;\cds&amp;(\sg_d^i)^2 \ep\end{split}\]</div>
<p>One simplification is to assume that all dimensions are independent, which leads to a diagonal covariance matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sg_i=\bp (\sg_1^i)^2&amp;0&amp;\cds&amp;0\\0&amp;(\sg_2^i)^2&amp;\cds&amp;0\\\vds&amp;\vds&amp;\dds&amp;\vds\\0&amp;0&amp;\cds&amp;(\sg_d^i)^2 \ep\end{split}\]</div>
<p><strong>Initialization</strong></p>
<p>For each cluster <span class="math notranslate nohighlight">\(C_i\)</span>, with <span class="math notranslate nohighlight">\(i=1,2,\cds,k\)</span>, we can randomly
initialize the cluster parameters <span class="math notranslate nohighlight">\(\mmu,\Sg_i\)</span>, and <span class="math notranslate nohighlight">\(P(C_i)\)</span>.</p>
<p><strong>Expectation Step</strong></p>
<div class="math notranslate nohighlight">
\[w_{ij}=P(C_i|\x_j)=\frac{f_i(\x_j)\cd P(C_i)}{\sum_{a=1}^kf_a(\x_j)\cd P(C_a)}\]</div>
<p><strong>Maximization Step</strong></p>
<p>The mena <span class="math notranslate nohighlight">\(\mmu_i\)</span> for cluster <span class="math notranslate nohighlight">\(C_i\)</span> can be estimated as</p>
<div class="math notranslate nohighlight">
\[\mmu_i=\frac{\sum_{j-1}^nw_{ij}\cd\x_j}{\sum_{j=1}^nw_{ij}}=\frac{\D^T\w_i}{\w_i^T\1}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\bar\D_i=\D-\1\cd\mmu_i^T\)</span> be the centered data matrix for cluster <span class="math notranslate nohighlight">\(C_i\)</span>.
Let <span class="math notranslate nohighlight">\(\bar\x_{ji}=\x_j-\mmu_i\in\R^d\)</span> denote the <span class="math notranslate nohighlight">\(j\)</span>th centered point in <span class="math notranslate nohighlight">\(\bar\D_i\)</span>.
We can express <span class="math notranslate nohighlight">\(\Sg_i\)</span> as</p>
<div class="math notranslate nohighlight">
\[\Sg_i=\frac{\sum_{j=1}^nw_{ij}\bar\x_{ji}\bar\x_{ji}^T}{\w_i^T\1}\]</div>
<p>The covariance between dimensions <span class="math notranslate nohighlight">\(X_a\)</span> and <span class="math notranslate nohighlight">\(X_b\)</span> is estimated as</p>
<div class="math notranslate nohighlight">
\[\sg_{ab}^i=\frac{\sum_{j=1}^nw_{ji}(x_{ja}-\mu_{ia})(x_{jb}-\mu_{ib})}{\sum_{j=1}^nw_{ij}}\]</div>
<p>The prior probability <span class="math notranslate nohighlight">\(P(C_i)\)</span> for each cluster is the same as in the one-dimensional case, given as</p>
<div class="math notranslate nohighlight">
\[P(C_i)=\frac{\sum_{j=1}^nw_{ij}}{n}=\frac{\w_i^T\1}{n}\]</div>
<p><strong>EM Clustering Algorithm</strong></p>
<img alt="../_images/Algo13.3.png" src="../_images/Algo13.3.png" />
<p><strong>Computational Complexity</strong></p>
<p>The computational complexity of the EM method is <span class="math notranslate nohighlight">\(O(t(kd^3+nkd^2))\)</span>, where <span class="math notranslate nohighlight">\(t\)</span> is the number of iterations.
If we use a diagonal covariance matrix, then the complexity is therefore <span class="math notranslate nohighlight">\(O(tnkd)\)</span>.
The I/O complexity for the EM algorithm is <span class="math notranslate nohighlight">\(O(t)\)</span> complete databases scans
because we read the entire set of points in each iteration.</p>
<p><strong>K-means as Specialization of EM</strong></p>
<p>K-menas can be considered as a special case of the EM algorithm, obtained as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(C_i|\x_j)=\left\{\begin{array}{lr}1\quad\rm{if\ }C_i=\arg\min_{C_a}
\{\lv\x_j-\mmu_a\rv^2\}\\0\quad\rm{otherwise}\end{array}\right.\end{split}\]</div>
<p>The posterior probability <span class="math notranslate nohighlight">\(P(C_i|\x_j)\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[P(C_i|\x_j)=\frac{P(\x_j|C_i)P(C_i)}{\sum_{a=1}^kP(\x_j|C_a)P(C_a)}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(P(C_i|\x_j)=\left\{\begin{array}{lr}1\quad\rm{if\ }\x_j\in C_i,\rm{\ i.e.,\ if\ }C_i=\arg\min_{C_a}\{\lv\x_j-\mmu_a\rv^2\}\\0\quad\rm{otherwise}\end{array}\right.\)</span></p>
</div>
</div>
<div class="section" id="maximum-likelihood-estimation">
<h3>13.3.3 Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">Â¶</a></h3>
<p><strong>Estimation of Mean</strong></p>
<p><strong>Estimation of Covariance Matrix</strong></p>
<p><strong>Estimating the Prior Probability: Mixture Parameters</strong></p>
</div>
<div class="section" id="em-approach">
<h3>13.3.4 EM Approach<a class="headerlink" href="#em-approach" title="Permalink to this headline">Â¶</a></h3>
<p><strong>Expectation Step</strong></p>
<p><strong>Maximization Step</strong></p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap14.html" class="btn btn-neutral float-right" title="Chapter 14 Hierarchical Clustering" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index3.html" class="btn btn-neutral float-left" title="Part 3 Clustering" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>