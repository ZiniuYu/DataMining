

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 17 Clustering Validation &mdash; DataMining  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 4 Classification" href="../part4/index4.html" />
    <link rel="prev" title="Chapter 16 Spectral and Graph Clustering" href="chap16.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataMining
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../part1/index1.html">Part 1 Data Analysis Foundations</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index3.html">Part 3 Clustering</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap13.html">Chapter 13 Representative-based Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap14.html">Chapter 14 Hierarchical Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap15.html">Chapter 15 Density-based Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap16.html">Chapter 16 Spectral and Graph Clustering</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 17 Clustering Validation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#external-measures">17.1 External Measures</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#matching-based-measures">17.1.1 Matching Based Measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="#entropy-based-measures">17.1.2 Entropy-based Measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pairwise-measures">17.1.3 Pairwise Measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="#correlation-measures">17.1.4 Correlation Measures</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#internal-measures">17.2 Internal Measures</a></li>
<li class="toctree-l3"><a class="reference internal" href="#relative-measures">17.3 Relative Measures</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cluster-stability">17.3.1 Cluster stability</a></li>
<li class="toctree-l4"><a class="reference internal" href="#clustering-tendency">17.3.2 Clustering Tendency</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../part4/index4.html">Part 4 Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../part5/index5.html">Part 5 Regression</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataMining</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index3.html">Part 3 Clustering</a> &raquo;</li>
        
      <li>Chapter 17 Clustering Validation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part3/chap17.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lag}{\langle}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\od}{\odot}
\newcommand{\ra}{\rightarrow}
\newcommand{\rag}{\rangle}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\td}{\tilde}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\d}{\boldsymbol{\mathrm{d}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\h}{\boldsymbol{\mathrm{h}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\m}{\boldsymbol{\mathrm{m}}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\o}{\boldsymbol{\mathrm{o}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{\mathrm{w}}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\z}{\boldsymbol{\mathrm{z}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{\mathrm{B}}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\M}{\boldsymbol{\mathrm{M}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\Y}{\boldsymbol{\mathrm{Y}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\cE}{\mathcal{E}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}
\newcommand{\ve}{\varepsilon}\\\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\ppi}{\boldsymbol{\pi}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\TT}{\mathcal{T}}\\
\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-17-clustering-validation">
<h1>Chapter 17 Clustering Validation<a class="headerlink" href="#chapter-17-clustering-validation" title="Permalink to this headline">¶</a></h1>
<p>Cluster validation and assessment encompasses three main tasks: <em>clustering</em>
<em>evaluation</em> seeks to asses the goodness or quality of the clustering,
<em>clustering stability</em> seeks to understand the sensitivity of the clustering
result to various algorithmic parameters, and <em>clustering tendency</em> assesses the
suitability of applying clustering in the first place.</p>
<p><strong>External</strong>: external validation measures employ criteria that are not inherent to the dataset.</p>
<p><strong>Internal</strong>: Internal validation measures employ critieria that are derived from the data itself.</p>
<p><strong>Relative</strong>: Relative validation measures aim to directly compare different
clusterings, usually those obtained via different parameter settings for the
same algorithm.</p>
<div class="section" id="external-measures">
<h2>17.1 External Measures<a class="headerlink" href="#external-measures" title="Permalink to this headline">¶</a></h2>
<p>External measures assume that the correct or ground-truth clustering is known a <em>priori</em>.
The true cluster labels play the role of external information that is used to evaluate a given clustering.</p>
<p>Let <span class="math notranslate nohighlight">\(\D\)</span> be a dataset consisting of <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\x_i\)</span> in a
<em>d</em>-dimensional space, partitioned into <span class="math notranslate nohighlight">\(k\)</span> clusters.
Let <span class="math notranslate nohighlight">\(y_i\in\{1,2,\cds,k\}\)</span> denote the ground-truth cluster membership or label information for each point.
The ground-truth clustering is given as <span class="math notranslate nohighlight">\(\cl{T}=\{T_1,T_2,\cds,T_k\}\)</span>,
where the cluster <span class="math notranslate nohighlight">\(T_j\)</span> consists of all the points with label <span class="math notranslate nohighlight">\(j\)</span>,
i.e., <span class="math notranslate nohighlight">\(T_j=\{\x_i\in\D|y_i=j\}\)</span>.
Also, let <span class="math notranslate nohighlight">\(\cl{C}=\{C_1,\cds,C_r\}\)</span> dentoe a clustering of the same
dataset into <span class="math notranslate nohighlight">\(r\)</span> clusters, obtained via some clustering algorithm, and let
<span class="math notranslate nohighlight">\(\hat{y_i}\in\{1,2,\cds,r\}\)</span> denote the cluster label for <span class="math notranslate nohighlight">\(\x_i\)</span>.</p>
<p>External evaluation measures try capture the extent to which points from the
same partition appear in the same cluster, and the extent to which points from
different partitions are grouped in different clusters.
All of the external measures rely on the <span class="math notranslate nohighlight">\(r\times k\)</span> <em>contingency tabel</em>
<span class="math notranslate nohighlight">\(\N\)</span> that is induced by a clustering <span class="math notranslate nohighlight">\(\cl{C}\)</span> and the ground-truth
partitioning <span class="math notranslate nohighlight">\(\cl{T}\)</span>, defined as follows</p>
<div class="math notranslate nohighlight">
\[\N(i,j)=n_{ij}=|C_i\cap T_j|\]</div>
<p>In other words, the count <span class="math notranslate nohighlight">\(n_{ij}\)</span> denotes the number of points that are
common to cluster <span class="math notranslate nohighlight">\(C_i\)</span> and ground-truth partition <span class="math notranslate nohighlight">\(T_j\)</span>.</p>
<div class="section" id="matching-based-measures">
<h3>17.1.1 Matching Based Measures<a class="headerlink" href="#matching-based-measures" title="Permalink to this headline">¶</a></h3>
<p><strong>Purity</strong></p>
<div class="math notranslate nohighlight">
\[purity_i=\frac{1}{n_i}\max_{j=1}^k\{n_{ij}\}\]</div>
<p>The purity of clustering <span class="math notranslate nohighlight">\(\cl{C}\)</span> is defined as the weighted sum of the clusterwise purity values:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp purity=\sum_{i=1}^r\frac{n_i}{n}purity_i=\frac{1}{n}\sum_{i=1}^r\max_{j=1}^k\{n_{ij}\}\)</span></p>
</div>
<p>The larger the purity of <span class="math notranslate nohighlight">\(\cl{C}\)</span>, the better the agreement with the groundtruth.
The maximum value of purity is 1, when each cluster comprises points from only one partition.
When <span class="math notranslate nohighlight">\(r=k\)</span>, a purity value of 1 indicates a perfect clustering, with a
one-to-one correspondence between the clsuters and partitions.
However, purity can be 1 even for <span class="math notranslate nohighlight">\(r&gt;k\)</span>, when each of the clusters is a subset of a ground-truth partition.
When <span class="math notranslate nohighlight">\(r&lt;k\)</span>, purity can never by 1, because at least one cluster must contain points from more than one partition.</p>
<p><strong>Maximum Matching</strong></p>
<p>The maximum matching measure selects the mapping between clusters and
partitions, such that the sum of the number of common points (<span class="math notranslate nohighlight">\(n_{ij}\)</span>) is
maximized, provided that onlyl one cluster can match with a given partition.</p>
<p>Formally, we treat the contigency table as a complete weighted bipartite graph
<span class="math notranslate nohighlight">\(G=(V,E)\)</span>, where each partition and cluster is a node, that is,
<span class="math notranslate nohighlight">\(V=\cl{C}\cup\cl{T}\)</span>, and there exists an edge <span class="math notranslate nohighlight">\((C_i,T_j)\in E\)</span>,
with weight <span class="math notranslate nohighlight">\(w(C_i,T_i)=n_{ij}\)</span>, for all <span class="math notranslate nohighlight">\(C_i\in\cl{C}\)</span> and
<span class="math notranslate nohighlight">\(T_j\in\cl{T}\)</span>.
A <em>matching</em> <span class="math notranslate nohighlight">\(M\)</span> in <span class="math notranslate nohighlight">\(G\)</span> is a subset of <span class="math notranslate nohighlight">\(E\)</span>, such that the
edges in <span class="math notranslate nohighlight">\(M\)</span> are pairwise nonadjacent, that is, they do not have a common
vertex.
The maximum matching measure is defined as the <em>maximum weight matching</em> in <span class="math notranslate nohighlight">\(G\)</span>:</p>
<div class="math notranslate nohighlight">
\[match=\arg\max_M\bigg\{\frac{w(M)}{n}\bigg\}\]</div>
<p>where the weight of a matching <span class="math notranslate nohighlight">\(M\)</span> is simply the sum of all the edge
weights in <span class="math notranslate nohighlight">\(M\)</span>, given as <span class="math notranslate nohighlight">\(w(M)=\sum)_{e\in M}w(e)\)</span>.
The maximum matching can be computed in time
<span class="math notranslate nohighlight">\(O(|V|^2\cd|E|)=O((r+k)^2rk)\)</span>, which is equivalent to <span class="math notranslate nohighlight">\(O(k^4)\)</span> if
<span class="math notranslate nohighlight">\(r=O(k)\)</span>.</p>
<p><strong>F-Measure</strong></p>
<p>Given cluster <span class="math notranslate nohighlight">\(C_i\)</span>, let <span class="math notranslate nohighlight">\(j_i\)</span> denote the partition that contains
the maximum number of points from <span class="math notranslate nohighlight">\(C_i\)</span>, that is,
<span class="math notranslate nohighlight">\(j_i=\max_{j=1}^k\{n_{ij}\}\)</span>.
The <em>precision</em> of a cluster <span class="math notranslate nohighlight">\(C_i\)</span> is the same as its purity:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp prec_i=\frac{1}{n_i}\max_{j=1}^k\{n_{ij}\}=\frac{n_{ij_i}}{n_i}\)</span></p>
</div>
<p>It measures the fraction of points in <span class="math notranslate nohighlight">\(C_i\)</span> from the majority partition <span class="math notranslate nohighlight">\(T_{j_i}\)</span>.</p>
<p>The <em>recall</em> of cluster <span class="math notranslate nohighlight">\(C_i\)</span> is defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp recall_i=\frac{n_{ij_i}}{|T_{j_{i}}|}=\frac{n_{ij_i}}{m_{j_i}}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(m_{j_i}=|T_{j_i}|\)</span>.
It measures the fraction of point in partition <span class="math notranslate nohighlight">\(T_{j_i}\)</span> shared in common with cluster <span class="math notranslate nohighlight">\(C_i\)</span>.</p>
<p>The F-measure is the harmonic mean of the precision and recall values for each cluster.
The F-measure for cluster <span class="math notranslate nohighlight">\(C_i\)</span> is therefore given as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp F_i=\frac{2}{\frac{1}{prec_i}+\frac{1}{recall_i}}=\frac{2\cd prec_i\cd recall_i}{prec_i+recall_i}\)</span>
<span class="math notranslate nohighlight">\(\dp=\frac{2n_{ij_i}}{n_i+m_{j_i}}\)</span></p>
</div>
<p>The F-measures for the clustering <span class="math notranslate nohighlight">\(\cl{C}\)</span> is the mean of clusterwise F-measure values:</p>
<div class="math notranslate nohighlight">
\[F=\frac{1}{r}\sum_{i=1}^rF_i\]</div>
<p>F-measure thus tries to balance the precision and recall values across all the clusters.
For a perfect clustering, when <span class="math notranslate nohighlight">\(r=k\)</span>, the maximum value of the F-measure is 1.</p>
</div>
<div class="section" id="entropy-based-measures">
<h3>17.1.2 Entropy-based Measures<a class="headerlink" href="#entropy-based-measures" title="Permalink to this headline">¶</a></h3>
<p><strong>Conditional Entropy</strong></p>
<p>The entropy of a clustering <span class="math notranslate nohighlight">\(\cl{C}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[H(\cl{C})=-\sum_{i=1}^rp_{C_i}\log p_{C_i}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_{C_i}=\frac{n_i}{n}\)</span> is the probability of cluster <span class="math notranslate nohighlight">\(C_i\)</span>.
The entropy of the partitioning <span class="math notranslate nohighlight">\(\cl{T}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[H(\cl{T})=-\sum_{j=1}^kp_{T_j}\log p_{T_j}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_{T_j}=\frac{m_j}{n}\)</span> is the probability of partition <span class="math notranslate nohighlight">\(T_j\)</span>.</p>
<p>The cluster-specific entropy of <span class="math notranslate nohighlight">\(\cl{T}\)</span>, that is, the conditional entropy
of <span class="math notranslate nohighlight">\(\cl{T}\)</span> with respect to cluster <span class="math notranslate nohighlight">\(C_i\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[H(\cl{T}|C_i)=-\sum_{j=1}^k\bigg(\frac{n_{ij}}{n_i}\bigg)\log\bigg(\frac{n_{ij}}{n_i}\bigg)\]</div>
<p>The conditional entropy of <span class="math notranslate nohighlight">\(\cl{T}\)</span> given clustering <span class="math notranslate nohighlight">\(\cl{C}\)</span> is then defined as the weighted sum:</p>
<div class="math notranslate nohighlight">
\[H(\cl{T}|\cl{C})=\sum_{i=1}^r\frac{n_i}{n}H(\cl{T}|C_i)=
\sum_{i=1}^r\sum_{j=1}^k\frac{n_{ij}}{n}\log\bigg(\frac{n_{ij}}{n_i}\bigg)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp=-\sum_{i=1}^r\sum_{j=1}^kp_{ij}\log\bigg(\frac{p_{ij}}{p_{C_i}}\bigg)\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(p_{ij}=\frac{n_{ij}}{n}\)</span> is the probability that a point in cluster
<span class="math notranslate nohighlight">\(i\)</span> also belongs to partition <span class="math notranslate nohighlight">\(j\)</span>.
For a perfect clustering, the conditional entropy value is zero, whereas the
worst possible conditional entropy value is <span class="math notranslate nohighlight">\(\log k\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H(\cl{T}|\cl{C})&amp;=-\sum_{i=1}^r\sum_{j=1}^kp_{ij}(\log p_{ij}-\log p_{C_i})\\&amp;=-\bigg(\sum_{i=1}^r\sum_{j=1}^kp_{ij}\log p_{ij}\bigg)+\sum_{i=1}^r\bigg(\log p_{C_i}\sum_{j=1}^kp_{ij}\bigg)\\&amp;=-\sum_{i=1}^r\sum_{j=1}^kp_{ij}\log p_{ij}+\sum_{i=1}^rp_{C_i}\log p_{C_i}\\&amp;=H(\cl{C},\cl{T})-H(\cl{C})\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(H(\cl{C},\cl{T})=-\sum_{i=1}^r\sum_{j=1}^kp_{ij}\log p_{ij}\)</span> is the
joint entropy of <span class="math notranslate nohighlight">\(\cl{C}\)</span> and <span class="math notranslate nohighlight">\(\cl{T}\)</span>.
The conditional entropy <span class="math notranslate nohighlight">\(H(\cl{T}|\cl{C})\)</span> thus measures the remaining
entropy of <span class="math notranslate nohighlight">\(\cl{T}\)</span> given the clustering <span class="math notranslate nohighlight">\(\cl{C}\)</span>.
In particular, <span class="math notranslate nohighlight">\(H(\cl{T}|\cl{C})=0\)</span> if and only if <span class="math notranslate nohighlight">\(\cl{T}\)</span> is
completely determined by <span class="math notranslate nohighlight">\(\cl{C}\)</span>, corresponding to the ideal clustering.
On the other hand, if <span class="math notranslate nohighlight">\(\cl{C}\)</span> and <span class="math notranslate nohighlight">\(\cl{T}\)</span> are independent of each
other, then <span class="math notranslate nohighlight">\(H(\cl{T}|\cl{C})=H(\cl{T})\)</span>, which means that <span class="math notranslate nohighlight">\(\cl{C}\)</span>
provides no information about <span class="math notranslate nohighlight">\(\cl{T}\)</span>.</p>
<p><strong>Normalized Mutual Information</strong></p>
<p>The <em>mutual information</em> tries to quantify the amount of shared information
between the clustering <span class="math notranslate nohighlight">\(\cl{C}\)</span> and partitioning <span class="math notranslate nohighlight">\(\cl{T}\)</span>, and it is
defined as</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp I(\cl{C},\cl{T})=\sum_{i=1}^r\sum_{j=1}^kp_{ij}\log\bigg(\frac{p_{ij}}{p_{C_i}\cd p_{T_j}}\bigg)\)</span></p>
</div>
<p>It measures the dependence between the observed joint probability <span class="math notranslate nohighlight">\(p_{ij}\)</span>
of <span class="math notranslate nohighlight">\(\cl{C}\)</span> and <span class="math notranslate nohighlight">\(\cl{T}\)</span>, and the expected joint probability
<span class="math notranslate nohighlight">\(p_{C_i}\cd p_{T_j}\)</span> under the independence assumption.
When <span class="math notranslate nohighlight">\(\cl{C}\)</span> and <span class="math notranslate nohighlight">\(\cl{T}\)</span> are independent then
<span class="math notranslate nohighlight">\(p_{ij}=p_{C_i}\cd p_{T_j}\)</span>, and thus <span class="math notranslate nohighlight">\(T(\cl{C},\cl{T})=0\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}I(\cl{C},\cl{T})=H(\cl{T})-H(\cl{T}|\cl{C})\\I(\cl{C},\cl{T})=H(\cl{C})-H(\cl{C}|\cl{T})\end{aligned}\end{align} \]</div>
<p>Finally, because <span class="math notranslate nohighlight">\(H(\CC,\TT)\geq 0\)</span> and <span class="math notranslate nohighlight">\(H(\TT|\CC)\geq 0\)</span>, we have
the inequalities <span class="math notranslate nohighlight">\(I(\CC,\TT)\leq H(\CC)\)</span> and
<span class="math notranslate nohighlight">\(I(\CC,\TT)\leq H(\TT)\)</span>.</p>
<p>The <em>normalized mutual information</em> (NMI) is defined as the geometric mean of two ratios:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp NMI(\CC,\TT)=\sqrt{\frac{I(\CC,\TT)}{H(\CC)}\cd\frac{I(\CC,\TT)}{H(\TT)}}=\)</span>
<span class="math notranslate nohighlight">\(\dp\frac{I(\CC,\TT)}{\sqrt{H(\CC)\cd H(\TT)}}\)</span></p>
</div>
<p>The NMI value lies in the range <span class="math notranslate nohighlight">\([0, 1]\)</span>.
Values close to 1 indicate a good clustering.</p>
<p><strong>Variation of Information</strong></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}VI(\CC,\TT)&amp;=(H(\TT)-I(\CC,\TT))+(H(\CC)-I(\CC,\TT))\\&amp;=H(\TT)+H(\CC)-2I(\CC,\TT)\end{aligned}\end{align} \]</div>
<p>Variation of information (VI) is zero only when <span class="math notranslate nohighlight">\(\CC\)</span> and <span class="math notranslate nohighlight">\(\TT\)</span> are identical.
Thus the lower the VI value the better the clustering <span class="math notranslate nohighlight">\(\CC\)</span>.</p>
<div class="math notranslate nohighlight">
\[VI(\CC,\TT)=H(\TT|\CC)+H(\CC|\TT)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(VI(\CC,\TT)=2H(\TT,\CC)-H(\TT)-H(\CC)\)</span></p>
</div>
</div>
<div class="section" id="pairwise-measures">
<h3>17.1.3 Pairwise Measures<a class="headerlink" href="#pairwise-measures" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\x_i,\x_j\in\D\)</span> be any two points, with <span class="math notranslate nohighlight">\(i\neq j\)</span>.
Let <span class="math notranslate nohighlight">\(y_i\)</span> denote the true partition label and let <span class="math notranslate nohighlight">\(\hat{y_i}\)</span> denote
the cluster label for point <span class="math notranslate nohighlight">\(\x_i\)</span>.
If both <span class="math notranslate nohighlight">\(\x_i\)</span> and <span class="math notranslate nohighlight">\(\x_j\)</span> belong to the same cluster, that is,
<span class="math notranslate nohighlight">\(\hat{y_i}=\hat{y_j}\)</span>, we call it a <em>positive</em> event, and if they do not
belong to the same cluster, we call that a <em>negative</em> event.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(True\ Positives=|\{(\x_i,\x_j):y_i=y_j\ \rm{and}\ \hat{y_i}=\hat{y_j}\}|\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(False\ Negatives=|\{(\x_i,\x_j):y_i=y_j\ \rm{and}\ \hat{y_i}\neq\hat{y_j}\}|\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(False\ Positives=|\{(\x_i,\x_j):y_i\neq y_j\ \rm{and}\ \hat{y_i}=\hat{y_j}\}|\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(True\ Negatives=|\{(\x_i,\x_j):y_i\neq y_j\ \rm{and}\ \hat{y_i}\neq\hat{y_j}\}|\)</span></p>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}N=\bp n\\2 \ep=\frac{n(n-1)}{2}=TP+FN+FP+TN\end{split}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}TP=\sum_{i=1}^r\sum_{j=1}^k\bp n_{ij}\\2 \ep=
\sum_{i=1}^r\sum_{j=1}^k\frac{n_{ij}(n_{ij}-1)}{2}=
\frac{1}{2}\bigg(\sum_{i=1}^r\sum_{j=1}^kn_{ij}^2-
\sum_{i=1}^r\sum_{j=1}^kn_{ij}\bigg)\end{split}\\=\frac{1}{2}\bigg(\bigg(\sum_{i=1}^r\sum_{j=1}^kn_{ij}^2\bigg)-n\bigg)\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}FN=\sum_{j=1}^k\bp m_j\\2 \ep-TP=\frac{1}{2}\bigg(\sum_{j=1}^km_j^2-
\sum_{j=1}^km_j-\sum_{i=1}^r\sum_{j=1}^kn_{ij}^2+n\bigg)\end{split}\\=\frac{1}{2}\bigg(\sum_{j=1}^km_j^2-\sum_{i=1}^r\sum_{j=1}^kn_{jj}^2\bigg)\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\begin{split}FP=\sum_{i=1}^r\bp n_i\\2 \ep-TP=\frac{1}{2}\bigg(\sum_{i=1}^rn_i^2-\sum_{i=1}^r\sum_{j=1}^kn_{ij}^2\bigg)\end{split}\]</div>
<div class="math notranslate nohighlight">
\[TN=N-(TP+FN+FP)=\frac{1}{2}\bigg(n^2-\sum_{i=1}^rn_i^2-\sum_{j=1}^km_j^2+\sum_{i=1}^r\sum_{j=1}^kn_{ij}^2\bigg)\]</div>
<p>Each of the four values can be computed in <span class="math notranslate nohighlight">\(O(rk)\)</span> time.
Because the contingency table can be obtained in linear time, the total time to
compute the four values is <span class="math notranslate nohighlight">\(O(n+rk)\)</span>, which is much better than the negative
<span class="math notranslate nohighlight">\(O(n^2)\)</span> bound.</p>
<p><strong>Jaccard Coefficient</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Jaccard=\frac{TP}{TP+FN+FP}\)</span></p>
</div>
<p>For a perfect clustering <span class="math notranslate nohighlight">\(\CC\)</span>, the Jaccard Coefficient has value 1, as in
that case there are no false positives or false negatives.
The Jaccard coefficient is asymmetric in terms of the true positives and
negatives because it ignores the true negatives.</p>
<p><strong>Rand Statistic</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Rand=\frac{TP+TN}{N}\)</span></p>
</div>
<p>The Rand statistic, which is symmetric, measures the fraction of point pairs
where both <span class="math notranslate nohighlight">\(\CC\)</span> and <span class="math notranslate nohighlight">\(\TT\)</span> agree.
A perfect clustering has a value of 1 for the statistic.</p>
<p><strong>Fowlkes-Mallows Measure</strong></p>
<p>Define the overall <em>pairwise precision</em> and <em>pairwise recall</em> values for a clustering <span class="math notranslate nohighlight">\(\CC\)</span>, as follows:</p>
<div class="math notranslate nohighlight">
\[prec=\frac{TP}{TP+FP}\quad\quad recall=\frac{TP}{TP+FN}\]</div>
<p>The Fowlkes-Mallows (FM) measure is defined as the geometric mean of the pairwise precision and recall</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp FM=\sqrt{prec\cd recall}=\frac{TP}{\sqrt{(TP+FN)(TP+FP)}}\)</span></p>
</div>
</div>
<div class="section" id="correlation-measures">
<h3>17.1.4 Correlation Measures<a class="headerlink" href="#correlation-measures" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\X\)</span> and <span class="math notranslate nohighlight">\(\bs{\rm{Y}}\)</span> be two symmetric <span class="math notranslate nohighlight">\(n\times n\)</span> matrics, and let <span class="math notranslate nohighlight">\(N=\bp n\\2 \ep\)</span>.
Let <span class="math notranslate nohighlight">\(\x,\y\in\R^N\)</span> denote the vectors obtained by linearizing the upper
triangular elements (excluding the main diagonal) of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>,
respectively.
Let <span class="math notranslate nohighlight">\(\mu_X\)</span> denote the element-wise mean of <span class="math notranslate nohighlight">\(\x\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[\mu_X=\frac{1}{N}\sum_{i=1}^{n-1}\sum_{j=i+1}^n\X(i,j)=\frac{1}{N}\x^T\x\]</div>
<p>and let <span class="math notranslate nohighlight">\(\bar{\x}\)</span> denote the centered <span class="math notranslate nohighlight">\(\x\)</span> vector, defined as</p>
<div class="math notranslate nohighlight">
\[\bar{\x}=\x-\1\cd\mu_X\]</div>
<p>The Hubert statistic is defined as the averaged element-wise product between <span class="math notranslate nohighlight">\(\X\)</span> and <span class="math notranslate nohighlight">\(\bs{\rm{Y}}\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\Gamma=\frac{1}{N}\sum_{i=1}^{n-1}\sum_{j=i+1}^n\X(i,j)\cd\bs{\rm{Y}}(i,j)=\frac{1}{N}\x^T\y\)</span></p>
</div>
<p>The normalized Hubert statistic is defined as the element-wise correlation between <span class="math notranslate nohighlight">\(\X\)</span> and <span class="math notranslate nohighlight">\(\bs{\rm{Y}}\)</span></p>
<div class="math notranslate nohighlight">
\[\Gamma_n=\frac{\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}(\X(i,j)-\mu_X)\cd
(\bs{\rm{Y}}(i,j)-\mu_Y)}{\sqrt{\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}
(\X(i,j)-\mu_X)^2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}(\bs{\rm{Y}}[i]-\mu_Y)^2}}
=\frac{\sg_{XY}}{\sqrt{\sg_X^2\sg_Y^2}}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\sg_X^2=\frac{1}{N}\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}(\X(i,j)-\mu_X)^2=
\frac{1}{N}\bar{\x}^T\bar{\x}=\frac{1}{N}\lv\bar{\x}\rv^2\\\sg_Y^2=\frac{1}{N}\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}(\bs{\rm{Y}}(i,j)-
\mu_Y)^2=\frac{1}{N}\bar{\y}^T\bar{\y}=\frac{1}{N}\lv\bar{\y}\rv^2\\\sg_{XY}=\frac{1}{N}\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}(\X(i,j)-\mu_x)
(\bs{\rm{Y}}(i,j)-\mu_Y)=\frac{1}{N}\bar{\x}^T\bar{\y}\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\Gamma_n=\frac{\bar{\x}^T\bar{\y}}{\lv\bar{\x}\rv\cd\lv\bar{\y}\rv}=\cos\th\)</span></p>
</div>
<p><strong>Discretized Hubert Statistic</strong></p>
<p>Let <span class="math notranslate nohighlight">\(\bs{\rm{T}}\)</span> and <span class="math notranslate nohighlight">\(\bs{\rm{C}}\)</span> be the <span class="math notranslate nohighlight">\(n\times n\)</span> matrices defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bs{\rm{T}}(i,j)=\left\{\begin{array}{lr}1\quad\rm{if\ }y_i=y_j,i\neq j\\
0\quad\rm{otherwise}\end{array}\right.\quad\quad
\bs{\rm{C}}(i,j)=\left\{\begin{array}{lr}1\quad\rm{if\ }
\hat{y_i}=\hat{y_j},i\neq j\\0\quad\rm{otherwise}\end{array}\right.\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\Gamma=\frac{1}{N}\rm{\bs{t}}^T\c=\frac{TP}{N}\]</div>
<p><strong>Normalized Discretized Hubert Statistic</strong></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\Gamma_n=\frac{\bar{\rm{\bs{t}}}^T\bar{\c}}{\lv\bar{\rm{\bs{t}}}\rv\cd\lv\bar{\c}\rv}=\cos\th\\\mu_T=\frac{\rm{\bs{t}}^T\rm{\bs{t}}}{N}=\frac{TP+FN}{N}\\\mu_C=\frac{\c^T\c}{N}=\frac{TP+FP}{N}\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\bar{\rm{\bs{t}}}^T\bar{\c}&amp;=(\rm{\bs{t}}-\1\cd\mu_T)^T(\c-\1\cd\mu_C)\\&amp;=\rm{\bs{t}}^T\c-\mu_C\rm{\bs{t}}^T\1-\mu_T\c^T\1+\1^T\1\mu_T\mu_C\\&amp;=\rm{\bs{t}}^T\c-N\mu_C\mu_T-N\mu_T\mu_C+N\mu_T\mu_C\\&amp;=\rm{\bs{t}}^T\c-N\mu_T\mu_C\\&amp;=TP-N\mu_T\mu_C\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\lv\bar{\rm{\bs{t}}}\rv^2=\bar{\rm{\bs{t}}}^T\bar{\rm{\bs{t}}}=
\rm{\bs{t}}^T\rm{\bs{t}}-N\mu_T^2=N\mu_T-N\mu_T^2=N\mu_T(1-\mu_T)\\\lv\bar{\c}\rv^2=\bar{\c}^T\bar{\c}=\c^T\c-N\mu_C^2=N\mu_C-N\mu_C^2=N\mu_C(1-\mu_C)\end{aligned}\end{align} \]</div>
<p>Discretized Hubert statistic can be written as</p>
<div class="math notranslate nohighlight">
\[\Gamma_n=\frac{\frac{TP}{N}-\mu_T\mu_C}{\sqrt{\mu_T\mu_C(1-\mu_T)(1-\mu_C)}}\]</div>
</div>
</div>
<div class="section" id="internal-measures">
<h2>17.2 Internal Measures<a class="headerlink" href="#internal-measures" title="Permalink to this headline">¶</a></h2>
<p>Internal evaluation measures do not have recourse to the ground-truth
partitioning, which is the typical scenario when clustering a dataset.
The internal measures are based on the <span class="math notranslate nohighlight">\(n\times n\)</span> <em>distance matrix</em>, also
called the <em>proximity matrix</em>, of all pairwise distances among the <span class="math notranslate nohighlight">\(n\)</span>
points:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bs{\rm{W}}=\{\lv\x_i-\x_j\rv\}_{i,j=1}^n\)</span></p>
</div>
<p>The proximity matrix <span class="math notranslate nohighlight">\(\bs{\rm{W}}\)</span> can also be considered as the adjacency
matrix of the weighted complete graph <span class="math notranslate nohighlight">\(G\)</span> over the <span class="math notranslate nohighlight">\(n\)</span> points, that
is, with nodes <span class="math notranslate nohighlight">\(V=\{\x_i|\x_i\in\D\}\)</span>, edges
<span class="math notranslate nohighlight">\(E=\{(\x_i,\x_j)|\x_i,\x_j\in\D\}\)</span> and edge weights
<span class="math notranslate nohighlight">\(w_{ij}=\bs{\rm{W}}(i,j)\)</span> for all <span class="math notranslate nohighlight">\(\x_i,\x_j\in\D\)</span>.</p>
<p>For internal measures, we assume that we are given a clustering
<span class="math notranslate nohighlight">\(\CC=\{C_1,\cds,C_k\}\)</span> comprising <span class="math notranslate nohighlight">\(r=k\)</span> clusteres, with cluster
<span class="math notranslate nohighlight">\(C_i\)</span> containing <span class="math notranslate nohighlight">\(n_i=|C_i|\)</span> points.
Let <span class="math notranslate nohighlight">\(\hat{y_i}\in\{1,2,\cds,k\}\)</span> denote the clsuter label for point <span class="math notranslate nohighlight">\(\x_i\)</span>.
The clustering <span class="math notranslate nohighlight">\(\CC\)</span> can be considered as a <span class="math notranslate nohighlight">\(k\)</span>-way cut in <span class="math notranslate nohighlight">\(G\)</span>
because <span class="math notranslate nohighlight">\(C_i\neq\emptyset\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>,
<span class="math notranslate nohighlight">\(C_i\cap C_j=\emptyset\)</span> for all <span class="math notranslate nohighlight">\(i,j\)</span>, and <span class="math notranslate nohighlight">\(\bigcup_iC_i=V\)</span>.
Given any subsets <span class="math notranslate nohighlight">\(S,R\subset V\)</span>, define <span class="math notranslate nohighlight">\(W(S,R)\)</span> as the sum of the
weights on all edges with one vertex in <span class="math notranslate nohighlight">\(S\)</span> and the other in <span class="math notranslate nohighlight">\(R\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[W(S,R)=\sum_{\x_i\in S}\sum_{\x_j\in R}w_{ij}\]</div>
<p>Also, given <span class="math notranslate nohighlight">\(S\subseteq V\)</span>, we denote by <span class="math notranslate nohighlight">\(\bar{S}\)</span> the complementary
set of vertices, that is, <span class="math notranslate nohighlight">\(\bar{S}=V-S\)</span>.</p>
<p>The sum of all the intracluster weights, denoted <span class="math notranslate nohighlight">\(W_{in}\)</span>, and
intercluster weights, denoted <span class="math notranslate nohighlight">\(W_{out}\)</span> are given as</p>
<div class="math notranslate nohighlight">
\[W_{in}=\frac{1}{2}\sum_{i=1}^kW(C_i,C_i)\]</div>
<div class="math notranslate nohighlight">
\[W_{out}=\frac{1}{2}\sum_{i=1}^kW(C_i,\bar{C_i})=\sum_{i=1}^{k-1}\sum_{j&gt;i}W(C_i,C_j)\]</div>
<p>The number of distinct intracluster edges, denoted <span class="math notranslate nohighlight">\(N_{in}\)</span>, and
intercluster edges, denoted <span class="math notranslate nohighlight">\(N_{out}\)</span>, are given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}N_{in}=\sum_{i=1}^k\bp n_i\\2 \ep=\frac{1}{2}\sum_{i=1}^kn_i(n_i-1)\end{split}\]</div>
<div class="math notranslate nohighlight">
\[N_{out}=\sum_{i=1}^{k-1}\sum_{j=i+1}^kn_i\cd n_j=\frac{1}{2}\sum_{i=1}^k\sum_{j=1,j\neq i}^kn_i\cd n_j\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}N=N_{in}+N_{out}=\bp n\\2 \ep=\frac{1}{2}n(n-1)\end{split}\]</div>
<p>The total number of distinct pairs of points <span class="math notranslate nohighlight">\(N\)</span> satisfies the identity</p>
<div class="math notranslate nohighlight">
\[\begin{split}N=N_{in}+N_{out}=\bp n\\2 \ep=\frac{1}{2}n(n-1)\end{split}\]</div>
<p><strong>BetaCV Measure</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp BetaCV=\frac{W_{in}/N_{in}}{W_{out}/N_{out}}=\frac{N_{out}}{N_{in}}\cd\frac{W_{in}}{W_{out}}\)</span>
<span class="math notranslate nohighlight">\(\dp\frac{N_{out}}{N_{in}}\frac{\sum_{i=1}^kW(C_i,C_i)}{\sum_{i=1}^kW(C_i,\bar{C_i})}\)</span></p>
</div>
<p>The smaller the BetaCV ratio, the better the clustering, as it indicates that
intracluster distances are on average smaller than intercluster distances.</p>
<p><strong>C-index</strong></p>
<p>Let <span class="math notranslate nohighlight">\(W_\min(N_{in})\)</span> be the sum of the smallest <span class="math notranslate nohighlight">\(N_{in}\)</span> distances
in the proximity matrix <span class="math notranslate nohighlight">\(\bs{\rm{W}}\)</span>, where <span class="math notranslate nohighlight">\(N_{in}\)</span> is the total
number of intracluster edges, or point pairs.
Let <span class="math notranslate nohighlight">\(W_\max(N_{in})\)</span> be the sum of the largest <span class="math notranslate nohighlight">\(N_{in}\)</span> distaces in
<span class="math notranslate nohighlight">\(\bs{\rm{W}}\)</span>.</p>
<p>The C-index measures to what extent the clustering puts together the
<span class="math notranslate nohighlight">\(N_{in}\)</span> points that are the closest across the <span class="math notranslate nohighlight">\(k\)</span> clusters.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp C-index=\frac{W_{in}-W_\min(N_{in})}{W_\max(N_{in})-W_\min(N_{in})}\)</span></p>
</div>
<p>The C-index lies in the range <span class="math notranslate nohighlight">\([0,1]\)</span>.
The smaller the C-index, the better the clustering, as it indicates more compact
clusters with relatively smaller distances with clusters rather than between
clusters.</p>
<p><strong>Normalized Cut Measure</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp NC=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{vol(C_i)}=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{W(C_i,V)}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(vol(C_i)=W(C_i,V)\)</span> is the volume of cluster <span class="math notranslate nohighlight">\(C_i\)</span>, that is,
the total weights on edges with at least one end in the cluster.</p>
<div class="math notranslate nohighlight">
\[NC=\sum_{i=1}^k\frac{W(C_i,\bar{C_i})}{W(C_i,C_i)+W(C_i,\bar{C_i})}=
\sum_{i=1}^k\frac{1}{\frac{W(C_i,C_i)}{W(C_i,\bar{C_i})}+1}\]</div>
<p>We can see that <span class="math notranslate nohighlight">\(NC\)</span> is maximized when the ratio
<span class="math notranslate nohighlight">\(\frac{W(C_i,C_i)}{W(C_i,\bar{C_i})}\)</span> (across the <span class="math notranslate nohighlight">\(k\)</span> clusters) are
as small as possible, which happens when the intracluster distances are much
smaller compared to intercluster distances, that is, when the clustering is
good.
The maximum possible value of <span class="math notranslate nohighlight">\(NC\)</span> is <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p><strong>Modularity</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Q=\sum_{i=1}^k\bigg(\frac{W(C_i,C_i)}{W(V,V)}-\bigg(\frac{W(C_i,V)}{W(V,V)}\bigg)^2\bigg)\)</span></p>
</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}W(V,V)&amp;=\sum_{i=1}^kW(C_i,V)\\&amp;=\sum_{i=1}^kW(C_i,C_i)+\sum_{i=1}^kW(C_i,\bar{C_i})\\&amp;=2(W_{in}+W_{out})\end{aligned}\end{align} \]</div>
<p>Modularity measures the difference between the observed and expected fraction of weights on edges within the clusters.
Since we are using the distance matrix, the smaller the modularity measure the
better the clustering, which indicates that the intracluster distaces are lower
than expected.</p>
<p><strong>Dunn Index</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp Dunn=\frac{W_{out}^\min}{W_{in}^\max}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(W_{out}^\min\)</span> is the minimum intercluster distance:</p>
<div class="math notranslate nohighlight">
\[W_{out}^\min=\min_{i,j&gt;i}\{w_{ab}|\x_a\in C_i,\x_b\in C_j\}\]</div>
<p>and <span class="math notranslate nohighlight">\(W_{in}^\max\)</span> is the maximum intracluster distance:</p>
<div class="math notranslate nohighlight">
\[W_{in}^\max=\max_i\{w_{ab}|\x_a,\x_b\in C_i\}\]</div>
<p>The larger the Dunn index the better the clustering because it means even the
closest distance between points in different clusters is much larger than the
farthest distance between points in the same cluster.
However, the Dunn index may be insensitive because the minimum intercluster and
maximum intracluster distances do not capture all the information about a
clustering.</p>
<p><strong>Davies-Bouldin index</strong></p>
<p>Let <span class="math notranslate nohighlight">\(\mu_i\)</span> denote the cluster mean, given as</p>
<div class="math notranslate nohighlight">
\[\mmu_i=\frac{1}{n_i}\sum_{\x_i\in C_i}\x_j\]</div>
<p>Further, let <span class="math notranslate nohighlight">\(\sg_{\mu_i}\)</span> denote the dispersion or spread of the points around the cluster mean, given as</p>
<div class="math notranslate nohighlight">
\[\sg_{\mu_i}=\sqrt{\frac{\sum_{\x_j\in C_i}\lv\x_j-\mmu_i\rv^2}{n_i}}=\sqrt{\rm{var}(C_i)}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp DB_{ij}=\frac{\sg_{\mu_i}+\sg_{\mu_j}}{\lv\mmu_i-\mmu_j\rv}\)</span></p>
</div>
<p><span class="math notranslate nohighlight">\(DB_{ij}\)</span> measures how compact the clsuters are compared to the distance between the cluster means.
The Davies-Bouldin index is then defined as</p>
<div class="math notranslate nohighlight">
\[DB=\frac{1}{k}\sum_{i=1}^k\max_{j\ne i}\{DB_{ij}\}\]</div>
<p>The smaller the DB value the better the clustering, as it means that the
clusters are well separated (i.e., the distance between cluster means is large),
and each cluster is well represented by its mean (i.e., has a small spread).</p>
<p><strong>Silhouette Coefficient</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp s_i=\frac{\mu_{out}^\min(\x_i)-\mu_{in}(\x_i)}{\max\{\mu_{out}^\min(\x_i),\mu_{in}(\x_i)\}}\)</span></p>
</div>
<p>where <span class="math notranslate nohighlight">\(\mu_{in}(\x_i)\)</span> is the mean distance from <span class="math notranslate nohighlight">\(\x_i\)</span> to points in its own cluster <span class="math notranslate nohighlight">\(\hat{y_i}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mu_{in}(\x_i)=\frac{\sum_{\x_j\in C_{\hat{y_i}},j\ne i}\lv\x_i-\x_j\rv}{n_{\hat{y_i}}-1}\]</div>
<p>and <span class="math notranslate nohighlight">\(\mu_{out}^\min(\x_i)\)</span> is the mean of the distance from <span class="math notranslate nohighlight">\(\x_i\)</span> to points in the closest cluster:</p>
<div class="math notranslate nohighlight">
\[\mu_{out}^\min(\x_i)=\min_{j\ne\hat{y_i}}\bigg\{\frac{\sum_{\y\in C_j}\lv\x_i-\y\rv}{n_j}\bigg\}\]</div>
<p>The silhouette coefficient is defined as the mean <span class="math notranslate nohighlight">\(s_i\)</span> value across all the points:</p>
<div class="math notranslate nohighlight">
\[SC=\frac{1}{n}\sum_{i=1}^ns_i\]</div>
<p>A value close to 1 indicates a good clustering.</p>
<p><strong>Hubert Statistic</strong></p>
<p>The Hubert <span class="math notranslate nohighlight">\(\Gamma\)</span> statistic, and its normalized version
<span class="math notranslate nohighlight">\(\Gamma_n\)</span>, can both be used as internal evaluation measures by letting
<span class="math notranslate nohighlight">\(\X=\bs{\rm{W}}\)</span> be the pairwise distance matrix, and by defining
<span class="math notranslate nohighlight">\(\bs{\rm{Y}}\)</span> as the matrix of distances between the cluster means.</p>
<div class="math notranslate nohighlight">
\[\bs{\rm{Y}}=\{\lv\mmu_i-\mmu_j\rv\}_{i,j=1}^N\]</div>
<p>where <span class="math notranslate nohighlight">\(\mmu_i\)</span> is the mean for cluster <span class="math notranslate nohighlight">\(C_i\)</span>.</p>
</div>
<div class="section" id="relative-measures">
<h2>17.3 Relative Measures<a class="headerlink" href="#relative-measures" title="Permalink to this headline">¶</a></h2>
<p>Relative measures are used to compare different cluesterings obtained by varying
different parameters for the same algorithm, for example, to choose the number
of cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p><strong>Silhouette Coefficient</strong></p>
<div class="math notranslate nohighlight">
\[SC_i=\frac{1}{n_i}\sum_{\x_j\in C_i}s_j\]</div>
<p>We can pick the value <span class="math notranslate nohighlight">\(k\)</span> that yields the best clustering, with many
points having high <span class="math notranslate nohighlight">\(s_j\)</span> values within each cluster, as well as high
values for <span class="math notranslate nohighlight">\(SC\)</span> and <span class="math notranslate nohighlight">\(SC_i(1\leq i\leq k)\)</span>.</p>
<p><strong>Calinski-Harabasz Index</strong></p>
<p>Given the dataset <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points <span class="math notranslate nohighlight">\(\x_j\)</span>, the scatter matrix for <span class="math notranslate nohighlight">\(\D\)</span> is given as</p>
<div class="math notranslate nohighlight">
\[\bs{\rm{S}}=n\Sg=\sum_{j=1}^n(\x_j-\mmu)(\x_j-\mmu)^T\]</div>
<p>The scatter matrix can be decomposed into two matrices
<span class="math notranslate nohighlight">\(\bs{\rm{S}}=\bs{\rm{S}}_W+\bs{\rm{S}}_B\)</span>, where <span class="math notranslate nohighlight">\(\bs{\rm{S}}_W\)</span> is
the within-cluster scatter matrix and <span class="math notranslate nohighlight">\(\bs{\rm{S}}_B\)</span> is the
between-cluster matrix, given as</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\bs{\rm{S}}_W=\sum_{i=1}^k\sum_{\x_j\in C_i}(\x_j-\mmu_i)(\x_j-\mmu_i)^T\\\bs{\rm{S}}_B=\sum_{i=1}^kn_i(\mmu_i-\mmu)(\mmu_i-\mmu)^T\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp CH(k)=\frac{tr(\bs{\rm{S}}_B)/(k-1)}{tr(\bs{\rm{S}}_W)/(n-k)}=\)</span>
<span class="math notranslate nohighlight">\(\dp\frac{n-k}{n-1}\cd\frac{tr(\bs{\rm{S}}_B)}{tr(\bs{\rm{S}}_W)}\)</span></p>
</div>
<p>For a good value of <span class="math notranslate nohighlight">\(k\)</span>, we expect the within-cluster scatter to be
smaller relative to the between-cluster scatter, which should result in a higher
<span class="math notranslate nohighlight">\(CH(k)\)</span> value.
On the other hand, we do not desire a very large value of <span class="math notranslate nohighlight">\(k\)</span>; thus the
term <span class="math notranslate nohighlight">\(\frac{n-k}{k-1}\)</span> panalizes larger values of <span class="math notranslate nohighlight">\(k\)</span>.
We could choose a value of <span class="math notranslate nohighlight">\(k\)</span> that maximizes <span class="math notranslate nohighlight">\(CH(k)\)</span>.
Alternatively, we can plot the <span class="math notranslate nohighlight">\(CH\)</span> values and look for a large increase
in the value followed by little or no gain.
For instance, we can choose the value that minimizes the term</p>
<div class="math notranslate nohighlight">
\[\Delta(k)=(CH(k+1)-CH(k))-(CH(k)-CH(k-1))\]</div>
<p>The intuition is that we want to find the value of <span class="math notranslate nohighlight">\(k\)</span> for which
<span class="math notranslate nohighlight">\(CH(k)\)</span> is much higher than <span class="math notranslate nohighlight">\(CH(k-1)\)</span> and there is only a little
improvement or a decrease in the <span class="math notranslate nohighlight">\(CH(k+1)\)</span> value.</p>
<p><strong>Gap Statistic</strong></p>
<p>The gap statistic compares the sum of intracluster weights <span class="math notranslate nohighlight">\(W_{in}\)</span> for
different values of <span class="math notranslate nohighlight">\(k\)</span> with their expected values assuming no apparent
clustering structure, which forms the null hypothesis.</p>
<p>Let <span class="math notranslate nohighlight">\(\CC_k\)</span> be the clustering obtained for a specified value of <span class="math notranslate nohighlight">\(k\)</span>, using a chosen clustering algorithm.
Let <span class="math notranslate nohighlight">\(W_{in}^k(\D)\)</span> denote the sum of intracluster weights (over all
clsuters) for <span class="math notranslate nohighlight">\(\CC_k\)</span> on the input dataset <span class="math notranslate nohighlight">\(\D\)</span>.
We would like to compute the probability of the observed <span class="math notranslate nohighlight">\(W_{in}^k\)</span> value
under the null hypothesis that the points are randomly placed in the same data
space as <span class="math notranslate nohighlight">\(\D\)</span>.</p>
<p>We resort to Monte Carlo simulations to obtain an empirical distribution for <span class="math notranslate nohighlight">\(W_{in}\)</span>.
We generate <span class="math notranslate nohighlight">\(t\)</span> random samples comprising <span class="math notranslate nohighlight">\(n\)</span> randomly distributed
points within the same <span class="math notranslate nohighlight">\(d\)</span>-dimensional data space as the input dataset
<span class="math notranslate nohighlight">\(\D\)</span>.
That is, for each dimension of <span class="math notranslate nohighlight">\(\D\)</span>, say <span class="math notranslate nohighlight">\(X_j\)</span>, we compute its range
<span class="math notranslate nohighlight">\([\min(X_j),\max(X_j)]\)</span> and generate values for the <span class="math notranslate nohighlight">\(n\)</span> points (for
the <span class="math notranslate nohighlight">\(j\)</span>th dimension) uniformly at random within the given range.
Let <span class="math notranslate nohighlight">\(\bs{\rm{R}}_i\in\R^{n\times d}\)</span>, <span class="math notranslate nohighlight">\(1\leq i\leq t\)</span> denote the <span class="math notranslate nohighlight">\(i\)</span>th sample.
Let <span class="math notranslate nohighlight">\(W_{in}^k(\bs{\rm{R}}_i)\)</span> denote the sum of intracluster weights for a
given clustering of <span class="math notranslate nohighlight">\(\bs{\rm{R}}_i\)</span> into <span class="math notranslate nohighlight">\(k\)</span> clusters.
From each sample dataset <span class="math notranslate nohighlight">\(\bs{\rm{R}}_i\)</span>, we generate clusterings for
different values of <span class="math notranslate nohighlight">\(k\)</span> using the same algorithm and record the
intraclsuter values <span class="math notranslate nohighlight">\(W_{in}^k(\bs{\rm{R}}_i)\)</span>.
Let <span class="math notranslate nohighlight">\(\mu_W(k)\)</span> and <span class="math notranslate nohighlight">\(\sg_W(k)\)</span> denote the mean and standard deviation
of these intracluster weights for each value of <span class="math notranslate nohighlight">\(k\)</span>, given as</p>
<div class="math notranslate nohighlight">
\[\mu_W(k)=\frac{1}{t}\sum_{i=1}^t\log W_{in}^k(\bs{\rm{R}}_i)\]</div>
<div class="math notranslate nohighlight">
\[\sg_W(k)=\sqrt{\frac{1}{t}\sum_{i=1}^t(\log W_{in}^k(\bs{\rm{R}}_i)-\mu_W(k))^2}\]</div>
<p>where we use the logarithm of the <span class="math notranslate nohighlight">\(W_{in}\)</span> values, as they can be quite large.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(gap(k)=\mu_W(k)-\log W_{in}^k(\D)\)</span></p>
</div>
<p>It measures the deviation of the observed <span class="math notranslate nohighlight">\(W_{in}^k\)</span> value from its expected value under the null hypothesis.
We can select the value of <span class="math notranslate nohighlight">\(k\)</span> that yields the largest gap statistic
because that indicates a clustering structure far away from the uniform
distribution of points.
A more robust approach is to choose <span class="math notranslate nohighlight">\(k\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[k^*=\arg\min_k\{gap(k)\geq gap(k+1)-\sg_W(k+1)\}\]</div>
<p>That is, we select the least value of <span class="math notranslate nohighlight">\(k\)</span> such that the gap statistic
exceeds one standard deviation of the gap at <span class="math notranslate nohighlight">\(k+1\)</span>.</p>
<div class="section" id="cluster-stability">
<h3>17.3.1 Cluster stability<a class="headerlink" href="#cluster-stability" title="Permalink to this headline">¶</a></h3>
<p>The main idea behind cluster stability is that the clusterings obtained from
several datasets sampled from the same underlying distribution as <span class="math notranslate nohighlight">\(\D\)</span>
should be similar or “stable”.</p>
<p>Considering the bootstrapping approach, we generate <span class="math notranslate nohighlight">\(t\)</span> samples of size
<span class="math notranslate nohighlight">\(n\)</span> by sampling from <span class="math notranslate nohighlight">\(\D\)</span> with replacement, which allows the same
point to be chosen possibly multiple times, and thus each sample <span class="math notranslate nohighlight">\(\D_i\)</span>
will be different.
Next, for each sample <span class="math notranslate nohighlight">\(\D_i\)</span> we run the same clustering algorithm with
different cluster values <span class="math notranslate nohighlight">\(k\)</span> ranging from 2 to <span class="math notranslate nohighlight">\(k^\max\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\CC_k(\D_i)\)</span> denote the clustering obtained from sample <span class="math notranslate nohighlight">\(\D_i\)</span>, for a given value of <span class="math notranslate nohighlight">\(k\)</span>.
Next, the method compares the distance between all pairs of clustering
<span class="math notranslate nohighlight">\(\CC_k(\D_i)\)</span> and <span class="math notranslate nohighlight">\(\CC_k(\D_j)\)</span> via some distance function.
We compute the expected pairwise distance for each value of <span class="math notranslate nohighlight">\(k\)</span>.
Finally, the value <span class="math notranslate nohighlight">\(k^*\)</span> that exhibits the least deviation between the
clusterings obtained from the resampled datasets is the best choice for
<span class="math notranslate nohighlight">\(k\)</span> because it exhibits the most stability.</p>
<p>Before computing the distance between the two clusterings, we have to restrict
the clusterings only to the points common to both <span class="math notranslate nohighlight">\(\D_i\)</span> and <span class="math notranslate nohighlight">\(\D_j\)</span>,
denoted as <span class="math notranslate nohighlight">\(\D_{ij}\)</span>.
Because sampling with replacement allows multiple instances of the same point,
we also have to acoount for this when creating <span class="math notranslate nohighlight">\(D_{ij}\)</span>.
For each point <span class="math notranslate nohighlight">\(\x_a\)</span> in the input dataset <span class="math notranslate nohighlight">\(\D\)</span>, let <span class="math notranslate nohighlight">\(m_i^a\)</span>
and <span class="math notranslate nohighlight">\(m_j^a\)</span> denote the number of occurrences of <span class="math notranslate nohighlight">\(\x_a\)</span> in
<span class="math notranslate nohighlight">\(\D_i\)</span> and <span class="math notranslate nohighlight">\(\D_j\)</span>, respectively.</p>
<div class="math notranslate nohighlight">
\[\D_{ij}=\D_i\cap\D_j=\{m^a\ \rm{instances\ of}\ \x_a|\x_a\in\D,m^a=\min\{m_i^a,m_j^a\}\}\]</div>
<img alt="../_images/Algo17.1.png" src="../_images/Algo17.1.png" />
<p>In general, those external measures that yield lower values for better agreement
between <span class="math notranslate nohighlight">\(\CC_k(\D_i)\)</span> and <span class="math notranslate nohighlight">\(\CC_k(\D_j)\)</span> can be used as distance
functions, whereas those that yield higher values for better agreement can be
used as similarity functions.</p>
</div>
<div class="section" id="clustering-tendency">
<h3>17.3.2 Clustering Tendency<a class="headerlink" href="#clustering-tendency" title="Permalink to this headline">¶</a></h3>
<p>Clustering tendency or clusterability aims to determine whether the dataset
<span class="math notranslate nohighlight">\(\D\)</span> has any meaningful groups to begin with.</p>
<p><strong>Spatial Histogram</strong></p>
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\cds,X_d\)</span> denote the <span class="math notranslate nohighlight">\(d\)</span> dimensions.
Given <span class="math notranslate nohighlight">\(b\)</span>, the number of bins for each dimension, we divide each dimension
<span class="math notranslate nohighlight">\(X_j\)</span> into <span class="math notranslate nohighlight">\(b\)</span> equi-width bins, and simply count how many points lie
in each of the <span class="math notranslate nohighlight">\(b^d\)</span> <span class="math notranslate nohighlight">\(d\)</span>-dimensional cells.
From this spatial histogram, we can obtain the empirical joint probability mass
function (EPMF) for the dataset <span class="math notranslate nohighlight">\(\D\)</span>, which is an approximation of the
unknown joint probability density function.
The EPMF is givne as</p>
<div class="math notranslate nohighlight">
\[f(\i)=P(\x_j\in\ \rm{cell}\ \i)=\frac{|\{\x_j\in\ \rm{cell}\ \i\}|}{n}\]</div>
<p>where <span class="math notranslate nohighlight">\(\i=(i_1,i_2,\cds,i_d)\)</span> denotes a cell index, with <span class="math notranslate nohighlight">\(i_j\)</span>
denoting the bin index along dimension <span class="math notranslate nohighlight">\(X_j\)</span>.</p>
<p>Next, we generate <span class="math notranslate nohighlight">\(t\)</span> random samples, each comprising <span class="math notranslate nohighlight">\(n\)</span> points
within the same <span class="math notranslate nohighlight">\(d\)</span>-dimensional space as the input dataset <span class="math notranslate nohighlight">\(\D\)</span>.
That is, for each dimension <span class="math notranslate nohighlight">\(X_j\)</span>, we compute its range
<span class="math notranslate nohighlight">\([\min(X_j),\max(X_j)]\)</span>, and generate values uniformly at random within
the givne range.
Let <span class="math notranslate nohighlight">\(\bs{\rm{R}}_j\)</span> denote the <span class="math notranslate nohighlight">\(j\)</span>th such random smaple.
We can then compute the corresponding EPMF <span class="math notranslate nohighlight">\(g_j(\i)\)</span> for each <span class="math notranslate nohighlight">\(\bs{\rm{R}}_j, 1\leq j\leq t\)</span>.</p>
<p>Finally, we can compute how much the distribution <span class="math notranslate nohighlight">\(f\)</span> differs from
<span class="math notranslate nohighlight">\(g_j\)</span> (for <span class="math notranslate nohighlight">\(j=1,\cds,t\)</span>), using the Kullback-Leibler (KL) divergence
from <span class="math notranslate nohighlight">\(f\)</span> to <span class="math notranslate nohighlight">\(g_j\)</span>, defined as</p>
<div class="math notranslate nohighlight">
\[KL(f|g_j)=\sum_\i f(\i)\log\bigg(\frac{f(\i)}{g_j(\i)}\bigg)\]</div>
<p>The KL divergence is zero only when <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g_j\)</span> are the same distributions.</p>
<p>The main limitation of this approach is that as dmensionality increases, the
number of cells <span class="math notranslate nohighlight">\((b^d)\)</span> increases exponentially, and with a fixed sample
size <span class="math notranslate nohighlight">\(n\)</span>, most of the cells will be empty, or will have only one point,
making it hard to estimate the divergence.
The method is also sensitive to the choice of parameter <span class="math notranslate nohighlight">\(b\)</span>.
Instead of histograms, and the corresponding EPMF, we can also use density
estimation methods to determine the joint probability density function (PDF) for
the dataset <span class="math notranslate nohighlight">\(\D\)</span>, and see how it differs from the PDF for the random
datasets.
However, the curse of dimensionality also causes problems for density estimation.</p>
<p><strong>Distance Distribution</strong></p>
<p>We create the EPMF from the proximity matrix <span class="math notranslate nohighlight">\(\bs{\rm{W}}\)</span> for
<span class="math notranslate nohighlight">\(\bs{\rm{D}}\)</span> by binning the distances into <span class="math notranslate nohighlight">\(b\)</span> bins:</p>
<div class="math notranslate nohighlight">
\[f(i)=P(w_{pq}\in\ \rm{bin}\ i|\x_p,\x_q\in\D,p&lt;q)=\frac{|\{w_{pq}\in\ \rm{bin}\ i\}|}{n(n-1)/2}\]</div>
<p><strong>Hopkins Statistic</strong></p>
<p>Given a dataset <span class="math notranslate nohighlight">\(\D\)</span> comprising <span class="math notranslate nohighlight">\(n\)</span> points, we generate <span class="math notranslate nohighlight">\(t\)</span>
random subsamples <span class="math notranslate nohighlight">\(\bs{rm{R}}_i\)</span> of <span class="math notranslate nohighlight">\(m\)</span> points each, where
<span class="math notranslate nohighlight">\(m\ll n\)</span>.
These samples are drawn from the same data space as <span class="math notranslate nohighlight">\(\D\)</span>, generated uniformly at random along each dimension.
Futher, we also generate <span class="math notranslate nohighlight">\(t\)</span> subsamples of <span class="math notranslate nohighlight">\(m\)</span> points directly from
<span class="math notranslate nohighlight">\(\D\)</span>, using sampling without replacement.
Let <span class="math notranslate nohighlight">\(\D_i\)</span> denote the <span class="math notranslate nohighlight">\(i\)</span>th direct subsample.
Next, we compute the minimum distance between each point <span class="math notranslate nohighlight">\(\x_j\in\D_i\)</span> and points in <span class="math notranslate nohighlight">\(\D\)</span></p>
<div class="math notranslate nohighlight">
\[\delta_\min(\x_j)=\min_{\x_i\in\D,\x_i\ne\x_j}\{\lv\x_j-\x_i\rv\}\]</div>
<p>Likewise, we compute the minimum distance <span class="math notranslate nohighlight">\(\delta_\min(\y_i)\)</span> between a
point <span class="math notranslate nohighlight">\(\y_i\in\bs{\rm{R}}_i\)</span> and points in <span class="math notranslate nohighlight">\(\D\)</span>.</p>
<p>The Hopkins statistic for the <span class="math notranslate nohighlight">\(i\)</span>th pair of samples <span class="math notranslate nohighlight">\(\bs{\rm{R}}_i\)</span> and <span class="math notranslate nohighlight">\(\D_i\)</span> is then defined as</p>
<div class="math notranslate nohighlight">
\[HS_i=\frac{\sum_{\y_j\in\bs{\rm{R}}_i}(\delta_\min(\y_j))^d}
{\sum_{\y_j\in\bs{\rm{R}}_i}(\delta_\min(\y_j))^d+
+sum_{\x_j\in\bs{\rm{R}}_i}(\delta_\min(\x_j))^d}\]</div>
<p>If the data is well clusterd we expect <span class="math notranslate nohighlight">\(\delta_\min(\x_j)\)</span> values to be
smaller compared to the <span class="math notranslate nohighlight">\(\delta_\min(\y_j)\)</span> values, and in this case
<span class="math notranslate nohighlight">\(HS_i\)</span> tends to 1.
If both nearest-neighbor distances are similar, then <span class="math notranslate nohighlight">\(HS_i\)</span> takes on
values close to 0.5, which indicates that the data is essentially random, and
there is no apparent clustering.
Finally, if <span class="math notranslate nohighlight">\(\delta_\min(\x_j)\)</span> values are larger compared to
<span class="math notranslate nohighlight">\(\delta_\min(\y_j)\)</span> values, then <span class="math notranslate nohighlight">\(HS_i\)</span> tends to 0, and it indicates
point repulsion, with no clustering.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../part4/index4.html" class="btn btn-neutral float-right" title="Part 4 Classification" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap16.html" class="btn btn-neutral float-left" title="Chapter 16 Spectral and Graph Clustering" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>